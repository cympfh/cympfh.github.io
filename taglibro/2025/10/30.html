<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="ja" xml:lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="description" content="Thu 11 Sep 2025, Tue 16 Sep 2025, Tue 23 Sep 2025, Tue 07 Oct 2025, Wed 08 Oct 2025, Thu 23 Oct 2025" />
  <meta name="og:url" content="http://cympfh.cc/taglibro">
  <meta name="og:title" content="月報 2025&#x2F;09, 2025&#x2F;10" />
  <meta name="og:description" content="Thu 11 Sep 2025, Tue 16 Sep 2025, Tue 23 Sep 2025, Tue 07 Oct 2025, Wed 08 Oct 2025, Thu 23 Oct 2025" />
  <meta name="og:image" content="http://cympfh.cc/resources/img/identicon.png" />
  <meta property="og:url" content="http://cympfh.cc/taglibro">
  <meta property="og:title" content="月報 2025&#x2F;09, 2025&#x2F;10" />
  <meta property="og:description" content="Thu 11 Sep 2025, Tue 16 Sep 2025, Tue 23 Sep 2025, Tue 07 Oct 2025, Wed 08 Oct 2025, Thu 23 Oct 2025" />
  <meta property="og:image" content="http://cympfh.cc/resources/img/identicon.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@cympfh" />
  <meta name="generator" content="unidoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>月報 2025&#x2F;09, 2025&#x2F;10</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
  </style>
  <link rel="stylesheet" href="../../resources/css/c.css" />
  <link rel="stylesheet" href="../../../resources/css/c.css" />
  <link rel="stylesheet" href="../../../resources/css/youtube.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" />
  <link href="https://unpkg.com/prismjs@1.x.0/themes/prism.css" rel="stylesheet" />
  <script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="taglibro">
    <header class="page-header"><a href='../../index.html'><i class="fas fa-fast-backward"></i></a></header>
<h1 class="title" id="1-%E6%9C%88%E5%A0%B1%202025%2F09%2C%202025%2F10">月報 2025/09, 2025/10</h1>
<h2 id="2-Thu%2011%20Sep%202025">Thu 11 Sep 2025</h2>
<h3 id="3-14%3A15%3A30">14:15:30</h3>
<p>エアフライヤーで鶏皮だけカリカリに焼いて塩かけて食べるのいいね.</p>
<p>油がどんどん出てくるからクッキングシートを敷くのはもはや諦めて受け皿で鶏油として回収した. 200円でコップ1/2杯分の旨い油が取れるのいいな. これで炒飯作ってみた. 旨いけどちょっと鶏油の量を控えすぎたかもしれない. まだあるので今度は使い切って入れてみる.</p>
<p>これを作るためだけに鶏油を回収して保管するというのはアホらしいので, チャーハンを作る前のその鍋で鶏皮を焼いちゃうのが良いな.</p>
<h3 id="3-18%3A46%3A13">18:46:13</h3>
<p>ニコニコでおすすめに従って連続再生してたら Turkey! っていうアニメが流れてきた. 一話最後の最後まで大真面目に見てたのにアホ加減に度肝を抜かれた. こういう方向のアニメ久しぶりだ. 六話まで見てるがトンチキ具合に拍車がかかってる.</p>
<h2 id="2-Tue%2016%20Sep%202025">Tue 16 Sep 2025</h2>
<h3 id="3-14%3A55%3A08%20%3Cem%3E%E6%9C%80%E8%BF%91%E8%AA%AD%E3%82%93%E3%81%A0%E8%AB%96%E6%96%87%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%82%8B%3C%2Fem%3E">14:55:08 <em>最近読んだ論文をまとめる</em></h3>
<p>ブラウザの reading スペースに読んでる/読んだ論文をひたすらタブ開いてるんだけど, 閉じる前にメモだけ残しておくテスト. 過去の日記と重複あり.</p>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2502.14541" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">LLM-based User Profile Management for Recommender System</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users&#x27; purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2502.14541" target="_blank" rel="nofollow">arxiv.org/abs/2502.14541</a></div></div>
    
<ul>
  <li>
    LLM-based User Profile Management for Recommender System
    <ul>
      <li>PURE</li>
    </ul>
  </li>
  <li>ユーザー情報をテキスト情報として持っておいて推薦システムもLLMがこれを使う</li>
</ul>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2305.12922v1" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">It&#x27;s Enough: Relaxing Diagonal Constraints in Linear Autoencoders for Recommendation</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Linear autoencoder models learn an item-to-item weight matrix via convex optimization with L2 regularization and zero-diagonal constraints. Despite their simplicity, they have shown remarkable performance compared to sophisticated non-linear models. This paper aims to theoretically understand the properties of two terms in linear autoencoders. Through the lens of singular value decomposition (SVD) and principal component analysis (PCA), it is revealed that L2 regularization enhances the impact of high-ranked PCs. Meanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs, leading to performance degradation for unpopular items. Inspired by this analysis, we propose simple-yet-effective linear autoencoder models using diagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and Relaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize linear autoencoders by adjusting the degree of diagonal constraints. Experimental results demonstrate that our models are comparable or superior to state-of-the-art linear and non-linear models on six benchmark datasets; they significantly improve the accuracy of long-tail items. These results also support our theoretical insights on regularization and diagonal constraints in linear autoencoders.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2305.12922v1" target="_blank" rel="nofollow">arxiv.org/abs/2305.12922v1</a></div></div>
    
<ul>
  <li>It’s Enough: Relaxing Diagonal Constraints in Linear Autoencoders for Recommendation</li>
  <li>EASE の対角成分の制約を緩くする</li>
</ul>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2507.03861" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">Continual Recommender Systems</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Modern recommender systems operate in uniquely dynamic settings: user interests, item pools, and popularity trends shift continuously, and models must adapt in real time without forgetting past preferences. While existing tutorials on continual or lifelong learning cover broad machine learning domains (e.g., vision and graphs), they do not address recommendation-specific demands-such as balancing stability and plasticity per user, handling cold-start items, and optimizing recommendation metrics under streaming feedback. This tutorial aims to make a timely contribution by filling that gap. We begin by reviewing the background and problem settings, followed by a comprehensive overview of existing approaches. We then highlight recent efforts to apply continual learning to practical deployment environments, such as resource-constrained systems and sequential interaction settings. Finally, we discuss open challenges and future research directions. We expect this tutorial to benefit researchers and practitioners in recommender systems, data mining, AI, and information retrieval across academia and industry.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2507.03861" target="_blank" rel="nofollow">arxiv.org/abs/2507.03861</a></div></div>
    
<ul>
  <li>Continual Recommender Systems</li>
  <li>毎回再学習するんじゃなくて, 前回までに獲得したモデルを引き継いで更新してく</li>
</ul>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2401.01497" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">A Pre-trained Sequential Recommendation Framework: Popularity Dynamics for Zero-shot Transfer</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Sequential recommenders are crucial to the success of online applications, \eg e-commerce, video streaming, and social media. While model architectures continue to improve, for every new application domain, we still have to train a new model from scratch for high quality recommendations. On the other hand, pre-trained language and vision models have shown great success in zero-shot or few-shot adaptation to new application domains. Inspired by the success of pre-trained models in peer AI fields, we propose a novel pre-trained sequential recommendation framework: PrepRec. We learn universal item representations by modeling item popularity dynamics. Through extensive experiments on five real-world datasets, we show that PrepRec, without any auxiliary information, can not only zero-shot transfer to a new domain, but achieve competitive performance compared to state-of-the-art sequential recommender models with only a fraction of the model size. In addition, with a simple post-hoc interpolation, PrepRec can improve the performance of existing sequential recommenders on average by 13.8\% in Recall@10 and 29.5% in NDCG@10. We provide an anonymized implementation of PrepRec at https://anonymous.4open.science/r/PrepRec--2F60/</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2401.01497" target="_blank" rel="nofollow">arxiv.org/abs/2401.01497</a></div></div>
    
<ul>
  <li>
    A Pre-trained Sequential Recommendation Framework: Popularity Dynamics for Zero-shot Transfer
    <ul>
      <li>PrepRec</li>
    </ul>
  </li>
  <li>
    ドメインに全く依存せず, アイテムを人気度のダイナミクスで表現する
    <ul>
      <li>学習データと適用データはアイテムもユーザーも全く異なってOK</li>
    </ul>
  </li>
</ul>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2507.19067" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Recommender systems, especially those based on graph neural networks (GNNs), have achieved remarkable success in capturing user-item interaction patterns. However, they remain susceptible to popularity bias--the tendency to over-recommend popular items--resulting in reduced content diversity and compromised fairness. In this paper, we propose PBiLoss, a novel regularization-based loss function designed to counteract popularity bias in graph-based recommender models explicitly. PBiLoss augments traditional training objectives by penalizing the model&#x27;s inclination toward popular items, thereby encouraging the recommendation of less popular but potentially more personalized content. We introduce two sampling strategies: Popular Positive (PopPos) and Popular Negative (PopNeg), which respectively modulate the contribution of the positive and negative popular items during training. We further explore two methods to distinguish popular items: one based on a fixed popularity threshold and another without any threshold, making the approach flexible and adaptive. Our proposed method is model-agnostic and can be seamlessly integrated into state-of-the-art graph-based frameworks such as LightGCN and its variants. Comprehensive experiments across multiple real-world datasets demonstrate that PBiLoss significantly improves fairness, as demonstrated by reductions in the Popularity-Rank Correlation for Users (PRU) and Popularity-Rank Correlation for Items (PRI), while maintaining or even enhancing standard recommendation accuracy and ranking metrics. These results highlight the effectiveness of directly embedding fairness objectives into the optimization process, providing a practical and scalable solution for balancing accuracy and equitable content exposure in modern recommender systems.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2507.19067" target="_blank" rel="nofollow">arxiv.org/abs/2507.19067</a></div></div>
    
<ul>
  <li>PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems</li>
  <li>
    アイテムの人気度合いで学習の重みを変える
    <ul>
      <li>人気アイテムに偏りたくない</li>
    </ul>
  </li>
</ul>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2505.04421" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Modeling ultra-long user behavior sequences is critical for capturing both long- and short-term preferences in industrial recommender systems. Existing solutions typically rely on two-stage retrieval or indirect modeling paradigms, incuring upstream-downstream inconsistency and computational inefficiency. In this paper, we present LONGER, a Long-sequence Optimized traNsformer for GPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism for stabilizing attention over long contexts, (ii) a token merge module with lightweight InnerTransformers and hybrid attention strategy to reduce quadratic complexity, and (iii) a series of engineering optimizations, including training with mixed-precision and activation recomputation, KV cache serving, and the fully synchronous model training and serving framework for unified GPU-based dense and sparse parameter updates. LONGER consistently outperforms strong baselines in both offline metrics and online A/B testing in both advertising and e-commerce services at ByteDance, validating its consistent effectiveness and industrial-level scaling laws. Currently, LONGER has been fully deployed at more than 10 influential scenarios at ByteDance, serving billion users.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2505.04421" target="_blank" rel="nofollow">arxiv.org/abs/2505.04421</a></div></div>
    
<ul>
  <li>
    LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders
    <ul>
      <li>ByteDance</li>
    </ul>
  </li>
  <li>めちゃ長いシーケンスを安定して入力できる Transformer を作った</li>
</ul>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2105.01601v4" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">MLP-Mixer: An all-MLP Architecture for Vision</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. &quot;mixing&quot; the per-location features), and one with MLPs applied across patches (i.e. &quot;mixing&quot; spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2105.01601v4" target="_blank" rel="nofollow">arxiv.org/abs/2105.01601v4</a></div></div>
    
<ul>
  <li>MLP-Mixer: An all-MLP Architecture for Vision</li>
  <li>
    CNN も Transformer も使わずに MLP だけで画像認識させる
    <ul>
      <li>入力データを転置させながら MLP に通すのがコツ</li>
    </ul>
  </li>
</ul>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2506.16942v1" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for Sequential Recommendation</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Sequential recommendation, a critical task in recommendation systems, predicts the next user action based on the understanding of the user&#x27;s historical behaviors. Conventional studies mainly focus on cross-behavior modeling with self-attention based methods while neglecting comprehensive user interest modeling for more dimensions. In this study, we propose a novel sequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer architecture to achieve efficient and complete modeling of user interests. Our method learns comprehensive user interests via cross-behavior and cross-feature user sequence modeling. The mixer layers are stacked in a pyramid way for cross-period user temporal interest learning. Through extensive offline and online experiments, we demonstrate the effectiveness and efficiency of our method, and we obtain a +0.106% improvement in user stay duration and a +0.0113% increase in user active days in the online A/B test. The Pyramid Mixer has been successfully deployed on the industrial platform, demonstrating its scalability and impact in real-world applications.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2506.16942v1" target="_blank" rel="nofollow">arxiv.org/abs/2506.16942v1</a></div></div>
    
<ul>
  <li>
    Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for Sequential Recommendation
    <ul>
      <li>ByteDance</li>
    </ul>
  </li>
  <li>
    時系列推薦をほぼ MLP だけでやる
    <ul>
      <li>時系列データを MLP-Mixer みたいに転置しながら通す</li>
    </ul>
  </li>
</ul>
<h2 id="2-Tue%2023%20Sep%202025">Tue 23 Sep 2025</h2>
<h3 id="3-14%3A35%3A33%20%3Cem%3E%E8%AB%B8%E8%A1%8C%E7%84%A1%E5%B8%B8%3C%2Fem%3E">14:35:33 <em>諸行無常</em></h3>
<p>特に npm パッケージを使ってるようなものは脆弱性のアラートが上がってしょうがないんで, ばしばしアーカイブしてく.</p>
<h2 id="2-Tue%2007%20Oct%202025">Tue 07 Oct 2025</h2>
<h3 id="3-15%3A37%3A09%20%3Cem%3E%E7%94%98%E3%81%88%3C%2Fem%3E">15:37:09 <em>甘え</em></h3>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://ja.wikipedia.org/wiki/%E3%80%8C%E7%94%98%E3%81%88%E3%80%8D%E3%81%AE%E6%A7%8B%E9%80%A0" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">「甘え」の構造 - Wikipedia</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;"></span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://ja.wikipedia.org/wiki/%E3%80%8C%E7%94%98%E3%81%88%E3%80%8D%E3%81%AE%E6%A7%8B%E9%80%A0" target="_blank" rel="nofollow">ja.wikipedia.org/wiki/%E3%80%8C%E7%94%98%E3%81%88%E3%80%8D%E3%81%AE%E6%A7%8B%E9%80%A0</a></div></div>
    
<p>嫌われたくない人は細かいことを言わないようにする.</p>
<h2 id="2-Wed%2008%20Oct%202025">Wed 08 Oct 2025</h2>
<h3 id="3-15%3A57%3A43%20%3Cem%3E%E5%8B%95%E7%94%BB%E7%94%9F%E6%88%90%E3%81%AF%E6%8E%A8%E8%AB%96%E3%81%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%A8%E3%81%84%E3%81%86%E4%B8%BB%E5%BC%B5%3C%2Fem%3E">15:57:43 <em>動画生成は推論ができるという主張</em></h3>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://video-zero-shot.github.io" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://video-zero-shot.github.io/assets/preview.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">Video models are zero-shot learners and reasoners</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Video models like Veo 3 are on a path to become vision foundation models.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://video-zero-shot.github.io" target="_blank" rel="nofollow">video-zero-shot.github.io</a></div></div>
    
<p>問題を画像に関する問題に変換することで, 動画生成 (i2v) は問題を解決できる, かもしれない.</p>
<ul>
  <li>
    Gravity earth/moon
    <ul>
      <li>羽とボールを落とすとどちらが先に地面に着くか?</li>
    </ul>
  </li>
  <li>
    Colorization
    <ul>
      <li>白黒写真をカラー化する過程</li>
    </ul>
  </li>
  <li>
    Maze
    <ul>
      <li>迷路を解く過程</li>
    </ul>
  </li>
</ul>
<h2 id="2-Thu%2023%20Oct%202025">Thu 23 Oct 2025</h2>
<h3 id="3-16%3A46%3A11">16:46:11</h3>
<p>ポケモン Legends Z-A 始めた. 先週の16日に発売されたんだけど, 17日の金曜日夕方から始めた. 真面目に最後までやるのは長いんだけど, スタッフロールが流れるところまでなら丸2日でクリアした. 早く終わらせるだけなら強いポケモンと取っ替え引っ替えするんだろうけど, ピジョットとどうしても最後まで旅をしたいので.</p>
<p>でも今はピジョットはもう引っ込めて, ジガルデに取って代わられた.</p>
<h3 id="3-16%3A52%3A50">16:52:50</h3>
<p>腕時計ってどうしても邪魔に思えるから付けてなかったんだけど, バイクとかで遠出するときは欲しくなった. というわけで2000円以内で買えるカシオの腕時計を買った. 軽くて薄いので良いかもしれない.</p>

    <footer>
      <p class="is-pulled-right">@cympfh / mail@cympfh.cc</p>
    </footer>
  </div>
  <script src="../../../resources/js/youtube.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="../../../resources/js/toc.js"></script>
</body>
</html>
