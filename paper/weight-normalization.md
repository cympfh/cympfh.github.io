% Weight Normalization
% https://arxiv.org/abs/1602.07868
% 深層学習

## 概要・手法

ディープラーニングにおける 1 層とは、ある重みパラメータ (行列) $W$ を以って
入力ベクトル (あるいは行列) $x$ から

$$y = W*x$$

という操作をすること.
$*$ は行列の掛け算だったり畳み込みだったりする演算.

Weight Normalization は $W$ というパラメータを陽に持たず、代わりに
スカラー $g$ 及び行列 $V$ を持ち、

$$W = g \frac{V}{\|V\|}$$

として使う.
学習も $g, V$ それぞれを更新する.

## メリット

学習の安定化、高速化

学習が全然吹っ飛ばないので学習率を思い切って上げて良い.
本来学習率 $lr$ に対して、学習の速度は

$$lr, lr(LR - lr)$$

という2つの値の大きさで決まる.
ここで $LR$ はある定数で、$lr$ が大きいほど一つ目の値が大きくなる分、早くなるが、
二つ目は小さくなるので遅くなり、ある以上大きくなるとマイナスになって学習が進まなくなる (吹っ飛ぶ).

Weight Normalization では、学習の速度は

$$lr, lr' (LR' - lr')$$

という2つの値の大きさで決まる.
$LR'$ はまた定数.
$lr'$ とは、$lr$ を $\|V\|$ の自乗くらいで割った大きさ.
$\|V\|$ は $W$ に影響しないのに速度には影響を与える.
ある程度大きければ、 $(LR' - lr')$ はマイナスにならないので学習が安定化して、普通よりも思い切って大きい学習率を使っていい.

特長として、学習の中で $\|V\|$ は単調に大きくなるというのがある.

$W$ に $V$ はノルム正規化してから使われてるので、微分したとき、$V$ の接方向 ($V$ と直交方向)
にしか動かないことは自明.
となると $\|V\|$ は大きくなる.

