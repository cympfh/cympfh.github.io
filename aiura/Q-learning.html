<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Q-learning</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../resources/css/c.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-stumbleupon" style="position:absolute; left:0.4em; top:0.4em; width:1.3em;border-radius:0.8em;"></i></a>
</header>
<header>
<h1 class="title">Q-learning</h1>
</header>
<p class="date" style="text-align: right">
2016-01-17 (Sun.) 01:31:22 JST
</p>
<div class="is-pulled-right">
<p><a class='tag is-red' href=index.html#機械学習>機械学習</a></p>
</div>
<p>本文書ではまず <a href="https://ja.wikipedia.org/wiki/Q学習">Q-learning (Q学習)</a> の概要を述べ、次に 二人完全情報ゲームである <a href="https://ja.wikipedia.org/wiki/三目並べ">tic-tac-toe (三目並べ)</a> への適用について説明する. 実際の実装は <a href="https://github.com/cympfh/tic-tac-toe">cympfh/tic-tac-toe</a> にある.</p>
<h2 id="強化学習の概要">強化学習の概要</h2>
<ul>
<li>(普通有限の) 状態集合 <span class="math inline">\(\mathcal{S}\)</span></li>
<li>有限のアクション集合 <span class="math inline">\(\mathcal{A}\)</span></li>
<li>初期状態 <span class="math inline">\(s_0 \in \mathcal{S}\)</span></li>
<li>意思決定機械とは、<span class="math inline">\(f: \mathcal{S} \rightarrow \mathcal{A}\)</span> なる写像 <span class="math inline">\(f\)</span> である</li>
<li>状態遷移とは、その時の状態とアクションに対して次の状態を決定する (決定的な) 関数である
<ul>
<li><span class="math inline">\(\rm{trans}: (\mathcal{S}, \mathcal{A}) \rightarrow \mathcal{S}\)</span></li>
</ul></li>
<li>エージェントとは、状態 <span class="math inline">\(s \in \mathcal{S}\)</span> を一つ持ち、意思決定機械を備え、アクションを行うものである
<ul>
<li>アクションを行うとは、エージェントが持つ状態を <span class="math inline">\(\rm{trans}\)</span> によって遷移させることである</li>
<li>エージェントは状態遷移の際に状態に関して報酬を得る</li>
</ul></li>
<li>報酬とは遷移後状態 (もしかしたら遷移の際にとったアクションも) に依って決定される実数である
<ul>
<li>エージェントの外部 (環境と呼ばれる) が実数関数 <span class="math inline">\(\rm{reward}\)</span> を備えており、エージェントの内部状態 <span class="math inline">\(s\)</span> によって <span class="math inline">\(\rm{reward}(s)\)</span> の値のみをエージェントに伝える</li>
<li><span class="math inline">\(\rm{reward}: \mathcal{S} \rightarrow \mathbb{R}\)</span></li>
</ul></li>
</ul>
<center>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.38.0 (20140413.2041)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="159pt" height="98pt" viewBox="0.00 0.00 159.00 98.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 94)">
<title>
%3
</title>
<!-- s -->
<g id="node1" class="node">
<title>
s
</title>
<ellipse fill="none" stroke="black" cx="27" cy="-72" rx="27" ry="18"/> <text text-anchor="middle" x="27" y="-68.3" font-family="Times,serif" font-size="14.00">s</text> </g> <!-- s&#39; --> <g id="node2" class="node">
<title>
s'
</title>
<ellipse fill="none" stroke="black" cx="124" cy="-45" rx="27" ry="18"/> <text text-anchor="middle" x="124" y="-41.3" font-family="Times,serif" font-size="14.00">s'</text> </g> <!-- s&#45;&gt;s&#39; --> <g id="edge1" class="edge">
<title>
s-&gt;s'
</title>
<path fill="none" stroke="black" d="M52.4649,-65.0468C63.6319,-61.873 77.0402,-58.0623 89.1537,-54.6195"/> <polygon fill="black" stroke="black" points="90.2847,-57.9367 98.9469,-51.8361 88.371,-51.2034 90.2847,-57.9367"/> <text text-anchor="middle" x="75.5" y="-62.8" font-family="Times,serif" font-size="14.00">a</text> </g> <!-- reward --> <g id="node3" class="node">
<title>
reward
</title>
<text text-anchor="middle" x="27" y="-14.3" font-family="Times,serif" font-size="14.00">reward</text> </g> <!-- reward&#45;&gt;s&#39; --> <g id="edge2" class="edge">
<title>
reward-&gt;s'
</title>
<path fill="none" stroke="black" d="M54.2106,-25.4493C65,-28.5158 77.6371,-32.1074 89.1155,-35.3697"/> <polygon fill="black" stroke="black" points="88.2529,-38.763 98.8288,-38.1303 90.1666,-32.0297 88.2529,-38.763"/> </g> </g>
</svg>
</center>
<p>状態 <span class="math inline">\(s\)</span> のときにアクション <span class="math inline">\(a\)</span> をしてみた. エージェントは、その結果として状態が <span class="math inline">\(s&#39;\)</span> に変化したことと、環境から報酬 <span class="math inline">\(r\)</span> が来たことを観測できる.</p>
<p>我々の目的は、エージェントがいい感じに報酬を得るような意思決定機械 <span class="math inline">\(f\)</span> を設計することである. いい感じ、というのは、例えば初期状態から数えて受け取った報酬を順に <span class="math inline">\(r_1, r_2, \ldots\)</span> としたときに</p>
<p><span class="math display">\[\max  \sum_{t=1}^N \gamma^{N-t} r_t  (0 &lt; \gamma &lt; 1)\]</span></p>
<p>基本的には、こういう状態にときにこういうアクションをしたら報酬がめっちゃ来た、というのを覚えておいて、再び同じ状態になったらそれを再実践する、ということが取られる. あんまりそういうのばっかりをすると局所解に陥るので、上手い学習が必要になる. Q学習は、理論的に最適解に落ちることが保証されている.</p>
<h3 id="q-learning-の概要">Q-learning の概要</h3>
<p>Q-learning は強化学習の一手法である. Q-learning ではある時点の状態 <span class="math inline">\(s\)</span> と、その時点から取ることのできるアクション <span class="math inline">\(a\)</span> の組 <span class="math inline">\((s, a)\)</span> に対して良さのようなものを数値として表現する. これは <span class="math inline">\(s\)</span> の時点でアクション <span class="math inline">\(a\)</span> を取ることが自分をどれだけ有利にするかの指標である. これを Q 値と呼び <span class="math inline">\(Q(s, a)\)</span> と書く (quality の &quot;q&quot;?). 初め全ての Q 値は適当に初期化をし、十分回数、ゲームをシミュレーションを繰り返す中でこの Q を最適な値に更新していく. 遷移関数はこの Q 値を使って確率的な振る舞いを与える. すなわち、状態 <span class="math inline">\(s\)</span> の時点でアクション <span class="math inline">\(a\)</span> を取る確率を</p>
<p><span class="math display">\[\pi(s, a) = \frac{\exp \left( Q(s,a) / T \right)}{\sum_{a&#39; \in \mathcal{A}} \exp \left( Q(s, a&#39;) / T \right)}\]</span></p>
<p>で与える. ただしここで <span class="math inline">\(T\)</span> は温度と呼ばれるパラメータで、通常、初めは大きな値にしておき学習が進む中で下げることにする (<span class="math inline">\(T_0 \to 1\)</span>). これは Q 値が大きくなるほど大きな確率でそのアクションを選択するような関数となっている. ただし <span class="math inline">\(T\)</span> が大きい場合は Q 値の差がそれほど顕著に反映されないようになっている. 従って学習が十分でない間は、Q 値の低いアクションも積極的に選択できるように設計されている.</p>
<h4 id="q-の更新">Q の更新</h4>
<p>Q 値は次のように更新 (学習) する. 状態 <span class="math inline">\(s\)</span> の時点でアクション <span class="math inline">\(a\)</span> を選び、状態 <span class="math inline">\(s&#39;\)</span> に遷移し、報酬 <span class="math inline">\(r\)</span> を受け取ったとする (報酬はアクションの遷移直後に受け取ることに註意). このアクション選択の良さ <span class="math inline">\(Q(s, a)\)</span> とは、 <span class="math inline">\(r\)</span> 及び、<span class="math inline">\(s&#39;\)</span> からとり得る最大の Q 値に凡そ比例すべきである. 従って、学習率 <span class="math inline">\(\alpha, \gamma\)</span> という2つのパラメータを以って</p>
<p><span class="math display">\[Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;) \right]\]</span></p>
<p>と、このように更新をする. <span class="math inline">\(\gamma\)</span> は <span class="math inline">\(0 \leq \gamma \leq 1\)</span> であるような定数である. <span class="math inline">\(\alpha\)</span> は学習につれて小さくなるような変数であることが求められる. すなわち時刻 <span class="math inline">\(t\)</span> の関数 <span class="math inline">\(\alpha = \alpha(t)\)</span> であって、しかも次の2条件が満たされる時、Q 値は最適解に収束することが保証されている.</p>
<ol type="1">
<li><span class="math inline">\(\sum_{t=0}^\infty \alpha(t) \to \infty\)</span></li>
<li><span class="math inline">\(\sum_{t=0}^\infty \alpha(t)^2 &lt; \infty\)</span></li>
</ol>
<p>さて、Q学習という素晴らしい古典手法が既に存在するので、意思決定機械はQ学習によって作ることにして、今度の我々の目的は、解きたいタスクを見つけて、それに適した報酬を設定することである. 報酬というのは基本的に、状態 (とアクション) をうまく評価するものでなければならない. ちゃんと数値として決まるようなタスクばかりではない.</p>
<h2 id="q学習で-tic-tac-toe">Q学習で tic-tac-toe</h2>
<p>tic-tac-toe (三目並べ) を学習させることを考えた.</p>
<p>しかしこれは、先ほどの枠組みがそのままは使えない. 先ほどの枠組みは、エージェントは自分一人で、ただ自分がアクションを続けさえすればよかった. これは謂わば、一人用ゲームである. 今度は二人用ゲームである.</p>
<p>普通に考えて、状態とは、3x3の盤面のことである. 初期状態は何も書かれてない3x3であって、マルかバツかが少しずつ埋まってくそれぞれが状態である. 普通に考えて、アクションとは自分一人が打つ手 (マルかバツかを一つ書く行為) のことである. そうすると状態は一つ進む. その状態に対してアクションを行うのは、今度は自分ではなく、相手になる. 報酬は？ どれに対して？</p>
<figure>
<img src="http://i.imgur.com/j7hrmVI.png" />
</figure>
<p>多少AIに甘く、設計した. 上の木はゲームツリーである. ノードは一つの状態を表す. A と書かれたノードは、「次に手を打つのはAIである」盤面に相当する. ノードから生えてるエッジは打てる手に相当する. 3本生えてるのは、打てる手が3つあって、それぞれ、打つとその先の状態に遷移する. 遷移先は黒いノードである. これは、相手の手番であることを意味する. 相手が手を打つと、また自分の手番になる.</p>
<p>本来、報酬を決定するはずの「遷移後の状態」を、次 (黒) と次の次 (A) で決定することにした. 具体的には</p>
<ul>
<li>次 (黒) においてAIの勝ちが決定したら十分な報酬</li>
<li>次 (黒) において引き分けが決定したら僅かにプラスの報酬</li>
<li>次の次においてAIの負けが決定したらマイナスの報酬
<ul>
<li>そのような相手の手が少なくとも1つ存在する</li>
</ul></li>
</ul>
<p>AIの負けが決定するのは、相手の手が決定したあとだからである. しかしながら、プログラムの設計上、相手の手まで待つのは嫌なので、さらに甘い設計をした.</p>
<p>ゲームツリーでいうところの、2ステップ先まですべてシミュレートして、予め報酬を受け取ってしまう. 本来、Q学習の枠組みでは、実際にアクションをとってしまわないと報酬がいくらか知り得ないのだが、まあ、先に報酬を知ったって、学習がスピードアップするだけだし、いいでしょ.</p>
<p>報酬の設定で技巧的な点としては、引き分けでもプラスの報酬があるところである. このゲームは本来、両者が最善を尽くせば引き分けで終わるゲームなのである. また、このゲームは、先手のほうが一手多く打てることが期待できるのでは、ランダムな手を打っていると先手のほうが後手より二倍程度有利でもある. 引き分けについて何も学習をしない場合、先手ばかり勝ち方を学習し、後手は何も学習できず、初手から結局どの手を打っても負けるのだと諦めてしまう.</p>
<p>Q学習というのはゲームツリーの上のDPだと感じた. つまりQ (quality) 関数というのは、ノードから生えてるエッジの重みである. エージェントはその重みに従った確率にしたがって勝手に手を打つので、我々はエッジの重みを正しく設定したい. しかしそのためのヒントは実際にAIが今いるノードと、その2ステップ先までしか分からない. しかもそのノードがゲームの終了状態にあるかどうかだけである (すなわち勝ち負けか引き分けかが決定する状態). 実際にすぐ先にゲームの終了状態があるエッジについては、 さっそくその重みを大幅に調整すればよい. それよりもっと上のエッジについては、何のヒント (報酬) がないので、 更に下のエッジをヒントに、重みを調整する. 逆伝播っぽいし、動的計画法ぽいなと思った.</p>
<h3 id="他">他</h3>
<p>報酬という形で、あまりにも当たり前なルールは教えてしまうことになる. つまり、今の場合、即座に自分が勝てる手があるなら打て (リーチがあるなら). そこに打った後に即座に相手が勝てる手がある場合、そこには打つな.</p>
<p>この程度のルールは教えることになってしまう. なので、何も学習してなくても、ランダムなAIにはかなり勝ててしまう. それでバグがあることに気づかずに数日放置していた... 学習していく過程を調べようと思い当たって初めて、何も学習していないということに気づいて、3つ以上の致命的なバグを発見し、この土曜日を潰した.</p>
<p>引き分けについて報酬を与えることは、かなり恣意的だ. 「勝つならそれは良い手だ」という事実を教えることとかなり程遠い. 将棋AIでは、定石を教えこむということが、たぶん今も、ある. あれは人間が発見した方法を教えることであるが、その人間が正しいとは限らない. 本当に良い方法は、相手の玉を取るのが良い手で、自分の王を取られるのが悪い手である、という2つの事実と、あとは細々とした例外的な禁じ手を教える以外のことをしないで、自然に勝ち筋を発見させることである. もちろん人間を信用するなら定石を教えたほうが早いけど.</p>
<p>引き分けも悪いものではないと教えたのは、このゲームを学習した収束値が引き分けだと私が偶然知っていたからである. しかしそれは人間の思い違いかもしれないのだ.</p>
<!--

  HTML として pandoc -B で include する.

  <H2> を列挙してそれらにリンクを貼った toc を id='toc' に埋め込む.
  markdown で書いてるだろうから例として次のような段落を書けばよい.

```
## INDEX
<div id=toc></div>
```

  used in
  - /memo/gnuplot
  - /memo/linux
  - /memo/imagemagick

-->
<script>
(function() {
  var sections = document.getElementsByTagName('h2');
  var i;
  var OL = document.createElement('ol');
  for (i=0; i < sections.length; ++i) {
    var LI = document.createElement('li');
    var A = document.createElement('a');
    A.innerHTML = sections[i].innerHTML;
    if (A.innerHTML.toUpperCase() == 'INDEX') continue;
    A.href = '#' + i;
    LI.appendChild(A);
    OL.appendChild(LI);

    var PREF = document.createElement('a');
    PREF.name = i;
    sections[i].appendChild(PREF);
  }

  var done = false;
  function work() {
    if (done) return;
    if ( document.getElementById('toc') === null) return; // no toc element
    document.getElementById('toc').appendChild(OL);
    done = true;
  };

  window.onload = work;
  setTimeout(work,800);
}());
</script>
</body>
</html>
