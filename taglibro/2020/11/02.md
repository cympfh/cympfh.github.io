% Mon Nov 02 2020

$\def\L{\mathcal L}\def\bar#1{\overline{#1}}$

## 自然言語

文として書き下すことが出来る自然言語を, 次のような数学的対象に簡易的にモデル化する.

自然言語 $\L$ は次の構造からなる.

- 単語集合 $V$
- 有限長の $V$ の列集合 $[V]$
    - $[V] = \bigcup_{i=0}^\infty V^i$
        - $u \in V^i$ は $V$ の要素をちょうど $i$ 個並べた列のこと
    - この単語の列のことを **文** と呼ぶ
    - 特に長さ $0$ の列を $\epsilon$ と書いて **空文** と呼ぶことにする
- 意味空間 $\Omega$
    - 今は単に集合であるとだけ定める
- 解釈関数 $\sigma \colon [V] \to \Omega + \{ \bot \}$
    - ここで $+$ は直和
    - $\bot$ は $\bot \not\in \Omega$ なる特別な記号
    - 文 $s \in [V]$ について $\sigma(s) = \bot$ のとき, $s$ を **非文** と呼ぶ
        - 非文ではない文を **正文** と呼ぶことにする（今私が考えた）
        - 単に「文」という場合は正文も非文も含む
- 非文ではない文だけを集めた集合を $L$ とする
    - 解釈関数は定義域を $L$ に制限することで自然に $\sigma \colon L \to \Omega$ に出来る

以上の構造を持った $\L(V, L, \Omega, \sigma)$ を自然言語と呼ぶ.

## 意味空間

自然言語 $\L$ には何とも定まってない意味空間 $\Omega$ がある.
ここに数理的な演算が出来るような拡張をほどこした空間 $X$ を代わりに構成することを考える.

$\Omega$ に出来なくて $X$ に出来るような違いとして次がある:

- 非文の意味が取れる
    - $\Omega$ への割当 $\sigma$ は非文の意味が取れない
    - 特に1単語の意味を $\Omega$ では考慮することができない
        - ある人がただ「猫」と発言したものをどう解釈すればよいだろうか？
- 意味の合成が出来る
    - $X$ に合成演算をもたせる
    - 文は結局, 単語や文の断片を結合させた列である
        - 単語や文の断片（それらは非文）に割り当てた意味の合成にそれを対応させたい

### 意味の割当

非文も含めた全ての文 $[V]$ から $X$ への写像を意味割当と呼ぶ.
割当関数を上付き線で表すことにする.
$$[V] \to X$$
$$u \mapsto \bar{u}$$

特に $V \simeq V^1 \subset [V]$ に割り当てる意味を **単語の意味** と呼ぶことにする.
$$a \in V \mapsto \bar{a} \in X$$

### 意味合成

合成演算 $\circ$ は $X \times X \to X$ なる二項演算子で, 意味の合成を表す.
つまり, 2つの文 $u_1, u_2 \in [V]$ について, これらを列結合した文 $u = u_1 + u_2$ の意味は次を満たす.
$$\bar{u} = \bar{u_1} \circ \bar{u_2}$$

以降で曖昧性のない限り, この $\circ$ は略して単に
$\bar{u} = \bar{u_1} \bar{u_2}$
のように書く.
ちょうど実数の掛け算を表記するのにいちいち $2 \cdot x$ とか $2 \times x$ とか書く代わりに $2x$ と書くのと同様に.

#### 意味合成の単位元

空文 $\epsilon$ の意味 $\bar{\epsilon}$ のことを $1 \in X$ という記号で表すことにする.

これは意味合成に関する単位元になっていてるべきだ.
なぜなら, $u + \epsilon = u = \epsilon + u$ であるから.
というわけで $1$ は
$$\forall x \in X, x \circ 1 = x = 1 \circ x$$
を満たす.

以上から $X$ は意味合成に関してモノイド構造を持つ.

### 意味の一貫性

文 $u$ が非文でないとき, $\sigma(u) \in \Omega$ という意味と, $\bar{u} \in X$ という意味とがある.
このときは同じ意味だとしてしまったほうが良さそう.
$$u \in L \implies \sigma(u) = \bar{u}$$
もっとも, $\Omega, \sigma$ の中身がどんなものか何も言っていないので, この主張は何も言っていないのと同じだ.

### 距離位相

直感的な説明として, 文 $u$ と $v$ の意味には近い遠いがある.
そこで $X$ に距離 $d$ を入れて, 近さを表現する.
$$d \colon X \to \mathbb R_{\geq 0}$$

2つの文 $u,v$ について
$d(u,v)=0$
のとき, 2つは全く同じ意味を持つという.

ただし現実の自然言語を考えると,
字面が違うのに全く同じ意味を持つことなんてそうそうないだろうから,
意味は同じか異なるかではなくて, 近い遠いという分類で考えることのほうが重要だ.

### 破綻した非文

非文にも意味を与えたい理由は, そこにさらに単語をいくつか付け加えれば文として意味が通るようにできるからだ.
しかし, あまりにもデタラメな単語を並べすぎると, それ以上単語を付け加えても文になりようがない.
そのような非文を **破綻文** と呼ぶことにしよう.
字面がまるで違っても破綻文はもはや解釈不能であるという点で同じ意味としてしまおう.
やはり特別な記号 $\bot \in X$ をその意味ということにする.
ここに任意の意味を合成しても尚破綻したままなので,
$$\forall x \in X, x \circ \bot = \bot = \bot \circ x$$
が成り立つ.

## word2vec への適用

現実の実装はここに書いた限りではないどころか全然違うだろうが,
強引に当てはめれば次のように言えるかもしれない.

### 意味空間

意味空間 $X$ として, 実ベクトル空間 $\mathbb R^m$ を用いる.
$$u \in L \mapsto \bar{u} \in \mathbb R^m$$
実際には合成があるので,
文の意味は単語の意味の合成によって求まるので単語の意味割当だけ決めれば十分.
その割り当て方については論文を読んでくれ.

### 意味合成

必ずしも決まっていないが, ただの加算を用いることが多い.
$$\bar{u} \circ \bar{v} := \bar{u} + \bar{v}$$
これの単位元はもちろんゼロベクトルになる.
$$\bar{\epsilon} = 0$$

また, 距離のとり方にもよるが, 単語数で割った単純平均を使う場合もあるようだ.

> 細かいことを言うと, 平均の場合は平均値だけから二項演算として定義することは出来ない.
例えば実数の平均だと $1 \circ 2 \circ 3 = 2$ （3つの値の単純平均）になるが,
これは結合則を満たさない（例えば $(1 \circ 2) \circ 3 = 1.5 \circ 3 = 2.25$）.
いくつの平均かという情報が失われてるのが問題だから, 平均値の代わりに, 何個の単語であるかと, 合計値を持つことにすればいい.
空間を $X = \mathbb N \times \mathbb R^m$ に修正して,
$$(m, x) \circ (n, y) := (m+n, x+y)$$
とすればいい.
単位元は $(0, 0)$ になる.
距離を計算するときには $(m,x)$ から平均値 $x / m$ を計算して使えばよい.

どちらにせよ, 文中の単語の順序は一切考慮されないことになる.

ただしNLPの常套テクニックとして,
文の始まりを表す特別なトークン `BOS`
及び終わりを表すトークン `EOS` を用意して,
それらにはゼロでないベクトルを割り当てるのが普通だ.
これに対しては特に修正は必要なく, 普通に単語集合 $V$ にこの2つを加えて,
`BOS` で始まって `EOS` で終わる文を正文であることの条件に加えれば良い.

### 意味の距離

ベクトルなどと言っているが,
結局ユークリッド空間上の点に過ぎない.
cos 距離 $1 - \cos(u,v)$ とか,
L2 距離 $\| u-v \|^2_2$ などが使われる.

ただし,
cos 類似度を計算する場合はゼロベクトルだと何も計算が出来ないので,
$d(u,0) = \infty (u \ne 0)$
などとしておく.

cos 距離だと `dog` という文と `dog dog` という文は同じ意味になる.
合成を平均でやる場合は当然だが, 加算であっても,
これは定数 $k>0$ に関して
$\cos(u,kv) = \cos(u,v)$
が成り立つため, やはり同じ意味になる.

### ゼロベクトルの扱い

現実の実装では $X = \mathbb R^m \setminus \{0\}$ としてしまうことが多い.
非文に意味を考えないといけないケースがまず無いからである.
その場合に合成の結果ゼロベクトルが作られないことが要求されるが, そんな保証は無いことが普通である.

ところで Skip-gram でゼロベクトルを入れるとどうなるだろうか.
このモデルのパラメータは行列 $W$ とバイアスベクトル $b$ であって, 単語に意味 $x \in \mathbb R^m$ が割り当てられるとき,
$$p = \mathrm{softmax}(Wx+b)$$
がその単語の周辺に出現する確率分布を表している（逆に言えばこれが成り立つように $W,b$ 及び $x$ を獲得している）.
$x=0$ を代入すれば,
$$p_0 = \mathrm{softmax}(b)$$
を手に入れる.

さて, ゼロベクトルは空文の意味ベクトルであった.
空文の周辺にどんな単語が出現するかは, 自由な箇所に単語が出現する確率なので, 言語全体で単語が出現する確率に過ぎない.
データセット上で単語をカウントすることで $p_0$ は尤度推定される.
これによって $b$ は決定される.

### 非文の扱い

非文は入力しないということで全くサポートされない.

## 圏化

色々妄想を書いたが, 結局ほとんどモノイド構造に毛を生やしただけのものだと言っているに過ぎない.
次のように至極単純に書き下す事ができる.

### 言語の圏

自然言語 $X$ の圏とは次のようなもの.

- 対象は唯一の $X$
- この言語から生成される文とは射 $X \to X$ のこと
    - 文の結合が射の合成に対応する
    - 特に恒等射 $1$ は空の文に対応する
    - また破綻文に対応する特別な射 $\bot \colon X \to X$ があって,
        - $\forall x, x \circ \bot = \bot \circ x = \bot$

**疑問**:
距離は？
実数への圏（とは？）への関手を考えるとよい？
位相群の圏化とか調べたらそれがそのまま使えそう.

### 翻訳関手

自然言語 $X$ の圏から $Y$ への翻訳とは, 関手 $F \colon X \to Y$ のことだと言える.
ここで準同型として特に

- $F1 = 1$
- $F \bot = \bot$

を要請する.

**疑問**:
$X$ の正文については $F$ は正しくマッピングされることが期待されるけれど,
どんな非文でもマッピングすることまで出来るだろうか？
