<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="ja" xml:lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="description" content="Tue 04 Mar 2025, Wed 05 Mar 2025, Thu 06 Mar 2025, Fri 14 Mar 2025, Wed 26 Mar 2025, Tue 08 Apr 2025, Mon 14 Apr 2025, Tue 15 Apr 2025, Thu 17 Apr 2025, Mon 21 Apr 2025, Python のインストールとバージョン管理, ライブラリの管理, Tue 22 Apr 2025, Wed 23 Apr 2025, Thu 24 Apr 2025, Fri 25 Apr 2025, /opt/wsl.port.sh, Sun 27 Apr 2025, Mon 28 Apr 2025, Wed 30 Apr 2025" />
  <meta name="og:url" content="http://cympfh.cc/taglibro">
  <meta name="og:title" content="月報 2025&#x2F;03, 2025&#x2F;04" />
  <meta name="og:description" content="Tue 04 Mar 2025, Wed 05 Mar 2025, Thu 06 Mar 2025, Fri 14 Mar 2025, Wed 26 Mar 2025, Tue 08 Apr 2025, Mon 14 Apr 2025, Tue 15 Apr 2025, Thu 17 Apr 2025, Mon 21 Apr 2025, Python のインストールとバージョン管理, ライブラリの管理, Tue 22 Apr 2025, Wed 23 Apr 2025, Thu 24 Apr 2025, Fri 25 Apr 2025, /opt/wsl.port.sh, Sun 27 Apr 2025, Mon 28 Apr 2025, Wed 30 Apr 2025" />
  <meta name="og:image" content="http://cympfh.cc/resources/img/identicon.png" />
  <meta property="og:url" content="http://cympfh.cc/taglibro">
  <meta property="og:title" content="月報 2025&#x2F;03, 2025&#x2F;04" />
  <meta property="og:description" content="Tue 04 Mar 2025, Wed 05 Mar 2025, Thu 06 Mar 2025, Fri 14 Mar 2025, Wed 26 Mar 2025, Tue 08 Apr 2025, Mon 14 Apr 2025, Tue 15 Apr 2025, Thu 17 Apr 2025, Mon 21 Apr 2025, Python のインストールとバージョン管理, ライブラリの管理, Tue 22 Apr 2025, Wed 23 Apr 2025, Thu 24 Apr 2025, Fri 25 Apr 2025, /opt/wsl.port.sh, Sun 27 Apr 2025, Mon 28 Apr 2025, Wed 30 Apr 2025" />
  <meta property="og:image" content="http://cympfh.cc/resources/img/identicon.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@cympfh" />
  <meta name="generator" content="unidoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>月報 2025&#x2F;03, 2025&#x2F;04</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
  </style>
  <link rel="stylesheet" href="../../resources/css/c.css" />
  <link rel="stylesheet" href="../../../resources/css/c.css" />
  <link rel="stylesheet" href="../../../resources/css/youtube.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" />
  <link href="https://unpkg.com/prismjs@1.x.0/themes/prism.css" rel="stylesheet" />
  <script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="taglibro">
    <header class="page-header"><a href='../../index.html'><i class="fas fa-fast-backward"></i></a></header>
<h1 class="title" id="1-%E6%9C%88%E5%A0%B1%202025%2F03%2C%202025%2F04">月報 2025/03, 2025/04</h1>
<h2 id="2-Tue%2004%20Mar%202025">Tue 04 Mar 2025</h2>
<h3 id="3-14%3A46%3A01">14:46:01</h3>
<ul>
  <li>
    DeepResearch
    <ul>
      <li>行列分解を全部まとめる</li>
      <li>Reparametrization Trick を全部まとめる</li>
    </ul>
  </li>
</ul>
<h2 id="2-Wed%2005%20Mar%202025">Wed 05 Mar 2025</h2>
<h3 id="3-16%3A51%3A23">16:51:23</h3>
<p>行列分解系の手法を全部まとめたい. そして読めてなかったものがあったら拾っておきたい.</p>
<p>前々から思ってたんだけど, そういえば ChatGPT に Deep Research が来てたんでこれ使ってみた. 内容の正しさの精査はしてない. モデルは o3-mini-high.</p>
<p>論文URLは別の論文だったりとっくにリンク切れであることがよくあったので結局全部手動で調べ直した. こうなると内容の正しさも怪しい. 本当に主要なものだけだし, アレが入ってないのか (EASE) てのもある.</p>
<ul>
  <li>
    1990年 – Latent Semantic Indexing (LSI)
    <ul>
      <li>Deerwesterらによる手法で、文書‐語彙行列に特異値分解 (SVD) を適用して低次元の「潜在意味」空間を抽出し、情報検索において文書間の意味的類似度を捉えることに成功しました。このLSIは、機械学習における行列分解の黎明期の応用例であり、基本概念となる次元圧縮（主成分分析に類似）の威力を示しました。</li>
      <li><a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</a></li>
    </ul>
  </li>
  <li>
    1999年 – 確率的潜在意味解析 (PLSA)
    <ul>
      <li>Hofmannによって提案された手法で、LSIを確率モデルに発展させたものです。文書と語彙の共起行列を確率的に分解し、各文書を潜在トピックの混合で表現します。PLSAは情報検索やレコメンダにも適用され、行列分解を統計的枠組みで解釈する先駆けとなりました（後のLDAにつながるモデルです）。</li>
      <li><a href="https://dl.acm.org/doi/10.1145/312624.312649">Just a moment...</a></li>
    </ul>
  </li>
  <li>
    1999年 – 非負値行列因子分解 (NMF)
    <ul>
      <li>LeeとSeungにより提案された手法で、行列要素に非負の制約を課して因子分解を行います。画像や音声信号などのデータに対し、パーツ（部分）ごとの表現を学習できる点が特長で、顔画像を分解して目や鼻など部分的特徴を捉えることに成功しました。NMFは推薦システム以外にも画像処理やバイオインフォマティクスなど幅広い分野で用いられています。</li>
      <li><a href="https://www.nature.com/articles/44565">Learning the parts of objects by non-negative matrix factorization | Nature</a></li>
      <li><a href="http://belohlavek.inf.upol.cz/vyuka/Lee-Seung-NMF-1999-p.pdf">http://belohlavek.inf.upol.cz/vyuka/Lee-Seung-NMF-1999-p.pdf</a></li>
    </ul>
  </li>
  <li>
    2000年 – 行列分解による協調フィルタリング
    <ul>
      <li>Sarwarらはユーザ-アイテム評価行列に対してSVDによる次元削減を適用し、大規模レコメンダの精度と計算効率を向上できることを示しました。従来のメモリベース手法に比べ、行列分解による潜在要因モデルがスケーラビリティと高精度を両立できることを実証し、以降の推薦システム研究の基盤となりました。</li>
      <li><a href="https://files.grouplens.org/papers/webKDD00.pdf">https://files.grouplens.org/papers/webKDD00.pdf</a></li>
    </ul>
  </li>
  <li>
    2003年 – 潜在ディリクレ配分法 (LDA)
    <ul>
      <li>Bleiらによって提案された階層ベイズモデルで、PLSAを拡張しトピック分布と単語分布にディリクレ事前分布を導入したものです。各文書を潜在トピックの確率混合で表現し、完全な生成モデルとしてテキストの潜在構造を捉えます。LDAはトピックモデルとして自然言語処理で画期的な成果を上げ、行列分解的手法をベイズ的に解釈・適用した代表例です。</li>
      <li><a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</a></li>
    </ul>
  </li>
  <li>
    2004年 – マックスマージン行列分解 (MMMF)
    <ul>
      <li>Srebroらは行列分解を大-margin学習の枠組みに位置付け、低ランク行列の学習においてランク制約の代わりにノルム正則化（トレースノルム）を用いる手法を提案しました。これは行列完成問題に理論的保証を与える方向で画期的であり、大規模データに対しては計算コストが課題でしたが、後の核ノルム最小化による行列完成理論にも影響を与えました。</li>
      <li><a href="https://home.ttic.edu/~nati/Publications/MMMFnips04.pdf">https://home.ttic.edu/~nati/Publications/MMMFnips04.pdf</a></li>
    </ul>
  </li>
  <li>
    2006年 – Funkの行列分解モデル
    <ul>
      <li>Netflix賞を契機に、Simon Funkがブログ投稿で紹介した手法です。ユーザ評価行列を二つの低次元行列に分解し、確率的勾配降下法でパラメータを学習する実装を公開しました（Funk SVD）。このモデルにより、従来の最近傍法を大きく上回る精度向上が報告され、行列分解モデルの有効性が広く認知される転機となりました。</li>
      <li><a href="https://github.com/gbolmier/funk-svd">GitHub - gbolmier&#x2F;funk-svd: :zap: A python fast implementation of the famous SVD algorithm popularized by Simon Funk during Netflix Prize</a></li>
    </ul>
  </li>
  <li>
    2007年 – 確率的行列分解 (PMF)
    <ul>
      <li>MnihとSalakhutdinovにより提案された手法で、評価行列の各要素を確率的生成モデルで説明します。ユーザとアイテムに対応する潜在要因にガウス事前分布を置き、観測された評価のみを対数尤度に含めて学習することで、大規模かつスパースなデータでも高精度に推薦を行えることを示しました。</li>
      <li><a href="https://papers.nips.cc/paper_files/paper/2007/hash/d7322ed717dedf1eb4e6e52a37ea7bcd-Abstract.html">Probabilistic Matrix Factorization</a></li>
    </ul>
  </li>
  <li>
    2008年 – ベイズ確率的行列分解 (BPMF)
    <ul>
      <li>PMFをさらに拡張し、完全ベイズ推論とマルコフ連鎖モンテカルロ (MCMC) によって潜在要因の不確実性まで推定する手法がSalakhutdinovとMnihにより導入されました。モデルの複雑度（次元数）を自動調整できる利点があり、極度にスパースな状況でも高い汎化性能を示しました。</li>
      <li><a href="https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf">https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf</a></li>
    </ul>
  </li>
  <li>
    2008年 – SVD++
    <ul>
      <li>KorenらはNetflixデータを用いた研究で、評価履歴だけでなく非評価の行動履歴（暗黙的フィードバック）も組み込んだ拡張行列分解モデルSVD++を提案しました。ユーザが評価していない閲覧や購入履歴を潜在要因に反映させることで精度を向上し、明示的評価と暗黙的フィードバックの両方を活用する手法です。</li>
      <li><a href="https://dl.acm.org/doi/10.1145/1401890.1401944">Just a moment...</a></li>
    </ul>
  </li>
  <li>
    2008年 – Collective Matrix Factorization
    <ul>
      <li>SinghとGordonは、複数の関連する行列（例：ユーザ‐アイテム評価行列とユーザ属性行列など）を共同で因子分解する手法を提案しました。エンティティが複数の行列にまたがって登場する場合に潜在表現を共有することで、データ間の関係性を学習します。これにより、レコメンドにおける補助情報の統合や、知識発見・リンク予測などマルチ関係データへの行列分解の応用が促進されました。</li>
      <li><a href="https://www.cs.cmu.edu/~ggordon/singh-gordon-kdd-factorization.pdf">https://www.cs.cmu.edu/~ggordon/singh-gordon-kdd-factorization.pdf</a></li>
    </ul>
  </li>
  <li>
    2009年 – Bayesian Personalized Ranking (BPR)
    <ul>
      <li>Rendleらは、暗黙的フィードバックからのアイテムランキングに特化した新たな最適化手法BPRを導入しました。これは行列分解モデルにおいて、ユーザが実際に好んだアイテムが好まなかったアイテムよりも順位が高くなるようなペアワイズな確率的ランキング損失を定義し、サンプリングを用いたSGDで学習します。その結果、評価値の予測ではなくランキング指標の最適化に直接焦点を当てることで、トップN推薦の精度向上に成功しました。</li>
      <li><a href="https://arxiv.org/abs/1205.2618">[1205.2618] BPR: Bayesian Personalized Ranking from Implicit Feedback</a></li>
    </ul>
  </li>
  <li>
    2009年 – 時間要因を組み込んだ行列分解
    <ul>
      <li>Korenは協調フィルタリングにおける時間的変動をモデルに取り入れました。TimeSVD++などと呼ばれる手法では、ユーザの嗜好やアイテムの人気度が時間とともに変化する現象を捉えるために、ユーザバイアス項や潜在因子に時間依存項を導入しています。Netflixの大規模データで検証した結果、時間による評価値のドリフトをモデル化することで精度が飛躍的に向上しました。</li>
      <li>https://dl.acm.org/doi/10.1145/1401890.1401944</li>
    </ul>
  </li>
  <li>
    2010年 – ファクタリゼーションマシン (FM)
    <ul>
      <li>Rendleによって発表された手法で、SVMと行列分解モデルの利点を統合した汎用的予測モデルです。特徴間の2次相互作用を低次元の潜在因子で表現することで、ユーザIDやアイテムID以外の任意の特徴（文脈情報や属性など）を含むデータに適用可能です。FMは入力の特徴設計次第で行列分解モデルやSVD++を含む様々なモデルを再現できる柔軟性を持ち、推薦システムのみならず広告のクリック予測など幅広い分野で応用されています。</li>
      <li><a href="https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf">https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf</a></li>
    </ul>
  </li>
  <li>
    2011年 – RESCAL（知識グラフのためのテンソル分解）
    <ul>
      <li>Nickelらは知識ベース上の多関係データに対し、三次元テンソルの要素分解によってエンティティと関係の潜在表現を学習するRESCALを提案しました。例えば「主体‐述語‐客体」の三項関係を持つ知識グラフを三次元の隣接テンソルとして表現し、それを因子分解することで各エンティティのベクトル表現と関係ごとの行列表現を学習します。RESCALは既存の関係学習手法を上回る精度でリンク予測を達成し、行列分解の概念を知識グラフや多関係学習へ拡張した先駆的研究です。</li>
      <li><a href="https://www.cip.ifi.lmu.de/~nickel/data/slides-icml2011.pdf">https://www.cip.ifi.lmu.de/~nickel/data/slides-icml2011.pdf</a></li>
    </ul>
  </li>
  <li>
    2015年 – AutoRec (オートエンコーダを用いた協調フィルタ)
    <ul>
      <li>Sedhainらはユーザ評価の欠損行列に対してオートエンコーダを適用するAutoRecを提案しました。ユーザ（またはアイテム）ごとの評価ベクトルを入力と出力に再現する自己符号化器を訓練し、その隠れ層を潜在表現とすることで非線形な行列分解を実現しています。実験では従来の行列分解手法やRBMベースのモデルを上回る推薦精度を示し、ニューラルネットによる行列分解の有効性を示しました。</li>
      <li><a href="https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf">https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf</a></li>
    </ul>
  </li>
  <li>
    2017年 – ニューラル協調フィルタリング (NCF)
    <ul>
      <li>Heらは行列分解モデルにおけるユーザとアイテムの内積を多層パーセプトロン(MLP)で置き換えるNCFを提案しました。GMF（汎用行列分解）とMLPを組み合わせたNeuMFモデルでは、ユーザ・アイテム間の非線形な相互作用を学習でき、従来の線形内積モデルを包含・拡張するフレームワークとなっています。実験ではNeuMFが従来手法を上回る精度を示し、深層学習により行列分解モデルの表現力を高めた例として注目されました。</li>
      <li><a href="https://arxiv.org/abs/1708.05031">[1708.05031] Neural Collaborative Filtering</a></li>
    </ul>
  </li>
  <li>
    2018年 – Variational Autoencoder for CF
    <ul>
      <li>Dawen Liangらは変分オートエンコーダ(VAE)を用いた協調フィルタリング手法を発表しました。特に暗黙的フィードバックの確率モデル化に焦点を当て、評価の有無を多項分布とみなした生成モデル（マルチノミアルVAE）で訓練を行います。線形な潜在要因モデルの表現力の限界を超える非線形確率モデルにより、先行する既存のニューラルネット手法や行列分解手法を大きく上回る推薦精度を達成しました。</li>
      <li><a href="https://arxiv.org/abs/1802.05814">[1802.05814] Variational Autoencoders for Collaborative Filtering</a></li>
    </ul>
  </li>
  <li>
    2017年 – 幾何学的行列完成 (Graph-based MF)
    <ul>
      <li>Montiらはユーザ間・アイテム間のグラフ構造を取り入れた行列完成手法を提案し、幾何学的ディープラーニングを推薦へ応用しました。ユーザ・アイテムをノードとするグラフ上でグラフ畳み込みネットワーク(GCN)により隣接関係を平滑化しつつ、評価行列を埋めるアーキテクチャを構築しています。これにより従来法を上回る精度を示し、ソーシャルネットワーク情報やアイテム類似度グラフなどを統合した新たな行列分解の方向性を示しました。グラフベースの潜在要因モデルはその後のGraph Neural Networkを用いた推薦手法へと発展していきます。</li>
      <li><a href="https://arxiv.org/abs/1703.01257">[1703.01257] Model Checking Cyber-Physical Systems using Particle Swarm Optimization</a></li>
    </ul>
  </li>
  <li>
    2020年 – LightGCN (グラフ協調フィルタの簡素化)
    <ul>
      <li>HeらはGCNベースの協調フィルタリング手法を簡素化し、本質的要素である隣接ノードからの埋め込み伝播のみに絞ったLightGCNを提案しました。ユーザ・アイテム二部グラフ上で各ノードの埋め込みを層ごとに線形伝播し、全層の埋め込みを重み付き和して最終表現とするシンプルなモデルですが、不要な活性化関数や重み変換を省くことで学習が安定し、先行の複雑なGCNモデルを上回る性能を達成しました。</li>
      <li><a href="https://arxiv.org/abs/2002.02126">[2002.02126] LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</a></li>
    </ul>
  </li>
</ul>
<h2 id="2-Thu%2006%20Mar%202025">Thu 06 Mar 2025</h2>
<h3 id="3-11%3A01%3A53">11:01:53</h3>
<p>夢日記。 ホラーワールドに三人で行く。 ４つくらいのパズルを解いてゴールするらしい。 他の二人とは早い段階で別れてしまい、わたしは一人になって怖かったのでじっと動かないでいたら、とっくにゴールした他の一人が私を迎えに来た。 その人に指南してもらいながら残り2つのパズルを解いた。 パズルは一見して難しいが実はとても簡単だった。 最後の問題は1から60の数字を使った数独。 でも隣に人間の集合写真があってこれが答えと対応していた。 そして一番下の行にいる人間と2番目の行にいる人間に確かに重複があったのでそれで数字が埋まって、そして全てを埋めなくても一部だけわかれば答えられたので、それで完成ということだった。</p>
<p>ゴールして、そして最後までこのワールドに残っていると、 この世界の真理を見せてもらえる。 私以外は実はゴム人間だということだ。 それを最初から知ってた人は、だから、傍若無人にやりたい放題の態度を取っていた。 そして私は2周目に参加することになった。</p>
<h2 id="2-Fri%2014%20Mar%202025">Fri 14 Mar 2025</h2>
<h3 id="3-13%3A20%3A25%20%3Cem%3E%E3%82%A2%E3%83%AB%E3%83%88%E3%83%A9%E3%82%92%E8%B2%B7%E3%81%A3%E3%81%9F%3C%2Fem%3E">13:20:25 <em>アルトラを買った</em></h3>
<p>Vive Ultimate Tracker を買った. VRChat 用として.</p>
<h4 id="4-%E5%8F%82%E8%80%83">参考</h4>
<p>特にセットアップに関しては出来るだけ新しいものを参照してくださいね.</p>
<ul>
  <li><a href="https://note.com/kikjin/n/n389c2ae3d127">アルトラ(VIVE Ultimate Tracker)って実際どうなの？｜きくじん</a></li>
  <li><a href="https://kxn4t.hatenablog.com/entry/2024/02/02/044322">【VRChat】VIVEトラッカー(Ultimate) をQuest3で使ってみた - 一年中こたつ出てる</a></li>
  <li><a href="https://www.youtube.com/watch?v=gsDV_v9CD8Q">【最強トラッカー爆誕！？】手軽に装着できるのに、トラッキング精度が高すぎる変態デバイスVive Ultimate Trackerをレビュー！VRChatのフルトラは、もうアルトラでいいじゃん！！！ - YouTube</a></li>
</ul>
<h4 id="4-%E7%A7%81%E3%81%AE%E5%A0%B4%E5%90%88">私の場合</h4>
<p>発売されてすでに一年経っており, 日本語の記事もだいぶ出てる. 発売されてすぐはそもそも SteamVR に対応してなかったし, 対応してからもしばらくはβ版だったこともあり, その頃と今とでは使い心地や特にセットアップの手間は改善されているそうで, 古い記事を読んでもそこまで役に立たない.</p>
<ul>
  <li>
    必要なもの
    <ul>
      <li>
        アルトラ本体
        <ul>
          <li>トラッカー三個と, ドングル一つのセットを購入した</li>
        </ul>
      </li>
      <li>
        装着する用のベルト
        <ul>
          <li>
            DanceDash 用のを購入した
            <ul>
              <li>めちゃ高いがこれは DanceDash 自体の値段が入ってる</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        もし必要なら Type-A to Type-C ケーブル
        <ul>
          <li>アルトラは全て Type-C に統一されてる</li>
          <li>PC に Type-A ポートしかないなら Type-A と Type-C を繋ぐケーブルが必要</li>
          <li>私は Type-C ポートが一つだけあったので買ってない</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    セットアップ
    <ul>
      <li>本体を充電する</li>
      <li>
        Vive Hub をインストールする
        <ul>
          <li>ドングルを始めてPCに接続したら勝手に Steam 版がインストールされた</li>
        </ul>
      </li>
      <li>
        セットアップの中で勝手に Vive Space Calibration もインストールされた
        <ul>
          <li>内部的には OpenVR SpaceCalibalation をフォークして作られた公式版</li>
          <li>後述するが OpenVR 版をそのまま使うメリットもある</li>
          <li>両立はできないので注意</li>
        </ul>
      </li>
      <li>
        トラッカーをペアリングする
        <ul>
          <li>電源入れて Vive Hub の設定画面を開けばOK</li>
          <li>基本的にはセットアップの指示に従えば問題ない</li>
        </ul>
      </li>
      <li>
        ファームウェアアップデートをする
        <ul>
          <li>
            トラッカーを USB でPCと直接接続する必要あり
            <ul>
              <li>USB2.0 以上なら大丈夫みたい</li>
              <li>古いとか給電専用ケーブルだとアップデートボタンが押せないので注意</li>
            </ul>
          </li>
          <li>
            トラッカー一つずつアップデートする
            <ul>
              <li>一つを繋いでアップデートして, を三回行った</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        部屋のスキャン
        <ul>
          <li>
            マスターとなるトラッカー一つを選んで行う
            <ul>
              <li>5分か10分は掛かる</li>
              <li>昔よりは改善されたらしいよ</li>
            </ul>
          </li>
          <li>
            私は一箇所だけスキャン精度が悪いままだが気にしてない
            <ul>
              <li>使ってる間に勝手に改善されるそう</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        SteamVR の設定でトラッカーの役割を割り当てる
        <ul>
          <li>これはやらなくてもいい</li>
          <li>
            私は一応設定してるけど
            <ul>
              <li>設定では「足首」にした状態で, 実際には足首に取り付けたり足の甲に取り付けても問題なかった</li>
              <li>
                足に付けてたのを肘に付けてみたりもしたが, VRChat でキャリブレーションし直したら勝手に認識された
                <ul>
                  <li>VRChat がすごいのかな</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<p>これくらいかな. 初めてのアプリケーションなので戸惑ったが, 実際に面倒くさいのは部屋のスキャンくらいだけだった.</p>
<ul>
  <li>
    SpaceCalibalation について
    <ul>
      <li>
        2つある
        <ul>
          <li>両立はできないのでどちらかだけでキャリブレーションすること</li>
        </ul>
      </li>
      <li>
        公式の Vive SpaceCalibalation
        <ul>
          <li>内部的には OpenVR SpaceCalibalation</li>
          <li>設定できる項目が少ない</li>
          <li>
            コントローラ（右または左）といっしょにトラッカーを八の字することでキャリブレーションする
            <ul>
              <li>
                スケール補正は必須
                <ul>
                  <li><code>Enable scaled compansation</code> という名前のチェックボックスがこっそりある</li>
                  <li>必ずチェックすること</li>
                  <li>これをしないと, トラッカーの移動距離と頭の移動距離がズレる</li>
                </ul>
              </li>
              <li>
                八の字は思ったより大きく動くこと
                <ul>
                  <li>
                    たぶんこれによって上に書いた移動距離を計測してるので
                    <ul>
                      <li>それでも補正はしたほうがいい</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        OpenVR SpaceCalibalation
        <ul>
          <li>Vive 公式があるより前から有志に作られたもの</li>
          <li>キャリブレーションしたあとに, そのキャリブレーションを手動で修正することができる</li>
          <li>
            頭にトラッカーを一つ付け足すことで, 八の字キャリブレーションが不要になる
            <ul>
              <li>これが大きい</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<p>八の字キャリブレーションを面倒と感じるか, トラッカーを買い足すかどうかで選ぶことになる. 私は Vive 公式で八の字してる.</p>
<ul>
  <li>
    精度について
    <ul>
      <li>良いね</li>
      <li>
        私がちゃんと経験したのは Haritora Wireless と HaritoraX2 だけで比較対象としては弱いけど
        <ul>
          <li>これらと比較して本当に良い</li>
        </ul>
      </li>
      <li>
        それでもたまにはトラッカーが飛ぶ
        <ul>
          <li>右足だけあらぬ場所に存在してることがある</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    トラッカーがズレたときにやること
    <ul>
      <li>
        ズレたトラッカーだけ取り外して以下を順に試す
        <ul>
          <li>胸の位置に掲げてみる</li>
          <li>カメラを覆い被せてわざとトラッキングを一度失敗させる</li>
          <li>
            そのトラッカーだけを再起動する
            <ul>
              <li>面倒くさいね</li>
            </ul>
          </li>
          <li>
            放って置く
            <ul>
              <li>いつの間にか治ることもある</li>
            </ul>
          </li>
          <li>
            諦める
            <ul>
              <li>ズレるといってもそのズレ方も厳密（例えば正しい位置より何センチだけちょうど右にあるとか）なのでTポーズキャリブレーションだけやり直しちゃえば実は困らない</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<p>アルトラはワンタッチで土台から取り外しが出来るので, そこだけが救いだ.</p>
<ul>
  <li>
    他のトラッカーと混ぜることについて
    <ul>
      <li>問題ない</li>
      <li>私が持ってるのが Haritora だけなので, 試したのはそれだけだけど</li>
      <li>
        HaritoraX2 の肘トラッカーだけ電源を入れて Shiftall VRManager を起動してみた
        <ul>
          <li>ちゃんと肘だけが追加された</li>
          <li>めちゃ良い</li>
          <li>Haritora 伝統の五分おきキャリブレーションも必須になるわけだが</li>
        </ul>
      </li>
      <li>
        さらに胸を追加してみた
        <ul>
          <li>
            横を向けば体が横向きに捻れる
            <ul>
              <li>いいね</li>
              <li>でも IMU 方式 (つまりHaritora) は特に捻じれ方向にドリフトしやすいので, たぶんもう使わない</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<p>HaritoraX2 （肘以外）はそのうちメルカリに出そうと思う.</p>
<ul>
  <li>
    追加で試そうとしてること
    <ul>
      <li>
        VirtualDesktop の仮想トラッキングと混ぜる
        <ul>
          <li>Meta Quest なら出来るらしい</li>
        </ul>
      </li>
      <li>
        充電用に Type-C をマグネットに変換するやつを使う
        <ul>
          <li>
            抜き差しする代わりに Apple 製品にあるマグセーフみたいなやつに変換してくれるやつ
            <ul>
              <li>とりあえず充電は出来てる</li>
              <li>
                マグネットがトラッキング精度に悪影響を与えないかどうかが未確認
                <ul>
                  <li>(追記) 全く問題なかった</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2 id="2-Wed%2026%20Mar%202025">Wed 26 Mar 2025</h2>
<h3 id="3-20%3A54%3A50">20:54:50</h3>
<p>忙しすぎて, どんくらい忙しいかというと, 先週の水曜日のダウンタウンを配信終了一時間前に慌てて見てるくらい.</p>
<p>3/2 に初めて交通違反をしてしまった. 正真正銘初めてで, 次の更新でゴールド免許剥奪ということになる. 6000円の反則金も, 納めるのに自分を納得させるのに時間がかかった. ちゃんとした名前を忘れたけど, 慌てて右折レーンに入ろうとしたら区分線がオレンジ色になっていて, ちょうど白バイに見られていた.</p>
<p>悲しい.</p>
<h2 id="2-Tue%2008%20Apr%202025">Tue 08 Apr 2025</h2>
<h3 id="3-17%3A33%3A04%20%3Cem%3E%E3%82%A2%E3%83%AB%E3%83%88%E3%83%A9%E3%82%92%E8%BF%BD%E5%8A%A0%E3%81%A7%E8%B2%B7%E3%81%A3%E3%81%9F%3C%2Fem%3E">17:33:04 <em>アルトラを追加で買った</em></h3>
<p>アルトラ (Vive Ultimate Tracker) の3+1セットを購入して使い始めて, もうすぐ一ヶ月になる. ほぼ毎日使い倒してる.</p>
<p>3+1 セットというのはトラッカー3つとドングル1つのセット. 9万1千円で手に入る. 「9万1千円のセットを1つ買えば上等なフルトラが手に入る」というのは本当に強いと思う. IMU方式を2つ買えば結局10万円になるからね.</p>
<ul>
  <li>
    良い
    <ul>
      <li>
        トラッキング精度が本当に良い
        <ul>
          <li>遅延もほぼない</li>
          <li>足が10cm動いたらVRの中でも即座に10cm動く</li>
        </ul>
      </li>
      <li>
        毎日のセットアップは簡単
        <ul>
          <li>
            最初に８の字キャリブレーションだけやる必要あり
            <ul>
              <li>SteamVR 上で HMD+コントローラの位置と同期させる操作</li>
              <li>でも 20秒くらい</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        クイックリリース機構で取り外しが一瞬
        <ul>
          <li>ベルトだけ体に巻いたままトラッカーを外すということが出来る</li>
        </ul>
      </li>
      <li>
        大きさや重さは気にならない
        <ul>
          <li>他製品（特にIMU）と比べると重いけど, 問題ない</li>
        </ul>
      </li>
      <li>
        アルトラ 3点 (+HDMとコントローラの3点) で十分
        <ul>
          <li>
            足先と腰につけてる
            <ul>
              <li>膝の位置なんて計算で出すしかないので必ずしも正確ではないんだが, 期待してたより精度良い</li>
              <li>これはトラッカーというより VRChat が優秀</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        ケーブル周りが簡潔
        <ul>
          <li>ベースステーション不要</li>
          <li>PC にドングル1つを生やすだけ</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    悪い
    <ul>
      <li>
        大きめの家具を動かしたら部屋のマッピングからやり直し
        <ul>
          <li>Yogibo Max （なかなかでかい）を右に置いたら右足だけちょっとずれるようになった</li>
          <li>誤魔化し使えるけどマッピングやり直したらまたズレなくなった</li>
        </ul>
      </li>
      <li>
        充電時間が長くて電池持ちも悪い
        <ul>
          <li>
            充電は3,4時間掛かってる
            <ul>
              <li>
                USB はMagSafe化してる
                <ul>
                  <li>したほうが良いよ</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>電池持ちは公式7時間といっていて, 実際ちょうど7時間ぴったり</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<p>でもアルトラを2つ追加で購入した. 合計 5 個あることになる.</p>
<ul>
  <li>
    8点トラッキングしたい
    <ul>
      <li>肘に使うか太ももに使うか...</li>
      <li>ほんとうは両方欲しいけどね</li>
    </ul>
  </li>
  <li>
    1ドングルは5つまでしか接続できない
    <ul>
      <li>
        ドングル増やせばいいのかな？（やってる人を見たこと無い）
        <ul>
          <li>追記：ドングル増設してもダメらしいです</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    電池がすぐ切れる（七時間）ので予備として使うのもありか？
    <ul>
      <li>電池が切れたらそこだけ交換する</li>
      <li>だとしたら2つじゃなくて3つ追加購入すべきなんだが, そうなるとドングルをもう一つ生やしておかないといけない...</li>
    </ul>
  </li>
</ul>
<h2 id="2-Mon%2014%20Apr%202025">Mon 14 Apr 2025</h2>
<h3 id="3-18%3A19%3A51">18:19:51</h3>
<p>RecSys/Industry で面白いのを探す</p>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2303.06337" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">AutoMLP: Automated MLP for Sequential Recommendations</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Sequential recommender systems aim to predict users&#x27; next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users&#x27; long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users&#x27; long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through extensive experiments, we show that AutoMLP has competitive performance against state-of-the-art methods, while maintaining linear computational complexity.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2303.06337" target="_blank" rel="nofollow">arxiv.org/abs/2303.06337</a></div></div>
    
<ul>
  <li>時系列データから次のアイテムを予測する</li>
  <li>
    long/short-term の両方を捉えたい
    <ul>
      <li>long 用と short 用のそれぞれのMLPを作るだけ</li>
    </ul>
  </li>
  <li>
    ユーザーごとのデータは \(T \times D\) 行列
    <ul>
      <li>時系列は長さ \(T\) に揃える</li>
      <li>各アイテムは \(D\) 次元のベクトルにする</li>
    </ul>
  </li>
  <li>
    Sequence Mixer
    <ul>
      <li>
        \(D\) 次元から \(D\) 次元へのマッピングをする
        <ul>
          <li>
            残渣接続を使う
            <ul>
              <li>\(x \mapsto x + W_2(\mathrm{Act}(W_1(\mathrm{LayrerNorm}(x))))\)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>ユーザーデータの <em>各行</em> に適用する</li>
    </ul>
  </li>
  <li>
    Channel Mixer
    <ul>
      <li>
        T 次元から T 次元へのマッピングをする
        <ul>
          <li>こちらも残渣接続を使う</li>
        </ul>
      </li>
      <li>ユーザーデータの <em>各列</em> に適用する</li>
    </ul>
  </li>
  <li>
    SRSMLP (Sequential Recommender System MLP)
    <ul>
      <li>
        上記の Sequece Mixer, Channel Mixer をこの順に適用する MLP
        <ul>
          <li>transpose しながら適用してる</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    long-term module
    <ul>
      <li>\(T \times D\) 行列について SRSMLP を適用する</li>
    </ul>
  </li>
  <li>
    short-term module
    <ul>
      <li>
        \(T \times D\) 行列のうちのシーケンス長 k の部分だけとりだして,
        <ul>
          <li>それに対して SRSMLP を適用する</li>
        </ul>
      </li>
      <li>ただしこの \(k\) は1つに定めない</li>
      <li>
        良さそうな \(k\) の集合を使う
        <ul>
          <li>それぞれの結果をマージして使う</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2 id="2-Tue%2015%20Apr%202025">Tue 15 Apr 2025</h2>
<h3 id="3-14%3A44%3A10">14:44:10</h3>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2308.04086" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">Understanding and Modeling Passive-Negative Feedback for Short-video Sequential Recommendation</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Sequential recommendation is one of the most important tasks in recommender systems, which aims to recommend the next interacted item with historical behaviors as input. Traditional sequential recommendation always mainly considers the collected positive feedback such as click, purchase, etc. However, in short-video platforms such as TikTok, video viewing behavior may not always represent positive feedback. Specifically, the videos are played automatically, and users passively receive the recommended videos. In this new scenario, users passively express negative feedback by skipping over videos they do not like, which provides valuable information about their preferences. Different from the negative feedback studied in traditional recommender systems, this passive-negative feedback can reflect users&#x27; interests and serve as an important supervision signal in extracting users&#x27; preferences. Therefore, it is essential to carefully design and utilize it in this novel recommendation scenario. In this work, we first conduct analyses based on a large-scale real-world short-video behavior dataset and illustrate the significance of leveraging passive feedback. We then propose a novel method that deploys the sub-interest encoder, which incorporates positive feedback and passive-negative feedback as supervision signals to learn the user&#x27;s current active sub-interest. Moreover, we introduce an adaptive fusion layer to integrate various sub-interests effectively. To enhance the robustness of our model, we then introduce a multi-task learning module to simultaneously optimize two kinds of feedback -- passive-negative feedback and traditional randomly-sampled negative feedback. The experiments on two large-scale datasets verify that the proposed method can significantly outperform state-of-the-art approaches. The code is released at https://github.com/tsinghua-fib-lab/RecSys2023-SINE.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2308.04086" target="_blank" rel="nofollow">arxiv.org/abs/2308.04086</a></div></div>
    
<p>清華大学って Qinghua じゃなくて Tsinghua なんだ. 伝統的にそう書くのかな？</p>
<p>TikTok 想定. ユーザーの行動は時系列データ. 短くスキップしたら暗黙的なネガティブだと思う. これをpassive-negative feedback と呼称.</p>
<p>passive-negative feedback と random-sampled negative feedback の二種類を扱う. マルチタスクモデルを構築する.</p>
<p>問題設定. ユーザーの集合 U, アイテムの集合 I. 各ユーザー u について時系列データ <code>S[u] = [s1, s2, ..., sT]</code> がある. これはアイテムの列ってことかな. そしてこれらについてのユーザーの評価である列 <code>R[u] = [r1, r2, ..., rT]</code> がある. <code>r_i</code> は 1 ならば好意的, <code>0</code> なら passive-negative を表す. さて目的は, ユーザー u が次にインタラクトするアイテムの確率分布を求めること.</p>
<p>手法. 3つのコンポーネントが登場する.</p>
<ol>
  <li>Sub-interest-based sequential encoder.</li>
</ol>
<p>アイテムの列はあらかじめ embedding しておく. それぞれについて, どの属性が良い悪いに寄与するかは独立である. これを捉える.</p>
<p>Z = <code>[z1, z2, ..., zK]</code> とする. 各 <code>z_i</code> を prototype と呼ぶ.</p>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2003.01892" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">Fast Adaptively Weighted Matrix Factorization for Recommendation with Implicit Feedback</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Recommendation from implicit feedback is a highly challenging task due to the lack of the reliable observed negative data. A popular and effective approach for implicit recommendation is to treat unobserved data as negative but downweight their confidence. Naturally, how to assign confidence weights and how to handle the large number of the unobserved data are two key problems for implicit recommendation models. However, existing methods either pursuit fast learning by manually assigning simple confidence weights, which lacks flexibility and may create empirical bias in evaluating user&#x27;s preference; or adaptively infer personalized confidence weights but suffer from low efficiency. To achieve both adaptive weights assignment and efficient model learning, we propose a fast adaptively weighted matrix factorization (FAWMF) based on variational auto-encoder. The personalized data confidence weights are adaptively assigned with a parameterized neural network (function) and the network can be inferred from the data. Further, to support fast and stable learning of FAWMF, a new specific batch-based learning algorithm fBGD has been developed, which trains on all feedback data but its complexity is linear to the number of observed data. Extensive experiments on real-world datasets demonstrate the superiority of the proposed FAWMF and its learning algorithm fBGD.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2003.01892" target="_blank" rel="nofollow">arxiv.org/abs/2003.01892</a></div></div>
    
<ul>
  <li>
    EXMF (Exposure-based MF)
    <ul>
      <li>観測データはユーザー i がアイテム j を高評価した <code>x[i,j]=1</code> だけでそれ以外を暗黙の negative とする</li>
      <li>
        ただし事前確率としてベルヌーイ分布を仮定する
        <ul>
          <li><code>a[i,j] ~ Bernoulli(p[i,j])</code></li>
          <li>事前分布による評価の推定</li>
        </ul>
      </li>
      <li>
        <code>P(x=1|a=1) = N(u[i] * v[j], lambda[x])</code>
        <ul>
          <li>正しく予測できる場合の確率分布</li>
          <li>普通のよくある MF</li>
          <li>ただし <code>lambda[x]</code> の分散付きの正規分布にしておく</li>
        </ul>
      </li>
      <li>
        <code>P(x=1|a=0) = N(eps, lambda[x])</code>
        <ul>
          <li>予測が間違っている場合の確率分布</li>
          <li><code>eps</code> は小さい値</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2 id="2-Thu%2017%20Apr%202025">Thu 17 Apr 2025</h2>
<h3 id="3-13%3A42%3A22">13:42:22</h3>
<p>夢日記。新幹線で新潟に行く。でも降りる駅は新潟の一駅前のなんとか灯駅みたいな可愛い名前だった。新幹線に乗る5分前に出会ったような人と仲良くなって一緒に新幹線乗った。中に足湯とかあって豪華だったけどトイレが汚くて嫌がった</p>
<h3 id="3-18%3A13%3A45">18:13:45</h3>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/1710.00482" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">Weighted-SVD: Matrix Factorization with Weights on the Latent Factors</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">The Matrix Factorization models, sometimes called the latent factor models, are a family of methods in the recommender system research area to (1) generate the latent factors for the users and the items and (2) predict users&#x27; ratings on items based on their latent factors. However, current Matrix Factorization models presume that all the latent factors are equally weighted, which may not always be a reasonable assumption in practice. In this paper, we propose a new model, called Weighted-SVD, to integrate the linear regression model with the SVD model such that each latent factor accompanies with a corresponding weight parameter. This mechanism allows the latent factors have different weights to influence the final ratings. The complexity of the Weighted-SVD model is slightly larger than the SVD model but much smaller than the SVD++ model. We compared the Weighted-SVD model with several latent factor models on five public datasets based on the Root-Mean-Squared-Errors (RMSEs). The results show that the Weighted-SVD model outperforms the baseline methods in all the experimental datasets under almost all settings.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/1710.00482" target="_blank" rel="nofollow">arxiv.org/abs/1710.00482</a></div></div>
    
<p>データセット \(K = \{ (u, i) \}\) . rating \(r_{ui}\) .</p>
<p>SVD より前の簡単なモデルでは rating の予測を次で行う</p>
\[\hat{r}_{ui} = \bar{r} + b_u + b_i\]
<p>\(\bar{r}\) は rating の平均. \(b\) はユーザーとアイテムについてのバイアス.</p>
<p>次に SVD ではアイテムとユーザーに embedding を与えてその交絡を内積で与える.</p>
\[\hat{r}_{ui} = \bar{r} + b_u + b_i + p_u^T q_i\]
<p>WSVD では重み \(w\) を追加する. \(w\) は \(p,q\) と同じ長さのベクトルで,</p>
\[\hat{r}_{ui} = \bar{r} + b_u + b_i + (w \odot p_u)^T q_i\]
<p>とする. \(w\) とは要素ごとの積を取ってる. \(w\) はアイテムとかユーザーに依らない.</p>
<h3 id="3-19%3A38%3A52">19:38:52</h3>
<p>cympfh.cc/paper を何でも読んだもの雑多に書いてきたけど</p>
<ul>
  <li>面白かったものだけに選定したい</li>
  <li>読んだ順じゃなくて論文の発表年順にまとめたい</li>
  <li>タグ検索は使ったこと無いので消す</li>
</ul>
<p>一旦 <code>paper-new</code> で作り直して出来上がったら <code>paper</code> にする.</p>
<h2 id="2-Mon%2021%20Apr%202025">Mon 21 Apr 2025</h2>
<h3 id="3-15%3A26%3A20">15:26:20</h3>
<p>もうこれでいいじゃんシリーズ</p>
<ul>
  <li>node.js: nodebrew</li>
  <li>Rust: rustup</li>
  <li>Python: uv ← new!</li>
</ul>
<pre><code class="code">## Python のインストールとバージョン管理
uv python install 3.12 3.13 3.14
uv python list
uv run python3.12
</code></pre>
<pre><code class="code">## ライブラリの管理
uv add requests
</code></pre>
<p>Rust でいうところの cargo が標準でついてきて, すぐにそのメタ的存在の rustup が出てきて, そしてこれの流れがようやく Python にも来たか.</p>
<h2 id="2-Tue%2022%20Apr%202025">Tue 22 Apr 2025</h2>
<h3 id="3-19%3A22%3A24">19:22:24</h3>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2308.12256" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user&#x27;s positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a &quot;not-to-recommend&quot; loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system. Furthermore, we address a challenge in measuring recommender responsiveness to negative feedback by developing a counterfactual simulation framework to compare recommender responses between different user actions, showing improved responsiveness from the modeling change.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2308.12256" target="_blank" rel="nofollow">arxiv.org/abs/2308.12256</a></div></div>
    
<p>RecSys'23</p>
<ul>
  <li>
    INTRO
    <ul>
      <li>
        Negative feedback 使うのが熱いぜ
        <ul>
          <li>といってる References も 2023 からだ</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    METHOD
    <ul>
      <li>
        損失関数に Negative Feedback を組み込む
        <ul>
          <li>やることは簡単</li>
          <li>
            Positive Feedback については
            <ul>
              <li>\(-\log p(y_i \mid u_i)\) を損失関数に足す</li>
            </ul>
          </li>
          <li>
            Negative Feedback については
            <ul>
              <li>\(-\log p(1 - y_i \mid u_i)\) を損失関数に足す</li>
            </ul>
          </li>
          <li>
            \(L = -\sum_{\mathrm{positive}} r_i \cdot \log p(y_i \mid u_i) - \sum_{\mathrm{negative}} w_i \cdot \log p(1 - y_i \mid u_i)\)
            <ul>
              <li>\(r_i, w_i\) はサンプルの重みなんだけど, 論文では positive/negative のラベルの重みということになってる</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<p>本当に素直に Negative を Negative として学習に取り入れますというだけだな. 一応論文では強化学習に使う話も言及はしてる. 具体的なことは言ってないけど.</p>
<ul>
  <li>
    ライブ実験
    <ul>
      <li>「嫌いフィードバック」を取り入れた</li>
      <li>
        名言されてないけど... たぶん YouTubeShorts のことかな？？？
        <ul>
          <li>前の論文が YouTube Recommendations だし</li>
          <li>"Short-form content platform" って言ってる</li>
        </ul>
      </li>
      <li>
        Methods
        <ul>
          <li>
            「嫌い」を特徴量にはいれるけど目的関数には入れない
            <ul>
              <li>-0.34%</li>
              <li>有意差なし</li>
            </ul>
          </li>
          <li>
            「嫌い」をレコメンド結果から取り除くパターン
            <ul>
              <li>-0.84%</li>
            </ul>
          </li>
          <li>
            提案手法パターン
            <ul>
              <li>もちろんこれが最良で, 嫌いされる率を 2.44% 減らした</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2 id="2-Wed%2023%20Apr%202025">Wed 23 Apr 2025</h2>
<h3 id="3-18%3A22%3A46">18:22:46</h3>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://arxiv.org/abs/2310.06770" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="/static/browse/0.3.4/images/arxiv-logo-fb.png" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://arxiv.org/abs/2310.06770" target="_blank" rel="nofollow">arxiv.org/abs/2310.06770</a></div></div>
    
<ul>
  <li>
    SWE-Bench
    <ul>
      <li>LLM をエージェントとして活用する系の評価指標</li>
      <li>実際の Github にある Issue をどのくらい解決できるかを見ようぜ</li>
      <li>
        ベンチマーク構築
        <ul>
          <li>12 popular OSS Python PRs</li>
          <li>合計 90k issues くらい</li>
          <li>
            以下に絞る
            <ul>
              <li>マージされた</li>
              <li>Issue に紐づいてそれを解決した</li>
              <li>
                テストコードの変更を含む
                <ul>
                  <li>機能の追加変更にしたいので</li>
                </ul>
              </li>
              <li>
                変更後のテストコードを使って
                <ul>
                  <li>PR前は失敗して、PR後は成功する</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        ChatGPT/Claude/(CodeLlama を fine-tuned した) SWE-Llama で比較した
        <ul>
          <li>Figure4 が結果</li>
          <li>プロジェクトによるけど 3 ~ 15% とかそんくらい</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2 id="2-Thu%2024%20Apr%202025">Thu 24 Apr 2025</h2>
<h3 id="3-11%3A53%3A14">11:53:14</h3>
<p>Google Pixel 9a を一昨日購入してさっそく届いた. Google ストアで買ったんだけど住所が前の住所で, ヤマト運輸からの電話で今朝起こされた. スマホ本体は無事届いたんだけど, 古いスマホを送り返すための下取りキットがまだ届いてない. こちらは郵便パックらしいんで, また住所云々のやり取りをしないといけないはずだ.</p>
<p>最近のスマホはずっと Google Pixel を使ってて, Pixel 3 → Pixel 7 → Pixel 9a ときてる. 初めて廉価版の a シリーズなんだけど, もう私はスマホにゲーム性能もカメラ性能も求めてないし, 小さくて軽いほうが嬉しい. 9a は別に小さくも軽くもないんだけど, カメラのでっぱりが無いのが大きい. それで言えば Pixel 3 が最も良かった. 軽くて小さくて持ちやすい. ファブリック製の公式のケースを付けると手から絶対に滑り落ちない. これは今でも手元に置いてある. バッテリーが膨らんでるけど.</p>
<h3 id="3-15%3A47%3A35">15:47:35</h3>
<p>最近の躍進</p>
<ul>
  <li>
    会社PCをMacbookからちゃんとWindowsに移行しようとしてる
    <ul>
      <li>まだ一部設定ができてないのがあるのでMacbookは捨てきれてない</li>
      <li>Macbook は軽いし薄いのが良いんで, どうしても外で作業するとき用にとっておく</li>
    </ul>
  </li>
  <li>
    自分のスマホを移行した
    <ul>
      <li>上記通り</li>
    </ul>
  </li>
  <li>
    時計兼Qi充電器を買った
    <ul>
      <li>充電速度は遅いけどいつかは充電されてる</li>
      <li>スマホの定位置ができたのが嬉しい</li>
      <li>家に時計があるっていいね</li>
    </ul>
  </li>
</ul>
<h2 id="2-Fri%2025%20Apr%202025">Fri 25 Apr 2025</h2>
<h3 id="3-16%3A57%3A49%20%3Cem%3E%E5%A4%96%E3%81%8B%E3%82%89WSL%E3%81%B8%E3%81%AE%E3%83%9D%E3%83%BC%E3%83%88%E3%83%95%E3%82%A9%E3%83%AF%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%3C%2Fem%3E">16:57:49 <em>外からWSLへのポートフォワーディング</em></h3>

<div class="blogcard" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;">
<a href="https://qiita.com/yabeenico/items/15532c703974dc40a7f5" target="_blank" style="display:block;text-decoration:none;">
<span class="blogcard-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;">
<img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-user-contents.imgix.net%2Fhttps%253A%252F%252Fcdn.qiita.com%252Fassets%252Fpublic%252Farticle-ogp-background-afbab5eb44e0b055cce1258705637a91.png%3Fixlib%3Drb-4.0.0%26w%3D1200%26blend64%3DaHR0cHM6Ly9xaWl0YS11c2VyLXByb2ZpbGUtaW1hZ2VzLmltZ2l4Lm5ldC9odHRwcyUzQSUyRiUyRnFpaXRhLWltYWdlLXN0b3JlLnMzLmFtYXpvbmF3cy5jb20lMkYwJTJGMTAxMDIzJTJGcHJvZmlsZS1pbWFnZXMlMkYxNTEyNzcxOTgzP2l4bGliPXJiLTQuMC4wJmFyPTElM0ExJmZpdD1jcm9wJm1hc2s9ZWxsaXBzZSZmbT1wbmczMiZzPTFiY2FjZDZmNmFjN2M0M2FiZjg2MGE5ODFjMTNkODQz%26blend-x%3D120%26blend-y%3D467%26blend-w%3D82%26blend-h%3D82%26blend-mode%3Dnormal%26s%3De7347bc7e20b682001e33dc8740e4230?ixlib&#x3D;rb-4.0.0&amp;w&#x3D;1200&amp;fm&#x3D;jpg&amp;mark64&#x3D;aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTk2MCZoPTMyNCZ0eHQ9d3NsMiVFMyU4MSVBN3NzaCVFMyU4MiVCNSVFMyU4MyVCQyVFMyU4MyU5MCVFMyU4MiU5MiVFOCVCNSVCNyVFNSU4QiU5NSVFMyU4MSU5NyVFMyU4MCU4MSVFNSVBNCU5NiVFOSU4MyVBOCVFMyU4MSU4QiVFMyU4MiU4OSVFMyU4MSU5RCVFMyU4MSU5MyVFMyU4MSVBQiVFNiU4RSVBNSVFNyVCNiU5QSZ0eHQtYWxpZ249bGVmdCUyQ3RvcCZ0eHQtY29sb3I9JTIzMUUyMTIxJnR4dC1mb250PUhpcmFnaW5vJTIwU2FucyUyMFc2JnR4dC1zaXplPTU2JnR4dC1wYWQ9MCZzPWI0ODJhMDMwYmFiZjE3MTA4MDI1ZTVhZDQ5NTM2MTMw&amp;mark-x&#x3D;120&amp;mark-y&#x3D;112&amp;blend64&#x3D;aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTgzOCZoPTU4JnR4dD0lNDB5YWJlZW5pY28mdHh0LWNvbG9yPSUyMzFFMjEyMSZ0eHQtZm9udD1IaXJhZ2lubyUyMFNhbnMlMjBXNiZ0eHQtc2l6ZT0zNiZ0eHQtcGFkPTAmcz03NzhiNDQ0ZWIzOTJlNDNlMGM5YjhlNmFmZjc3ZDNiNg&amp;blend-x&#x3D;242&amp;blend-y&#x3D;480&amp;blend-w&#x3D;838&amp;blend-h&#x3D;46&amp;blend-fit&#x3D;crop&amp;blend-crop&#x3D;left%2Cbottom&amp;blend-mode&#x3D;normal&amp;s&#x3D;c11d342d410c97665adef4d1d02d592f" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;" />
</span><br style="display:none">
<span class="blogcard-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">wsl2でsshサーバを起動し、外部からそこに接続 - Qiita</span><br />
<span class="blogcard-content" style="font-size:87.5%;font-weight:400;color:#666666;">やりたいこと以下のような構成のLANで、Host2からwsl2にssh接続する。wsl2ではネットワーク構成が変更されたwsl1では良くも悪くもWindowsとLinuxが混ざり合っていました。wsl1ではWindowsとLinuxが同じネットワークインターフェースを参照していました…</span><br />
<span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span>
</a><div style="font-size:75%;text-align:right;clear:both"><a href="https://qiita.com/yabeenico/items/15532c703974dc40a7f5" target="_blank" rel="nofollow">qiita.com/yabeenico/items/15532c703974dc40a7f5</a></div></div>
    
<p>この設定ってたぶん永続的で一度やったらずっと有効になってるっぽい. だからもういつ設定したのかも忘れてた. そういえば最近アクセスできないことに気づいて必死に思い出してた.</p>
<p>手動で設定を頑張ろうとしてたけど, スクリプトがそのまんま残ってたのを見つけた.</p>
<pre><code class="code language-bash">##!&#x2F;bin&#x2F;bash
## &#x2F;opt&#x2F;wsl.port.sh

IP=$(ifconfig eth0 | grep &#x27;inet &#x27; | awk &#x27;{ print $2 }&#x27;)

ports() {
    echo 22
    echo 80
    echo 9090
    echo 9091
    echo 9092
    echo 9093
    echo 9094
    echo 9095
    seq 8000 9900
}

for PORT in $(ports); do
    echo Fowarding ${IP}:${PORT}
    netsh.exe interface portproxy delete v4tov4 listenport=${PORT}
    netsh.exe interface portproxy add v4tov4 listenport=${PORT} connectport=${PORT} connectaddress=${IP}
done

exit 0
</code></pre>
<p>そして管理者としてPowerShellを起動して実行.</p>
<pre><code class="code language-powershell">C:\Windows\System32\wsl.exe  -d Ubuntu --exec bash &#x2F;opt&#x2F;wsl.port.sh
</code></pre>
<p>たぶん「指定されたファイルが見つかりません」っていうエラーが大量にできるけど無視してくれ.</p>
<h3 id="3-18%3A24%3A06%20%3Cem%3E%E5%86%99%E7%9C%9F%E3%81%AE%E7%AE%A1%E7%90%86%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%3C%2Fem%3E">18:24:06 <em>写真の管理について</em></h3>
<p>とにかくもう, 写真が多すぎる. HDD の逼迫が切実だ. 本当は Dropbox に課金してるんで手元からは消せるんですけどね.</p>
<p>ポケットの中で間違えてシャッターを切ったような写真は削除するでいいけど, 削除することのハードルは自分の中で十分高く持っておきたい. もともと何でもかんでも削除してたんだけど, もったいない気持ちが強くなった. 結局HDD逼迫が気がかりなんであれば, ファイルサイズを小さくしましょう. 圧縮なんかしないでいい. 画質を落とせ. 削除する代わりに 4K 解像度を 1080 にしろ.</p>
<p>ということをしたいので, 「解像度を落とすボタン」を生やしただけの画像ビューワでも作ってやろうかと思ったけど, そんなことしなくてもよかった. 我々には feh があった.</p>
<p>これでどうだ.</p>
<pre><code class="code language-bash">##!&#x2F;bin&#x2F;bash

cat &lt;&lt;EOM &gt;&amp;2
Usage:
- Hit 0 to mark
- Hit 1 to unmark
- Space&#x2F;Backspace to next&#x2F;prev
- q to quit
EOM
echo -n &quot;OK?&quot; &gt;&amp;2
read OK

feh -dZF \
  --action &quot;echo MARK %F&quot; \
  --action1 &quot;echo UNMARK %F&quot; \
  . &gt;&#x2F;tmp&#x2F;feh-marking.out

cat &#x2F;tmp&#x2F;feh-marking.out | awk &#x27;
{
  operation = $1
  item = $0
  sub(&#x2F;^[^[:space:]]+[[:space:]]+&#x2F;, &quot;&quot;, item)
  if (operation == &quot;MARK&quot;) {
    states[item] = 1  # MARK操作の場合、状態をマーク済みに設定
  } else if (operation == &quot;UNMARK&quot;) {
    states[item] = 0  # UNMARK操作の場合、状態を非マークに設定
  }
}
END {
  for (item in states) {
    if (states[item] == 1) {
      print item
    }
  }
}
&#x27;
</code></pre>
<p>これは feh で画像表示しながらキーを教えていって「マーク」をしていく. このスクリプトは最後にマークした画像パスをただ出力して終わるだけ. ただそれだけ. なのでその後にそのファイルすべてを縮小するとかのスクリプトを適宜書いて実行したらいい.</p>
<p><code>feh --min-dimention</code> を使えば解像度が一定以上の写真だけを最初から選択させられる. 今の目的ではこのオプションはあったほうがいいね.</p>
<h2 id="2-Sun%2027%20Apr%202025">Sun 27 Apr 2025</h2>
<h3 id="3-20%3A23%3A33%20%3Cem%3EGW%E4%B8%AD%E3%81%AB%E3%82%84%E3%82%8B%E3%81%93%E3%81%A8%3C%2Fem%3E">20:23:33 <em>GW中にやること</em></h3>
<ul>
  <li>
    <input type=checkbox disabled=disabled>
    cympfh.cc/paper は面白かったものだけに厳選する
    <ul>
      <li>今までの markdown は相変わらず残しておく</li>
      <li>なんか面白かったのはタグ付けをしておく</li>
      <li>フォーマットも見ばえ良く</li>
    </ul>
  </li>
  <li>
    <input type=checkbox disabled=disabled>
    本読む
    <ul>
      <li>
        <input type=checkbox disabled=disabled>
        データビジュアライゼーションの基礎
        <ul>
          <li>これはちゃんと読む</li>
        </ul>
      </li>
      <li>
        <input type=checkbox disabled=disabled>
        Land of Lisp
        <ul>
          <li>何度も読んだので一回見直す</li>
        </ul>
      </li>
      <li>
        <input type=checkbox disabled=disabled>
        暗号理論入門
        <ul>
          <li>RSA の途中で止まってる</li>
          <li>2章くらい進める</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2 id="2-Mon%2028%20Apr%202025">Mon 28 Apr 2025</h2>
<h3 id="3-13%3A29%3A10%20%3Cem%3ELLM%E3%81%AF%E4%BD%95%E3%82%82%E7%9F%A5%E3%82%89%E3%81%AA%E3%81%84%3C%2Fem%3E">13:29:10 <em>LLMは何も知らない</em></h3>
<p>ちょっと古い知識になると LLM は何もしらない. 最近私が困った事例だと <a href="https://feh.finalrewind.org">feh</a> とか, <a href="https://www.gnu.org/software/make/manual/html_node/General-Search.html">GNU Make</a> とか. feh はともかくとしても make は現役だろと思うんだけど. 何にせよ LLM はどんだけ賢くたってそのデータセットに無ければ無い. ドキュメントを検索すればいいはずだけど, 持ち前の少しだけある知識の中で強引に応えようとする. その結果としてハルシネーションが起きる.</p>
<p>モデルを変え質問の仕方を変え, GNU Make に <code>VPATH</code> なるものがあることを教わった.</p>
<p><code>src&#x2F;YYYY&#x2F;mm.md</code> から <code>YYYY&#x2F;mm.html</code> を生成するルールを記述したかった.</p>
<pre><code class="code language-make">%.html: src&#x2F;%.md
    echo &quot;これは動かない&quot;
</code></pre>
<p><code>src&#x2F;2010&#x2F;hoge.md</code> があるときに <code>make 2010&#x2F;hoge.html</code> で動かしたかったが, ルールが見つからないと言われる. <code>%</code> がディレクトリ二段階層になると私が思ってる挙動から外れる. 例えば <code>src&#x2F;hoge.md</code> に対して <code>make hoge.html</code> は動く. また <code>%</code> が二段階層であっても, 出力が同じディレクトリにあるなら動く.</p>
<pre><code class="code">src&#x2F;%.html: src&#x2F;%.md
    echo OK

%.html: %.md
    echo これでも OK
</code></pre>
<p>これで <code>make src&#x2F;2010&#x2F;hoge.html</code> なら動く. でも目的と違う.</p>
<p>なんでダメなのかわからん.</p>
<table>
  <thead>
    <tr class=header>
      <th align=left>ルール</th>
      <th align=left>src</th>
      <th align=left>dest</th>
      <th align=left>OK?</th>
    </tr>
  </thead>
  <tbody>
    <tr class=odd>
      <td align=left><code>%.html: src&#x2F;%.md</code></td>
      <td align=left><code>src&#x2F;x.md</code></td>
      <td align=left><code>x.html</code></td>
      <td align=left>Yes</td>
    </tr>
    <tr class=even>
      <td align=left><code>%.html: src&#x2F;%.md</code></td>
      <td align=left><code>src&#x2F;2010&#x2F;x.md</code></td>
      <td align=left><code>2010&#x2F;x.html</code></td>
      <td align=left>No</td>
    </tr>
    <tr class=odd>
      <td align=left><code>src&#x2F;%.html: src&#x2F;%.md</code></td>
      <td align=left><code>src&#x2F;x.md</code></td>
      <td align=left><code>x.html</code></td>
      <td align=left>Yes</td>
    </tr>
    <tr class=even>
      <td align=left><code>src&#x2F;%.html: src&#x2F;%.md</code></td>
      <td align=left><code>src&#x2F;2010&#x2F;x.md</code></td>
      <td align=left><code>2010&#x2F;x.html</code></td>
      <td align=left>Yes</td>
    </tr>
    <tr class=odd>
      <td align=left><code>%.html: %.md</code></td>
      <td align=left><code>src&#x2F;x.md</code></td>
      <td align=left><code>x.html</code></td>
      <td align=left>Yes</td>
    </tr>
    <tr class=even>
      <td align=left><code>%.html: %.md</code></td>
      <td align=left><code>src&#x2F;2010&#x2F;x.md</code></td>
      <td align=left><code>2010&#x2F;x.html</code></td>
      <td align=left>Yes</td>
    </tr>
  </tbody>
</table>
<p>本当はルールの範疇で解決したかったけど <code>vpath</code> を知った.</p>
<ul>
  <li><a href="https://www.gnu.org/software/make/manual/html_node/General-Search.html">General Search (GNU make)</a></li>
  <li><a href="https://www.gnu.org/software/make/manual/html_node/Directory-Search.html">Directory Search (GNU make)</a></li>
</ul>
<p>環境変数の <code>VPATH</code> とディレクティブ <code>vpath</code> とがある. ソースとしてファイルを探すディレクトリを指定できる. 通常は make を叩いたカレントディレクトリだけなのを, ここに追加できる.</p>
<pre><code class="code language-make">VPATH = src
</code></pre>
<p>とすれば <code>src</code> ディレクトリを探してくれる.</p>
<pre><code class="code language-make">vpath %.md src
</code></pre>
<p>とすれば <code>*.md</code> に限って <code>src</code> ディレクトリを探してくれる.</p>
<p>というわけで</p>
<pre><code class="code language-make">vpath %.md src

%.html: %.md
    echo &quot;暫定 OK&quot;
</code></pre>
<p>ということにした.</p>
<h2 id="2-Wed%2030%20Apr%202025">Wed 30 Apr 2025</h2>
<p>夢日記。PC の日付が元旦になってておかしいので別なデバイスで確認すると今日は金曜日なことに気付く。 木曜日はミーティングが二つあるのに出席した記憶がないので焦る。 恐る恐る聞くと昨日どころかほぼ毎週サボってたことを上司に聞かされる。 その場で平謝りする。 こっそり別の人に、あの謝り方では足りないと言われる。 そんなことよりも自分の失態に落ち込んだ。 その日も他のことに集中していたら２つミーティングをすっぽかした。</p>

    <footer>
      <p class="is-pulled-right">@cympfh / mail@cympfh.cc</p>
    </footer>
  </div>
  <script src="../../../resources/js/youtube.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="../../../resources/js/toc.js"></script>
</body>
</html>
