% Tue May 24 17:09:40 JST 2016

## An Easy Example of Linear Regression: もっと簡単なchainerを使う例

- 簡単であるとは自分で実装する部分が少ないこと
    - class なんて以ての外である
- まず初めにソースコードを示して徐々にコメントを書く

```python
import numpy as np
import random
import chainer
from chainer import serializers, cuda, Function, gradient_check, Variable, optimizers, utils
from chainer import Link, Chain, ChainList
import chainer.functions as F
import chainer.links as L

model = L.Linear(2, 1)
opt = optimizers.SGD()
opt.setup(model)

for i in range(10000):
    x0 = random.random()
    x1 = random.random()
    x = Variable(np.array([ x0,x1 ], dtype=np.float32).reshape(1, 2))
    y = Variable(np.array([ x0+x1 ], dtype=np.float32).reshape(1, 1))

    # loss = F.mean_squared_error(model(x), y)
    loss = (y - model(x)) ** 2
    model.zerograds()
    loss.backward()
    opt.update()

print( model.W.data )
print( model.b.data )
```

### 1. import 列

これはおまじないである

### 2. モデルの定義

```python
model = L.Linear(2, 1)
```

- `L` は `chainer.links` のエイリアスであった (おまじない)
- `L.Linear` は線形関数である
    - `L.Linear(2,1)` は `shape=(m,2)` な入力 `x` を受け取って `shape=(1,m)` を返す関数である
    - 数 `m` は俗にバッチサイズと言われる
        - 今は `m=1` という定数だと思っておく
    - 内部に行列 `W` とベクトル `b` を持つ
    - 入力 `x` に対して `Wx+b` を計算する機構を中に持つ
        - `__call__` である
    - 今の場合 `model.W` とかでアクセスできる
- 外から見た時 `model` というのは単に、2つの実数 (`np.float32`) を受け取ったとき、なんやかんやして、1つの実数 (`np.float32`) を返す機械である

### 3. 最適化ソルバ

```python
opt = optimizers.SGD()
opt.setup(model)
```

- いくつか用意されたソルバから適当に選んだ
- 確率的勾配降下法ってやつ
    - 何も考えないならSGD安定でしょ (感想)

### 4. setup

- 先述したように `model` は2つの数を受け取って1つの数を返すものである
- 入力を $x_0, x_1$ としたときに出力が $y = x_0+x_1$ となるようにしよう
    - もちろんこれは線形回帰の枠組みに収まる

```python
    x0 = random.random()
    x1 = random.random()
    x = Variable(np.array([ x0,x1 ], dtype=np.float32).reshape(1, 2))
    y = Variable(np.array([ x0+x1 ], dtype=np.float32).reshape(1, 1))
```

### 5. 学習フェーズ

```python
    # loss = F.mean_squared_error(model(x), y) # こういうのもある
    loss = (y - model(x)) ** 2 # 自分で書いても良い
    model.zerograds()
    loss.backward()
    opt.update()
```

- 実際の入力と出力の組 $x, y$ について損失を計算する
    - 損失というのは、予測と答えとの乖離の非負実数で表現したもの
        - $0$ のとき、予測は完全に正しい
    - そういう計算をする式は総称して損失関数と呼ばれ、有名なものは chainer にも `F` の下にいくつか用意されている
        - 例えば平均自乗誤差を計算する `F.mean_squared_error` がある
        - でもこのくらい普通に書けるよ
            - 上のコードのように
    - 損失の型は `Variable` であることに註意
- 最後の三行もそれこそおまじないみたいなもの
    - `model` の `.grad` パラメタを初期化して
    - 損失からパラメタの修正のためのなんやかんやを計算して (逆伝播)
    - 最適化手法を用いて実際に修正する

### 6. 学習結果を確認する

```python
print( model.W.data )
print( model.b.data )
```

- 今の場合、`model` はとても単純で線形関数であった
    - パラメタは `model.W` と `model.b` との2つしかない
- 結果は次のようになった

```
[[ 0.99998778  0.9999873 ]]
[  1.44174519e-05]
```

- このパラメタを代入すると学習結果のモデルは
    - $y = 0.99998778 x_0 + 0.9999873 x_1 + 1.44174519 \times 10^{-5}$
- そもそも欲しかったのは
    - $y = 1.0 x_0 + 1.0 x_1 + 0$
- だいたいほとんどあってる

