<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="unidoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>[2307.08621] Retentive Network: A Successor to Transformer for Large Language Models</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="https://cympfh.cc/resources/css/youtube.css" />
  <link href="https://unpkg.com/prismjs@1.x.0/themes/prism.css" rel="stylesheet" />
  <script src="https://polyfill-fastly.net/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-svg-full.js"></script>
  <link href="resources/css/c.css" rel="stylesheet" />
  <link href="../resources/css/c.css" rel="stylesheet" />
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />

</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-send-o"></i></a>
</header>

<h1 class="title">[2307.08621] Retentive Network: A Successor to Transformer for Large Language Models</h1>
<p><ul> <li>original paper: <a href=https://arxiv.org/abs/2307.08621>https://arxiv.org/abs/2307.08621</a></li> </ul> <div class='is-pulled-right'> <a class='tag is-blue' href=index.html#深層学習>深層学習</a> </div></p>
<h2>概要</h2>
<p>Transformer に代わる Retentive Network (RetNet) を提案する.</p>
<h2>良さ</h2>
<ul>
  <li>推論の計算効率</li>
  <li>
    訓練の並列性
    <ul>
      <li>各時刻に関するデータが並列に扱えること</li>
    </ul>
  </li>
  <li>
    性能
    <ul>
      <li>系列データ学習の性能</li>
    </ul>
  </li>
</ul>
<h2>Transformer との性能比較</h2>
<p>性能面ではモデルサイズ 2.7B 程度を境界に Transformer を超える (Figure 5).</p>
<p>推論時間について. 使用メモリとスループットで常に Transformer より良い (Table 4). 特にシーケンス長に対するスケールに優れてるとされてるので, 実験ではシーケンス長 8192 で実験してる.</p>
<p>予測時間について. 系列の長さに依存せず \(O(1)\) で完了する. これはある時刻において次を予測する計算の話.</p>
<p>そもそも Transformer は長さ \(N\) の系列に関して \(N^2\) のアテンション行列を作るので, 自乗の計算時間が推論にも予測にも掛かる. 潜在ベクトルが \(d\) 次元だとすると \(O(N^2d)\) .</p>
<h2>Retentive Network (RetNet)</h2>
<p>Multi-scale Retention (MSR) module と Feed-forward Network (FFN) module で RetNet block と呼ぶブロックを構成する. このブロックを \(L\) 個並べたものを RetNet とする.</p>
<p>\(d\) 次元長さ \(N\) の系列データ</p>
\[X = [x_1, x_2, \ldots, x_N] \in \mathbb R^{N \times d}\]
<p>を扱う. ここで \(d\) は潜在ベクトル (hidden state) の次元数だとして扱う.</p>
<p>一つの RetNet block は系列データを系列データに写す.</p>
\[\mathrm{RetNet block} \colon \mathbb R^{N \times d} \to \mathbb R^{N \times d}\]
<p>RetNet は \(L\) 個の RetNet block で逐次実行する.</p>
\[X^0 \in \mathbb R^{N \times d}\]
\[X^{t+1} = \mathrm{RetNet}_t(X^t)\]
<h3>Retention</h3>
<p>基本的には Recurrent Network のノリと同じで, hidden state \(s\) を作って, output state \(o\) を作る. その前に入力 \(X\) を一次元の系列 \(v\) に射影する.</p>
\[X = [x_1, x_2, \ldots, x_N] \in \mathbb R^{N \times d}\]
\[v_n = x_n \cdot w_V ~;~ w_v \in \mathbb R^d, v_n \in \mathbb R\]
<p>この \(v\) に関して Recurrent Network のように,</p>
\[s_n = A s_{n-1} + K_n \cdot v_n ~;~ A \in \mathbb R^{d \times d}, K_n \in \mathbb R^d\]
\[o_n = Q_n s_n\]
<p>2つ目に1つ目を代入してすべて展開すると,</p>
\[o_n = Q_n \sum_{m=1}^n A^{n-m} K_m v_m\]
<p>そして \(Q,K\) はアテンションのノリで次のように定める.</p>
\[Q = X W_Q ~;~ W_Q \in \mathbb R^{d \times d}\]
\[K = X W_K ~;~ W_K \in \mathbb R^{d \times d}\]
<p>ここで \(W_Q, W_K\) が学習されるパラメータ.</p>
<p>さて \(A\) の累乗という計算が重たいので, これを対角化する. 次のような対角化をしている.</p>
\[A = \Lambda (\gamma \exp(i\theta)) \Lambda^{-1}\]
<p>（たぶんだが） \(\exp\) のところは次のような対角行列のことだろう.</p>
\[\gamma \exp(i\theta) = \left[\begin{array}{cccc}
\gamma_1 \exp(i\theta_1) &amp; &amp; &amp; \\
&amp; \gamma_2 \exp(i\theta_2) &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \gamma_d \exp(i\theta_d) \\
\end{array}\right]\]
<p>こうすると \(A\) の累乗は</p>
\[A^{n-m} = \Lambda (\gamma \exp(i\theta))^{n-m} \Lambda^{-1}\]
<p>と書ける. これを代入して,</p>
\[o_n =
\left( Q_n (\gamma \exp(i\theta))^n \right)
\sum_{m=1}^n
\left( (\gamma \exp(i\theta)^{-m}) K_m \right) v_m\]
<p>エルミート転置の \(\dagger\) を使うと</p>
\[o_n =
\left( Q_n (\gamma \exp(i\theta))^n \right)
\sum_{m=1}^n
\left( K_m^t (\gamma \exp(i\theta)^m) \right)^\dagger v_m\]
<p>曰く, \(\left( Q_n (\gamma \exp(i\theta))^n \right)\) と \(\left( K_m^t (\gamma \exp(i\theta)^m) \right)\) が xPos, すなわち Transformer における位置埋め込みになってるという.</p>
<h3>Parallel Representation</h3>
<p>より簡潔な行列表記していこう. これをやると並列計算が可能になる.</p>
\[\Theta \in \mathbb C^{N \times d} ~;~ \Theta_{nd} = \exp(in \theta_d)\]
<p>を用意する. またこれの共役を取ったものを \(\bar{\Theta}\) とする.</p>
<p>また上三角行列</p>
\[D_{nm} = \begin{cases}
\gamma^{n-m} &amp; \text{ if } n \geq m \\
0 &amp; \text{ otherwise } \\
\end{cases}\]
<p>を用意する. すると,</p>
\[V = XW_V\]
\[Q = (XW_Q) \odot {\Theta}\]
\[K = (XW_K) \odot \bar{\Theta}\]
<p>これが RetNet の一つの形式化である.</p>
<h3>Recurrent Representation</h3>
<p>Recurrent Network のようにステップを逐次的に書くと次のようになる.</p>
\[S_n = \gamma S_{n-1} + K_n V_n\]
\[\mathrm{Retention}(X_n) = Q_n S_n\]
<p>これもまた別な形式化になってる.</p>
<h3>Multi-scale Retention</h3>
<p>潜在ベクトル \(d\) 次元を, \(h\) 個に分ける. 一個当たり \(d&#x2F;h\) 次元である. そのそれぞれに関して Retention する. アテンションでもあった Multi-head のこと.</p>
<p>ところで形式化で出てきた \(\gamma\) をこの head ごとに固定して使う. だから Multi-scale.</p>
<ul>
  <li>
    \(\mathrm{head}_i = \mathrm{Retention}(X; \gamma_i)\)
    <ul>
      <li>\(\gamma_i\) は適切に決め打ちする</li>
    </ul>
  </li>
  <li>\(Y = \mathrm{GroupNorm}(\mathrm{Concat}( \mathrm{head}_1, \ldots, \mathrm{head}_h ))\)</li>
  <li>\(\mathrm{MSR}(X) = (\mathrm{swish}(XW_G) \odot Y) W_O\)</li>
</ul>

<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>

  <script src="https://cympfh.cc/resources/js/youtube.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>