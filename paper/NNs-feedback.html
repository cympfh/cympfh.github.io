<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Direct Feedback Alignment Provides Learning in Deep Neural Networks</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../resources/css/c.css">
  <link rel="stylesheet" href="resources/index.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 style="font-size:1em; position:absolute; left:0; top:-0.8em; font-size:2.0em !important">
    <a href='index.html'>
    <img src="../resources/img/identicon.png" style="position:relative; top:0.4em; width:1.3em;border-radius:0.8em;" /> paper/
    </a>
</h1>
<header>
<h1 class="title">Direct Feedback Alignment Provides Learning in Deep Neural Networks</h1>
</header>
<ul>
<li>
url: <a href=https://arxiv.org/abs/1609.01596>https://arxiv.org/abs/1609.01596</a>
</li>
<li>
tags: <span class="tag"><a href=index.html#深層学習>深層学習</a></span>
</li>
</ul>
<p>NNs における逆伝播 (BP) は、出力の誤差から始まってその偏微分式に従って、ちょうど順伝播の逆順に計算される. すなわち、各層の誤差は厳密には、その層の重みと、次の層 (出力方面) での誤差に依存する. 依存関係故に、順伝播の逆順に計算する必要がある.</p>
<p><a href="https://arxiv.org/abs/1411.0247">Feedback Alignment (FA)</a> ではこの誤差の計算の際に、層の重みを用いない. 普通、重みはランダムに初期化するわけだが、その依存性を少なくすることで学習の安定性を図る.</p>
<p>本論文で提案される Direct Feedback Alignment (DFA) は、FA 同様に層の重みを用いないことに加えて、 次の層での誤差の代わりに最終層の誤差を用いる. 数値計算上の発散や、初期層での誤差が薄れてしまうことを防ぐ. もはや順伝播の逆順に計算する必要すらないため、自由なルートを辿る逆伝播が可能になる. それを Indirect Feedback Alignment (IFA) と呼ぶ.</p>
<h2 id="方法">方法</h2>
<p>NNs は、例えば次のようなもの:</p>
<ul>
<li><span class="math inline">\(a_1 = W_1 x + b_1\)</span>
<ul>
<li><span class="math inline">\(h_1 = f(a_1)\)</span></li>
</ul></li>
<li><span class="math inline">\(a_2 = W_2 h_1 + b_2\)</span>
<ul>
<li><span class="math inline">\(h_2 = f(a_2)\)</span></li>
</ul></li>
<li><span class="math inline">\(a_y = W_3 h_2 + b_3\)</span>
<ul>
<li><span class="math inline">\(\hat{y} = f(a_y)\)</span></li>
</ul></li>
</ul>
<p><span class="math inline">\(\hat{y}\)</span> と真の <span class="math inline">\(y\)</span> との差から、各層での誤差 <span class="math display">\[\delta a_y = \hat{y} - y, \delta a_2, \delta a_3\]</span> を求めるとともに、 層の重みの更新を例えば次のようにやる (実際には学習率とか色々あるが): <span class="math display">\[\delta W_1 = - x&#39; \delta a_1, \delta W_2 = - h_1&#39; \delta a_2, \delta W_3 = - h_2&#39; \delta a_y\]</span> 以上を以って学習の1ステップとする.</p>
<h3 id="bp-逆伝播">BP (逆伝播)</h3>
<p>普通に、<span class="math inline">\(a_2\)</span>, <span class="math inline">\(a_1\)</span> に関して誤差を偏微分したら、次が導出されるハズ.</p>
<ul>
<li><span class="math inline">\(\delta a_2 = (W_3&#39;~\delta a_y) \bigodot f&#39;(a_2)\)</span></li>
<li><span class="math inline">\(\delta a_1 = (W_2&#39;~\delta a_2) \bigodot f&#39;(a_1)\)</span></li>
</ul>
<p>ここで <span class="math inline">\(\bigodot\)</span> は同じ型の行列またはベクトル同士について要素ごとの積を取る二項演算子.</p>
<div class="figure">
<img src="img/back-propagation.png" />

</div>
<h3 id="fa-feedback-alignment">FA (Feedback Alignment)</h3>
<ul>
<li><span class="math inline">\(\delta a_2 = (B_2~\delta a_y) \bigodot f&#39;(a_2)\)</span></li>
<li><span class="math inline">\(\delta a_1 = (B_1~\delta a_2) \bigodot f&#39;(a_1)\)</span></li>
</ul>
<p><span class="math inline">\(B_2, B_1\)</span> は各層に関して固定されたランダム行列.</p>
<h3 id="dfa-direct-">DFA (Direct-)</h3>
<p>Direct バージョンは次の層の誤差ではなく直接出力層の誤差に影響させる.</p>
<ul>
<li><span class="math inline">\(\delta a_2 = (B_2~\delta a_y) \bigodot f&#39;(a_2)\)</span></li>
<li><span class="math inline">\(\delta a_1 = (B_1~\delta a_y) \bigodot f&#39;(a_1)\)</span></li>
</ul>
<div class="figure">
<img src="img/direct-feedback-alignment.png" />

</div>
<h3 id="ifa-indirect-">IFA (Indirect-)</h3>
<p>Indirect バージョンは自分で自由に誤差の依存関係を作ることで逆伝播を設計する.</p>
<p>例えば</p>
<div class="figure">
<img src="img/indirect-feedback-alignment.png" />

</div>
<p>とするには、</p>
<ul>
<li><span class="math inline">\(\delta a_2 = (B_2~\delta a_1) \bigodot f&#39;(a_2)\)</span></li>
<li><span class="math inline">\(\delta a_1 = (B_1~\delta a_y) \bigodot f&#39;(a_1)\)</span></li>
</ul>
<p>手法としては以上.</p>
<h2 id="実験">実験</h2>
<p>BP/FA/DFA を MNIST/CIFAR-10/CIPHAR-100 で比較する. 性能は</p>
<p><span class="math display">\[BP &gt; DFA &gt; FA\]</span></p>
<p>に見える.</p>
<!--

  HTML として pandoc -B で include する.

  <H2> を列挙してそれらにリンクを貼った toc を id='toc' に埋め込む.
  markdown で書いてるだろうから例として次のような段落を書けばよい.

```
## INDEX
<div id=toc></div>
```

  used in
  - /memo/gnuplot
  - /memo/linux
  - /memo/imagemagick

-->
<script>
(function() {
  var sections = document.getElementsByTagName('h2');
  var i;
  var OL = document.createElement('ol');
  for (i=0; i < sections.length; ++i) {
    var LI = document.createElement('li');
    var A = document.createElement('a');
    A.innerHTML = sections[i].innerHTML;
    if (A.innerHTML.toUpperCase() == 'INDEX') continue;
    A.href = '#' + i;
    LI.appendChild(A);
    OL.appendChild(LI);

    var PREF = document.createElement('a');
    PREF.name = i;
    sections[i].appendChild(PREF);
  }

  var done = false;
  function work() {
    if (done) return;
    if ( document.getElementById('toc') === null) return; // no toc element
    document.getElementById('toc').appendChild(OL);
    done = true;
  };

  window.onload = work;
  setTimeout(work,800);
}());
</script>
</body>
</html>
