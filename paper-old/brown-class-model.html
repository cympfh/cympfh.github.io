<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="unidoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Class-based n-gram models of natural language</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="https://cympfh.cc/resources/css/youtube.css" />
  <link href="https://unpkg.com/prismjs@1.x.0/themes/prism.css" rel="stylesheet" />
  <script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-chtml-full.js"></script>
  <link href="resources/css/c.css" rel="stylesheet" />
  <link href="../resources/css/c.css" rel="stylesheet" />
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />

</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-send-o"></i></a>
</header>

<h1 class="title">Class-based n-gram models of natural language</h1>
<p><ul> <li>original paper: <a href=http://dl.acm.org/citation.cfm?id=176316>http://dl.acm.org/citation.cfm?id=176316</a></li> </ul> <div class='is-pulled-right'> <a class='tag is-blue' href=index.html#言語モデル>言語モデル</a> <a class='tag is-blue' href=index.html#自然言語処理>自然言語処理</a> </div></p>
<p>Brown+ら.</p>
<p><a href="memo/learning-phrase-patterns.md">Learning phrase patterns for Text Classification</a> の中で, "単語のクラスを1次マルコフモデル尤度を最大化することによって自動分類した" とあって引用されていた.</p>
<h2>Introduction</h2>
<p>noisy channel 経由で来た, 歪んだ英語の文章を元に戻したい. これが第一の議論である. それに関与することとして, 単語にクラスを当てはめることを統計的にしたい. これが第二の議論である.</p>
<h2>言語モデル</h2>
<p>次のような言語モデルを考える.</p>
<p>単語列 \(w_1, \ldots, w_k\) を条件付き確率</p>
\[P(w_k | w_1, \ldots, w_{k-1})\]
<p>で特徴づける. ここで \(w_1,\ldots,w_{k-1}\) のことを \(w_k\) の history と呼ぶことにする.</p>
<p>文章全体が出来上がる確率は先頭から順に生成されると仮定して,</p>
\[P(w_1,\ldots,w_k) = \prod_{i=1}^k Pr(w_k | w_1,\ldots,w_{k-1})\]
<h3>n-gram 言語モデル</h3>
<p>n-gram 言語モデルでは, history の内の最後の \(n-1\) words だけを見る. すなわち</p>
\[P(w_k | w_1,\ldots,w_{k-1}) = P(w_k | w_{k-n+1},\ldots,w_{k-1})\]
<p>とする.</p>
<p>ただし \(k &lt; n\) のときは勝手に短くする.</p>
<p>では確率 \(P(w_k | w_{k-n+1},\ldots,w_{k-1})\) をどっからもってくるか. training text における最尤推定を行う. すなわち数えて割合を出すことをする.</p>
<p>training text において (連続) 部分列 \(w_1,\ldots,w_k\) の頻度を \(C(w_1,\ldots,w_k)\) とするとき,</p>
\[Pr(w_n | w_1,\ldots,w_{n-1}) = \dfrac{C(w_1,\ldots,w_n)}{\sum_v C(w_1,\ldots,w_{n-1}, v)}\]
<h3>interpolated estimation (Jelinek and Mercer, 1980)</h3>
<p>語彙は十分多いことが望ましいが \(n\) が増えるに従って必要な語彙数は指数的に増える. しかしながら精度のためには \(n\) はできるだけ多い方がよい.</p>
<p>interpolated estimation はいくつかの言語モデル \(P^{(j)}\) を構築して, それらをcombineすることで \(P&#x27;\) を得る手法.</p>
\[P&#x27;(w_k | w_1,\ldots,w_{i-k}) = \sum_j \lambda_j(w_1,\ldots,w_{k-1}) P^{(j)}(w_k | w_1,\ldots,w_{i-k})\]
<p>重み \(\lambda_j\) は EMアルゴリズムで作る.</p>
<h2>Word Classes</h2>
<p>意味的にか, 構造的にか, ある語とある語が似ているということがある. 例えば <code>Thursday</code> , <code>Friday</code> とか.</p>
<p>vocabulary \(V\) , classes \(C\) があって, 語 \(w\) をclass \(c\) に写す写像を \(\pi\) とする.</p>
\[\pi : V \to C\]
<h3>n-gram class model</h3>
<p>写像 \(\pi\) が既に与えられた上で, クラスを用いた言語モデルを次のように定める. 単語列 \(w_1,\ldots,w_k,\ldots,w_N\) についてクラス列 \(c_i = \pi(w_i)\) があって,</p>
\[P(w_k | w_1,\ldots,w_{k-1}) = P(w_k | c_k) P(c_k | c_1,\ldots,c_k)\]
<p>さらに n-gram クラスモデルとは, 上の history を \(n-1\) に制限したもの:</p>
\[P(w_k | w_1,\ldots,w_{k-1}) = P(w_k | c_k) P(c_k | c_{k-n+1},\ldots,c_k)\]
<p>training text から, 右辺の2つの確率を最尤推定する.</p>
<p>1-gram であれば,</p>
<ul>
  <li>\(P(w | c) = \dfrac{C(w)}{C(c)}\)</li>
  <li>\(P(c) = \dfrac{C(c)}{C()}\)</li>
</ul>
<p>2-gram であれば,</p>
<ul>
  <li>\(P(w | c) = \dfrac{C(w)}{C(c)}\) (同じ)</li>
  <li>\(P(c_2 | c_1) = \dfrac{C(c_1, c_2)}{\sum_d C(c_1, d)}\)</li>
</ul>
<p>ただし空列の \(C()\) とは training text に登場する単語数のこと.</p>
<h3>尤度</h3>
<p>\(T=C()\) として, training data 全体の単語列を \(t_1,\ldots,t_T\) とする.</p>
\[L(\pi) = \dfrac{1}{T-1} \log P(t_2,\ldots,t_T | t_1)\]
<p>を \(\pi\) の尤度とする. 2-gram model の下でこれを式変形すると,</p>
\[L(\pi) = \sum_{w_1, w_2} \dfrac{C(w_1 w_2)}{T-1} \log P(c_2 | c_1) P(w_2 | c_2)\]
<p>さらにうわぁーってやると,</p>
\[L(\pi) = -H(w) + I(c1, c2)\]
<p>ここで, \(H\) はエントロピー, \(I\) は相互情報量. \(w\) は training text から降ってくる.</p>
<p>\(L(\pi)\) を最大化するような \(\pi\) を選択する, というのは, 相互情報量を最大化するようなクラス分類を選択することになる.</p>
<h3>最適化</h3>
<p>\(\max I\) を厳密に解くのは大変 (また最大であるかを判定するのも大変). 貪欲法で頑張る.</p>
<p>語彙数 \(V\) に対して欲しいクラスの数 \(C\) を決める ( \(C &lt; V\) ). 始めは全ての語彙は異なるクラスにあるとし, 一個ずつマージしてクラス数が \(C\) になったら止める. 始めはクラス数が \(V\) あるのでちょうど \(V-k\) 回マージした時点でクラス数は \(k\) 種類ある. それを</p>
\[C^k_1, \ldots,C^k_k\]
<p>とする.</p>
<p>\(1 \leq i &lt; j \leq k\) について \(C^k_i\) と \(C^k_j\) とをマージすることを考える.</p>
<p>次の4つの値を導入する:</p>
<ul>
  <li>\(p_k(l,m) = P(C^k_l, C^k_m)\)</li>
  <li>\(\def\pl{\mathrm{pl}}\pl_k(l) = \sum_m p_k(l,m)\)</li>
  <li>\(\def\pr{\mathrm{pr}}\pr_k(m) = \sum_l p_k(l,m)\)</li>
  <li>\(q_k(l,m) = p_k(l,m) \log \dfrac{p_k(l, m)}{\pl_k(l) \pr_k(m)}\)</li>
</ul>
<p>こうすると \(C^k\) の相互情報量は</p>
\[I_k = \sum_{l,m} q_k(l, m)\]
<p>で表される.</p>
<p>さて \(i\) と \(j\) をマージするならば, あたらしい \(i \oplus j\) クラスが出来て, 各値は</p>
<ul>
  <li>\(p_k(i \oplus j, m) = p_k(i, m) + p_k(j, m)\)</li>
  <li>\(q_k(i \oplus j, m) = p_k(i \oplus j, m) + \log \dfrac{p_k(i \oplus j, m)}{\pl_k(i \oplus j) \pr_k(m)}\)</li>
</ul>
<p>で更新されて, 相互情報量は</p>
\[I&#x27; = I_k - s_k(i) - s_k(j) + q_k(i,j) + q_k(j,i) + q_k(i+j,i+j) + \sum_{l \ne i,j} q_k(l, i+j) + \sum_{m \ne i,j} q_k(i+j,m)\]
<p>where \(s_k(i) = \sum_l q_k(l, i) + \sum_m q_k(i, m) - q_k(i,i)\)</p>
<p>というわけで \(I&#x27;\) を最大化するペア \((i,j)\) を探してマージしていけばよい.</p>
<blockquote>そのまま実装すると \(V^2\) で大変そう.</blockquote>
<h3>Classes gotten with this alogrithm</h3>
<p>次のようなものが同じクラスの語彙として得られた:</p>
<ol>
  <li>Friday, Monday, ... Sunday, weekends</li>
  <li>June, March, July ...</li>
  <li>people guys folks fellows ...</li>
  <li>down, backwards, ashore, sideways ...</li>
  <li>water, gas, coal, liquid ...</li>
  <li>had, hadn't hath would've could've ...</li>
</ol>

<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>

  <script src="https://cympfh.cc/resources/js/youtube.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>