<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Class-based n-gram models of natural language</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../resources/css/c.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-send-o" style="position:absolute; left:0.4em; top:0.4em; width:1.3em;border-radius:0.8em;"></i></a>
</header>
<header>
<h1 class="title">Class-based n-gram models of natural language</h1>
</header>
<ul>
<li>
original paper: <a href=http://dl.acm.org/citation.cfm?id=176316>http://dl.acm.org/citation.cfm?id=176316</a>
</li>
</ul>
<div class="is-pulled-right">
<p><a class='tag is-blue' href=index.html#言語モデル>言語モデル</a> <a class='tag is-blue' href=index.html#自然言語処理>自然言語処理</a></p>
</div>
<p>Brown+ら.</p>
<p><a href="memo/learning-phrase-patterns.md">Learning phrase patterns for Text Classification</a> の中で, &quot;単語のクラスを1次マルコフモデル尤度を最大化することによって自動分類した&quot; とあって引用されていた.</p>
<h2 id="introduction">Introduction</h2>
<p>noisy channel 経由で来た, 歪んだ英語の文章を元に戻したい. これが第一の議論である. それに関与することとして, 単語にクラスを当てはめることを統計的にしたい. これが第二の議論である.</p>
<h2 id="言語モデル">言語モデル</h2>
<p>次のような言語モデルを考える.</p>
<p>単語列 <span class="math inline">\(w_1, \ldots, w_k\)</span> を条件付き確率 <span class="math display">\[P(w_k | w_1, \ldots, w_{k-1})\]</span> で特徴づける. ここで <span class="math inline">\(w_1,\ldots,w_{k-1}\)</span> のことを <span class="math inline">\(w_k\)</span> の history と呼ぶことにする.</p>
<p>文章全体が出来上がる確率は先頭から順に生成されると仮定して, <span class="math display">\[P(w_1,\ldots,w_k) = \prod_{i=1}^k Pr(w_k | w_1,\ldots,w_{k-1})\]</span></p>
<h3 id="n-gram-言語モデル">n-gram 言語モデル</h3>
<p>n-gram 言語モデルでは, history の内の最後の <span class="math inline">\(n-1\)</span> words だけを見る. すなわち <span class="math display">\[P(w_k | w_1,\ldots,w_{k-1}) = P(w_k | w_{k-n+1},\ldots,w_{k-1})\]</span> とする.</p>
<p>ただし <span class="math inline">\(k &lt; n\)</span> のときは勝手に短くする.</p>
<p>では確率 <span class="math inline">\(P(w_k | w_{k-n+1},\ldots,w_{k-1})\)</span> をどっからもってくるか. training text における最尤推定を行う. すなわち数えて割合を出すことをする.</p>
<p>training text において (連続) 部分列 <span class="math inline">\(w_1,\ldots,w_k\)</span> の頻度を <span class="math inline">\(C(w_1,\ldots,w_k)\)</span> とするとき, <span class="math display">\[Pr(w_n | w_1,\ldots,w_{n-1}) = \dfrac{C(w_1,\ldots,w_n)}{\sum_v C(w_1,\ldots,w_{n-1}, v)}\]</span></p>
<h3 id="interpolated-estimation-jelinek-and-mercer-1980">interpolated estimation (Jelinek and Mercer, 1980)</h3>
<p>語彙は十分多いことが望ましいが <span class="math inline">\(n\)</span> が増えるに従って必要な語彙数は指数的に増える. しかしながら精度のためには <span class="math inline">\(n\)</span> はできるだけ多い方がよい.</p>
<p>interpolated estimation はいくつかの言語モデル <span class="math inline">\(P^{(j)}\)</span> を構築して, それらをcombineすることで <span class="math inline">\(P&#39;\)</span> を得る手法. <span class="math display">\[P&#39;(w_k | w_1,\ldots,w_{i-k}) = \sum_j \lambda_j(w_1,\ldots,w_{k-1}) P^{(j)}(w_k | w_1,\ldots,w_{i-k})\]</span></p>
<p>重み <span class="math inline">\(\lambda_j\)</span> は EMアルゴリズムで作る.</p>
<h2 id="word-classes">Word Classes</h2>
<p>意味的にか, 構造的にか, ある語とある語が似ているということがある. 例えば <code>Thursday</code>, <code>Friday</code> とか.</p>
<p>vocabulary <span class="math inline">\(V\)</span>, classes <span class="math inline">\(C\)</span> があって, 語 <span class="math inline">\(w\)</span> をclass <span class="math inline">\(c\)</span> に写す写像を <span class="math inline">\(\pi\)</span> とする. <span class="math display">\[\pi : V \to C\]</span></p>
<h3 id="n-gram-class-model">n-gram class model</h3>
<p>写像 <span class="math inline">\(\pi\)</span> が既に与えられた上で, クラスを用いた言語モデルを次のように定める. 単語列 <span class="math inline">\(w_1,\ldots,w_k,\ldots,w_N\)</span> についてクラス列 <span class="math inline">\(c_i = \pi(w_i)\)</span> があって, <span class="math display">\[P(w_k | w_1,\ldots,w_{k-1}) = P(w_k | c_k) P(c_k | c_1,\ldots,c_k)\]</span></p>
<p>さらに n-gram クラスモデルとは, 上の history を <span class="math inline">\(n-1\)</span> に制限したもの: <span class="math display">\[P(w_k | w_1,\ldots,w_{k-1}) = P(w_k | c_k) P(c_k | c_{k-n+1},\ldots,c_k)\]</span></p>
<p>training text から, 右辺の2つの確率を最尤推定する.</p>
<p>1-gram であれば,</p>
<ul>
<li><span class="math inline">\(P(w | c) = \dfrac{C(w)}{C(c)}\)</span></li>
<li><span class="math inline">\(P(c) = \dfrac{C(c)}{C()}\)</span></li>
</ul>
<p>2-gram であれば,</p>
<ul>
<li><span class="math inline">\(P(w | c) = \dfrac{C(w)}{C(c)}\)</span> (同じ)</li>
<li><span class="math inline">\(P(c_2 | c_1) = \dfrac{C(c_1, c_2)}{\sum_d C(c_1, d)}\)</span></li>
</ul>
<p>ただし空列の <span class="math inline">\(C()\)</span> とは training text に登場する単語数のこと.</p>
<h3 id="尤度">尤度</h3>
<p><span class="math inline">\(T=C()\)</span> として, training data 全体の単語列を <span class="math inline">\(t_1,\ldots,t_T\)</span> とする.</p>
<p><span class="math display">\[L(\pi) = \dfrac{1}{T-1} \log P(t_2,\ldots,t_T | t_1)\]</span> を <span class="math inline">\(\pi\)</span> の尤度とする. 2-gram model の下でこれを式変形すると, <span class="math display">\[L(\pi) = \sum_{w_1, w_2} \dfrac{C(w_1 w_2)}{T-1} \log P(c_2 | c_1) P(w_2 | c_2)\]</span></p>
<p>さらにうわぁーってやると, <span class="math display">\[L(\pi) = -H(w) + I(c1, c2)\]</span> ここで, <span class="math inline">\(H\)</span>はエントロピー, <span class="math inline">\(I\)</span>は相互情報量. <span class="math inline">\(w\)</span> は training text から降ってくる.</p>
<p><span class="math inline">\(L(\pi)\)</span>を最大化するような <span class="math inline">\(\pi\)</span> を選択する, というのは, 相互情報量を最大化するようなクラス分類を選択することになる.</p>
<h3 id="最適化">最適化</h3>
<p><span class="math inline">\(\max I\)</span> を厳密に解くのは大変 (また最大であるかを判定するのも大変). 貪欲法で頑張る.</p>
<p>語彙数 <span class="math inline">\(V\)</span> に対して欲しいクラスの数 <span class="math inline">\(C\)</span> を決める (<span class="math inline">\(C &lt; V\)</span>). 始めは全ての語彙は異なるクラスにあるとし, 一個ずつマージしてクラス数が <span class="math inline">\(C\)</span> になったら止める. 始めはクラス数が <span class="math inline">\(V\)</span> あるのでちょうど <span class="math inline">\(V-k\)</span> 回マージした時点でクラス数は <span class="math inline">\(k\)</span> 種類ある. それを <span class="math display">\[C^k_1, \ldots,C^k_k\]</span> とする.</p>
<p><span class="math inline">\(1 \leq i &lt; j \leq k\)</span> について <span class="math inline">\(C^k_i\)</span> と <span class="math inline">\(C^k_j\)</span> とをマージすることを考える.</p>
<p>次の4つの値を導入する:</p>
<ul>
<li><span class="math inline">\(p_k(l,m) = P(C^k_l, C^k_m)\)</span></li>
<li><span class="math inline">\(\def\pl{\mathrm{pl}}\pl_k(l) = \sum_m p_k(l,m)\)</span></li>
<li><span class="math inline">\(\def\pr{\mathrm{pr}}\pr_k(m) = \sum_l p_k(l,m)\)</span></li>
<li><span class="math inline">\(q_k(l,m) = p_k(l,m) \log \dfrac{p_k(l, m)}{\pl_k(l) \pr_k(m)}\)</span></li>
</ul>
<p>こうすると <span class="math inline">\(C^k\)</span> の相互情報量は <span class="math display">\[I_k = \sum_{l,m} q_k(l, m)\]</span> で表される.</p>
<p>さて <span class="math inline">\(i\)</span> と <span class="math inline">\(j\)</span> をマージするならば, あたらしい <span class="math inline">\(i \oplus j\)</span> クラスが出来て, 各値は</p>
<ul>
<li><span class="math inline">\(p_k(i \oplus j, m) = p_k(i, m) + p_k(j, m)\)</span></li>
<li><span class="math inline">\(q_k(i \oplus j, m) = p_k(i \oplus j, m) + \log \dfrac{p_k(i \oplus j, m)}{\pl_k(i \oplus j) \pr_k(m)}\)</span></li>
</ul>
<p>で更新されて, 相互情報量は <span class="math display">\[I&#39; = I_k - s_k(i) - s_k(j) + q_k(i,j) + q_k(j,i) + q_k(i+j,i+j) + \sum_{l \ne i,j} q_k(l, i+j) + \sum_{m \ne i,j} q_k(i+j,m)\]</span> where <span class="math inline">\(s_k(i) = \sum_l q_k(l, i) + \sum_m q_k(i, m) - q_k(i,i)\)</span></p>
<p>というわけで <span class="math inline">\(I&#39;\)</span> を最大化するペア <span class="math inline">\((i,j)\)</span> を探してマージしていけばよい.</p>
<blockquote>
<p>そのまま実装すると <span class="math inline">\(V^2\)</span> で大変そう.</p>
</blockquote>
<h3 id="classes-gotten-with-this-alogrithm">Classes gotten with this alogrithm</h3>
<p>次のようなものが同じクラスの語彙として得られた:</p>
<ol type="1">
<li>Friday, Monday, ... Sunday, weekends</li>
<li>June, March, July ...</li>
<li>people guys folks fellows ...</li>
<li>down, backwards, ashore, sideways ...</li>
<li>water, gas, coal, liquid ...</li>
<li>had, hadn't hath would've could've ...</li>
</ol>
<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>
</body>
</html>
