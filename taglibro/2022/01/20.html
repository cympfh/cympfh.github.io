<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="description" content="推論の圏, 予測の合成" />
  <meta property="og:url" content="http://cympfh.cc/taglibro">
  <meta property="og:title" content="Thu Jan 20 2022" />
  <meta property="og:description" content="推論の圏, 予測の合成" />
  <meta property="og:image" content="http://cympfh.cc/resources/img/identicon.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@cympfh" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Thu Jan 20 2022</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../../resources/css/c.css" />
  <link rel="stylesheet" href="../../../resources/css/c.css" />
  <link rel="stylesheet" href="../../../resources/css/youtube.css" />
  <link rel="stylesheet" href="../../../resources/css/web_emb.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
  <div class="taglibro">
<header class="page-header">
    <a href='../../index.html'><i class="fas fa-fast-backward"></i></a>
</header>
<header>
<h1 class="title">Thu Jan 20 2022</h1>
</header>
<p>日本において Coinhive が合法化された. ここが EU なら, ユーザーに許可を取る限りで合法だとなっていたはずだ. ユーザーの意図に反してユーザーのデバイス資源を占有するプログラムであって, そして提供側が利益を得るという点で, web 広告と比較されがちだ. 産業になってしまったものは誰も今から違法化するわけにいかない. 日本が海外から見れば児童ポルノだと言われるようなイラストを取ってつけたゲームを許してるのも, 日本の産業になってしまったからだと思われる.</p>
<p>といった文章を書くことで日記の体裁を保った. 以下駄文.</p>
<h2 id="推論の圏">推論の圏</h2>
<p><span class="math inline">\(\def\Hom{\mathrm{Hom}}\)</span></p>
<h3 id="推論">推論</h3>
<p>最終的にやりたいことは推論だ.</p>
<ul>
<li>対象は（ありとあらゆる）集合</li>
<li>入力（特徴量） <span class="math inline">\(A\)</span> から, 出力（ラベル集合） <span class="math inline">\(B\)</span> を推論する操作を <span class="math inline">\(A \to B\)</span> と書く</li>
</ul>
<p>推論という操作は大雑把には写像だ. MNIST というデータセットは 28x28 の白黒画像に手書きした数字 (0-9) が1つ書かれている.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST database - Wikipedia</a></li>
</ul>
<p>一枚の画像が入力として与えられるから, 書かれてる数字を推論せよという問題がある. 28x28 の画像は（画像という構造を忘れて）便宜的に <span class="math inline">\(\{0,1\}\)</span> が <span class="math inline">\(784 = 28 \times 28\)</span> 個並んだものとすれば,</p>
<ul>
<li><span class="math inline">\(A = \{ 0,1\}^{784}\)</span></li>
<li><span class="math inline">\(B = \{ 0,1,2,\ldots, 9 \}\)</span></li>
</ul>
<p>これがこの推論における入出力になる. <span class="math inline">\(a \in A\)</span> が与えられたとき, ちょうど１つの <span class="math inline">\(b \in B\)</span> を与えるとしてよい. <span class="math inline">\(B\)</span> の上の確率分布を与えるようなものとしてもよい（この二つに本質的な差は内容に思える）. ここでは単に写像として書く.</p>
<p>さて <span class="math inline">\(A,B\)</span> があったときに, その間に写像さえあればいくらでも推論を作る事ができる. どんな入力に対しても <span class="math inline">\(0 \in B\)</span> を予測するようなものも推論ではある. もちろん我々が欲しいのは, 良い推論である. 推論の良し悪しというのは普通データセットの上で定量的に評価する. 適当に用意した推論 <span class="math inline">\(f \colon A \to B\)</span> があったときに, それより良い推論 <span class="math inline">\(f&#39;\)</span> があるかどうか, あるならどうすれば手に入るかを考えるのは自然だ.</p>
<p>さて以上のようにありとあらゆる集合を対象にして, その間のありとあらゆる推論を射とする圏を <strong>推論の圏</strong> と呼ぶことにする.</p>
<h3 id="予測モデル">予測モデル</h3>
<p>個々のやりたい推論に対して, 良さそうな推論を緻密に構築するのは難しい. しかしながら, 推論 <span class="math inline">\(f \colon A \to B\)</span> から <span class="math inline">\(f\)</span> から具体的なパラメータだけを分離した <span class="math inline">\(\overline{f}\)</span> は構成が容易だ. この <span class="math inline">\(\overline{f}\)</span> を推論 <span class="math inline">\(f\)</span> のための <strong>予測モデル</strong> という. とり得るパラメータ空間を <span class="math inline">\(P\)</span> とすれば, 推論 <span class="math display">\[f \colon A \to B\]</span> に対して <span class="math display">\[\overline{f} \colon A \times P \to B\]</span> と書こう. なぜこのような <span class="math inline">\(\overline{f}\)</span> を構成するのが簡単かといえば, 一般に <span class="math inline">\(\mathbb R^M \to \{0,1,\ldots,N-1\}\)</span> といった形式の推論のための予測モデルはすでにいくらでも提案されており, 書籍やインターネットで手に入るからである. 上記の <span class="math inline">\(M, N\)</span> を今回の <span class="math inline">\(A,B\)</span> に沿うように調整するだけでよい.</p>
<p>普通, <span class="math display">\[\overline{f} \colon A \times P \to B\]</span> があったら <span class="math inline">\(\overline{f}\)</span> を予測モデル, <span class="math inline">\(A\)</span> を入力, <span class="math inline">\(P\)</span> をパラメータと呼ぶが, しかしそのような意味付けはプログラマ（人間）のための便宜上のものに過ぎない. 推論の圏においては <span class="math inline">\((A \times P)\)</span> から <span class="math inline">\(B\)</span> を予測する推論でしかない.</p>
<p>この形式の推論（予測モデル）では, 射 <span class="math inline">\(\overline f\)</span> を採用すると一度決めたら, 「良い推論を探す」とは「良いパラメータ <span class="math inline">\(p \in P\)</span> を探す」ことにほかならない. この操作のことを <strong>学習</strong> という.</p>
<p>定数 <span class="math inline">\(p \in P\)</span> のことを単集合 <span class="math inline">\(I = \{\ast\}\)</span> からの射 <span class="math inline">\(p \colon I \to P\)</span> と同一視すれば, <span class="math display">\[f \circ (1_A \times p) \colon A \to B\]</span> がシンプルに <span class="math inline">\(A\)</span> から <span class="math inline">\(B\)</span> への推論になる. この合成を「パラメータを <span class="math inline">\(p\)</span> で固定する」という.</p>
<h3 id="教師データ">教師データ</h3>
<p>学習に必要な道具について考える. 推論 <span class="math inline">\(A \to B\)</span> を得るためには, <span class="math inline">\(a \in A\)</span> については <span class="math inline">\(b \in B\)</span> を（あるいは確率分布を）予測すべきという教師データが必要である.</p>
<ul>
<li>教師データとは（小さな）部分集合
<ul>
<li><span class="math inline">\(\mathcal D \subset A \times B\)</span></li>
</ul></li>
</ul>
<p>もし全ての <span class="math inline">\(a \in A\)</span> に対して教師データが用意されているなら, コレ自体を良い推論として使うことができる. しかし, 教師データとして与えられたものだけそれを使って推論して, 与えられていない場合には代わりに <span class="math inline">\(\bot\)</span> を出力する推論が構築できる.</p>
<p><span class="math display">\[\Delta \colon A \to B \cup \{\bot\}\]</span> <span class="math display">\[\Delta(a) = \begin{cases} b    &amp; \text{ if } (a,b) \in D \\ \bot &amp; \text{ else } \\ \end{cases}\]</span></p>
<p>この定義は <span class="math inline">\(a\)</span> が重複して含まれてる場合を考慮してないといった問題があるが細かいことは省く.</p>
<p><span class="math inline">\(\Delta\)</span> のことも <span class="math inline">\(\mathcal D\)</span> と同様に単に教師データと呼ぶことにする.</p>
<h3 id="生成">生成</h3>
<p>推論 <span class="math inline">\(f \colon A \to B\)</span> に対して, 推論 <span class="math display">\[g \colon B \to A\]</span> であって, <span class="math display">\[f \circ g = 1_B\]</span> なるような <span class="math inline">\(g\)</span> を <span class="math inline">\(f\)</span> に対する生成と呼ぶ（<span class="math inline">\(1_B\)</span> は恒等射）.</p>
<p>もし <span class="math inline">\(f\)</span> の生成があれば, そのようなものの1つを <span class="math inline">\(f^\top\)</span> と書く.</p>
<p>特に, 教師データ <span class="math inline">\(\mathcal D\)</span> による推論 <span class="math inline">\(\Delta\)</span> については, 簡単にその生成 <span class="math inline">\(\Delta^\top\)</span> が構築できる.</p>
<p>以下では一般の推論 <span class="math inline">\(f\)</span> から生成 <span class="math inline">\(f^\top\)</span> が構築できるとする.</p>
<h3 id="誤差伝播に基づいた学習">誤差伝播に基づいた学習</h3>
<p>深層学習における具体的な方法をここで述べることはしない. 圏のレベルでは, 次の操作があるということだけ言うことにする.</p>
<p>二つの推論 <span class="math inline">\(f,g \colon A \to B\)</span> について, <span class="math display">\[L \colon \Hom(A, B) \times A \times \Hom(A, B) \to A\]</span> がある.</p>
<p>ここで <span class="math inline">\(\Hom(X,Y)\)</span> とは <span class="math inline">\(X\)</span> から <span class="math inline">\(Y\)</span> への推論の集合を表す.</p>
<p>これは何をやりたいかというと, <span class="math inline">\(f(a)\)</span> の値が <span class="math inline">\(g(a)\)</span> の値に近づくような, <span class="math inline">\(a&#39; = L(f,a,g)\)</span> を <span class="math inline">\(a\)</span> の周りで探す操作であることを期待している.</p>
<ul>
<li>予測モデル
<ul>
<li><span class="math inline">\(\overline f \colon A \times P \to B\)</span></li>
</ul></li>
<li>学習データ
<ul>
<li><span class="math inline">\(\Delta \colon A \to B\)</span></li>
</ul></li>
</ul>
<p>があるとき, 型を揃えるために, <span class="math inline">\(\Delta \circ \pi_1 \colon A \times P \to B\)</span> を用意すれば, <span class="math display">\[L(\overline f, -, \Delta \circ \pi_1) \colon A \times P \to A \times P\]</span> が得られる. 便宜的にこれを <span class="math display">\[\overline f \triangleleft \Delta ~ \colon ~ A \times P \to A \times P\]</span> と書こう.</p>
<p>これの <span class="math inline">\(\to P\)</span> だけ抽出して, <span class="math display">\[\pi_2 \circ ( \overline f \triangleleft \Delta ) ~ \colon ~ A \times P \to P\]</span> とすれば, これがパラメータ更新の操作になっている.</p>
<ul>
<li>HILL CLIMBING LEARNING ALGORITHM:
<ol type="1">
<li>データセット <span class="math inline">\(\mathcal D\)</span> を用意する</li>
<li>予測モデル <span class="math inline">\(\overline f\)</span> を決定する</li>
<li>パラメータ <span class="math inline">\(p \in P\)</span> はランダムに初期化する</li>
<li>Until 十分に良い推論になるまで
<ul>
<li><span class="math inline">\(a \in A\)</span> を上手にサンプリングしてくる</li>
<li><span class="math inline">\(p \leftarrow \pi_2(\overline{f} \triangleleft \Delta)(a, p)\)</span></li>
</ul></li>
</ol></li>
</ul>
<p>ただし <span class="math inline">\(\bot\)</span> の分は上手に無視することにする.</p>
<h2 id="予測の合成">予測の合成</h2>
<p>ここまでの説明では, ちょうど1つの予測モデルで最初の入力から最後の出力を推論していた. 実際にはこの推論は複数に分解してもよい. これは, パラメータの探索空間が狭まるとか実用上の効果がある.</p>
<p>MNIST の例で A, B の間に C という余計な空間を挟む. 例えば <span class="math inline">\(M\)</span> 次元ユークリッド空間などである.</p>
<ul>
<li><span class="math inline">\(A = \{ 0,1\}^{784}\)</span></li>
<li><span class="math inline">\(C = \mathbb R^M\)</span></li>
<li><span class="math inline">\(B = \{ 0,1,2,\ldots, 9 \}\)</span></li>
</ul>
<p>予測 <span class="math inline">\(A \to B\)</span> を作るのに, 予測 <span class="math inline">\(f \colon A \to C\)</span> と <span class="math inline">\(g \colon C \to B\)</span> をそれぞれ作って, これの合成 <span class="math inline">\(g \circ f\)</span> として構築することを考える.</p>
<p>予測モデルを使う場合はそれぞれに</p>
<ul>
<li>予測モデル <span class="math inline">\(\overline f \colon A \times P \to C\)</span></li>
<li>予測モデル <span class="math inline">\(\overline g \colon C \times Q \to B\)</span></li>
</ul>
<p>を与える必要がある. この合成は,</p>
<p><span class="math display">\[\overline{g} \circ \overline{f} \colon A \times (P \times Q) \to B\]</span></p>
<p>として動く.</p>
<h3 id="学習">学習</h3>
<p>やればできる.</p>
<ul>
<li>教師データ <span class="math inline">\(\Delta \colon A \to B\)</span></li>
<li>ランダムなパラメータ <span class="math inline">\(p \in P, q \in Q\)</span> で初期化</li>
<li>以下ループ
<ul>
<li><span class="math inline">\(f_p = \overline f \circ (1_A \times p) \colon A \to C\)</span> を信用して <span class="math inline">\(\overline g\)</span> を学習する
<ul>
<li><span class="math inline">\(\Delta \circ f_p^\top \colon C \to B\)</span> を教師データだとして,</li>
<li><span class="math inline">\(\overline{g} \triangleleft (\Delta \circ f_p^\top)\)</span> で <span class="math inline">\(q \in Q\)</span> をアップデート</li>
</ul></li>
<li><span class="math inline">\(g_q\)</span> を信用して
<ul>
<li><span class="math inline">\((g_q \triangleleft f_p^\top) \circ f_p \colon A \to C\)</span> を教師データとして,</li>
<li><span class="math inline">\(\overline{f} \triangleleft \cdots\)</span> で <span class="math inline">\(p \in P\)</span> をアップデート</li>
</ul></li>
</ul></li>
</ul>
<footer>
    <p class="is-pulled-right">
    @cympfh / mail@cympfh.cc
    </p>
</footer>
<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>
  </div>
  <script src="../../../resources/js/youtube.js"></script>
  <script src="../../../resources/js/web_emb.js"></script>
</body>
</html>
