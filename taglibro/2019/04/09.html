<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Tue Apr 09 2019</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../../../resources/css/c.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="page-header">
    <a href='../../index.html'><i class="fa fa-save" style="position:absolute; left:0.4em; top:0.4em; width:1.3em;border-radius:0.8em;"></i></a>
</header>
<header>
<h1 class="title">Tue Apr 09 2019</h1>
</header>
<p>行列分解モデルがなぜ、ノイズの除去に役立つのだっけ. NNの文脈で語ると、一度低ランクに写す回帰モデル (autoencoder ではないけどそれっぽい何か) に相当するから、と考えれば、たしかにノイズ除去になっていそう. そもそも行列分解モデルは確率的PCAに相当するものとして示されたものらしい.</p>
<p>ところで純粋な数学的事実に著作権はないと思う.</p>
<p>例えば「角谷の不動点定理」は数え切れない程多く本に載ってあるが、 私がその中の一冊を読んで、その定理をこのブログに載せても、それ自体は何も問題がないと思う. また、名前が付いているくらい（「角谷の不動点定理」といえば、あの定理だと分かる）古典的な論文ともなると、いちいち、その初出の論文だかなんだかを引用すらしない. そんな本を見たことがない.</p>
<p>で、思ったのは、読んでる専門書の内容を、ここに丸写しするのは問題があるのだろうか. 本のスキャンを載せるのは断然アウトだと思う. 自分の言葉で書き直すのは全くセーフだと思う. どの程度まで書き直せばセーフだろうか？ ある本に <span class="math inline">\(1+1=2\)</span> であると書いてあったので私がブログに <span class="math inline">\(1+1=2\)</span> なんだよ、と何の引用も無く書いたら、誰かが怒ってくるのだろうか？</p>
<h2 id="行列分解モデル">行列分解モデル</h2>
<p>ノイズや事前分布は基本的に全部ガウス分布だとする.</p>
<p>2種類のデータを考える. それぞれ <span class="math inline">\(n\)</span>, <span class="math inline">\(m\)</span> 通りの値を取るとする. それを <span class="math inline">\(1,\ldots,n\)</span> と <span class="math inline">\(1,\ldots,m\)</span> だとする. 仮に <span class="math inline">\(n\)</span> 種類の方を「ユーザー」, <span class="math inline">\(m\)</span> 種類の方を「アイテム」と呼ぶことにする. 各ユーザー <span class="math inline">\(i \in n\)</span> と アイテム <span class="math inline">\(j \in m\)</span> について, ある実数 <span class="math inline">\(X_{ij}\)</span> が観測されるとする. 従って <span class="math inline">\(X\)</span> は <span class="math inline">\(n \times m\)</span> 行列として表現される.</p>
<p>ここで観測には当然ながらノイズが乗るものだから, <span class="math inline">\(X_{ij}\)</span> は真の値 <span class="math inline">\(\tilde{X}_{ij}\)</span> にノイズ <span class="math inline">\(E_{ij}\)</span> が加えられたものである. <span class="math display">\[X = \tilde{X}+E\]</span> ここでノイズは <span class="math display">\[E_{ij} \sim \mathcal N(0, \sigma^2)\]</span> というガウス分布からサンプリングされた値だ.</p>
<p><span class="math inline">\(X\)</span> を観測したときに, 真の値 <span class="math inline">\(\tilde{X}\)</span> を推測したいと思うのは自然な欲求である. 行列分解モデルは <span class="math inline">\(X\)</span> から <span class="math inline">\(\tilde{X}\)</span> を推測するための枠組みである.</p>
<p>小さな自然数 <span class="math inline">\(d &lt; \min\{n,m\}\)</span> を適当に選んで, 2つの行列 <span class="math inline">\(A \in \mathbb R^{n \times d}\)</span>, <span class="math inline">\(B \in \mathbb R^{m \times d}\)</span> によって <span class="math display">\[\tilde{X} = A B^T\]</span> なのだとモデル化する. <span class="math inline">\(\tilde{X}\)</span> のランクは自動的に <span class="math inline">\(d\)</span> 以下になっていることに註意.</p>
<p><span class="math inline">\(A\)</span> の第 <span class="math inline">\(i\)</span> 行ベクトルを <span class="math inline">\(A_i\)</span> とし, 同様に <span class="math inline">\(B\)</span> の第 <span class="math inline">\(j\)</span> 行ベクトルを <span class="math inline">\(B_j\)</span> とすれば, <span class="math display">\[X_{ij} = A_i B_j^T + E_{ij}\]</span> と言っている.</p>
<p>ノイズにガウス分布を仮定した場合の最尤推定は自乗誤差最小化になる. 即ち, 行列分解モデルが <span class="math inline">\(A,B,\sigma\)</span> の下で <span class="math inline">\(X_{ij}\)</span> を観測する確率は,</p>
<p><span class="math display">\[P(X_{ij} \mid A,B,\sigma) =
\beta \exp\left[ - \alpha (X_{ij} - A_i B_j^T)^2 \right]\]</span></p>
<p>ここで <span class="math inline">\(\alpha, \beta\)</span> は正の定数.</p>
<p>同様に <span class="math inline">\(X\)</span> を観測する確率は, <span class="math display">\[P(X \mid A,B,\sigma) =
\beta^{nm} \exp\left[ - \sum_n \sum_m \alpha (X_{ij} - A_i B_j^T)^2 \right]\]</span> である. 最尤推定をするならば, これを最大化すればよくて, 結局 <span class="math display">\[\min_X \sum_n \sum_m (X_{ij} - A_i B_j^T)^2\]</span> を解けばよいことになる.</p>
<blockquote>
<p>言い忘れてたけど（言わなくても分かると思って）, 自然数 <span class="math inline">\(n, m\)</span> に対して <span class="math inline">\(n=\{1,\ldots,n\}\)</span>, <span class="math inline">\(m=\{1,\ldots,m\}\)</span> という記号も混ぜて用いている.</p>
</blockquote>
<h2 id="線形一層回帰モデル">線形一層回帰モデル</h2>
<p>線形で（活性化しない）、中間層が一層だけの多層パーセプトロンによる回帰モデルを考える. 問題設定はさっきと同じで.</p>
<p><span class="math inline">\(X\)</span> または <span class="math inline">\(\tilde{X}\)</span> は <span class="math inline">\(n \times m\)</span> 行列なので, 左から掛けることによって <span class="math inline">\(\mathbb R^n \to \mathbb R^m\)</span> なる（線形）関数だと見做せる.</p>
<p><span class="math display">\[X : \mathbb R^n \to \mathbb R^m\]</span> <span class="math display">\[X : x \mapsto Xx\]</span></p>
<p>線形関数は基底の写し方だけを調べれば決まるので, <span class="math inline">\(\mathbb R^n\)</span> の基底として, <span class="math inline">\(i\)</span> 番目だけ <span class="math inline">\(1\)</span> で他が全部ゼロなベクトル <span class="math display">\[\delta_i = \left[ 0,\ldots,0,1,0,\ldots,0 \right]^T\]</span> を <span class="math inline">\(x\)</span> として入れることを考えると, <span class="math display">\[X \delta_i = x_i\]</span> ここで, <span class="math inline">\(X\)</span> の <span class="math inline">\(i\)</span> <strong>列</strong> 目を <span class="math inline">\(x_i ( \in \mathbb R^m)\)</span> と書いた.</p>
<p>今からこの <span class="math inline">\(X\)</span> 関数を線形一層回帰モデルで表現する. ここで中間層のユニット数を <span class="math inline">\(d &lt; \min\{n,m\}\)</span> とする.</p>
<p><span class="math display">\[\mathbb R^n \to \mathbb R^d \to \mathbb R^m\]</span></p>
<p>最初の1層目を行列 <span class="math inline">\(B \in \mathbb R^{d \times n}\)</span>, 2層目を行列 <span class="math inline">\(A \in \mathbb R^{d \times m}\)</span> とすれば, この多層パーセプトロンは, <span class="math display">\[y = ABx\]</span> と書ける.</p>
<p>ところでやっぱりガウス分布のノイズを仮定すると, 真の <span class="math inline">\(y\)</span> と <span class="math inline">\(ABx\)</span> との差の自乗誤差の最小化が最尤推定になる. ここで <span class="math inline">\(x\)</span> を <span class="math inline">\(\delta_1, \ldots, \delta_n\)</span> で考えると, <span class="math display">\[\min \sum_n \| x_i - (AB) \delta_i \|_2^2\]</span> ベクトルのL2ノルムを分解して書けば, <span class="math display">\[\min \sum_n \sum_m ( X_{ij} - A_i B_j )^2\]</span> となって, さっきの行列分解と全く同じことをしていたことがわかる (それはそう).</p>
<blockquote>
<p>というわけでこの2つのモデルは等価である. それはいいとして. 結局なんでこれが良いモデルなんだろう？</p>
</blockquote>
<footer>
    <p class="is-pulled-right">
    @cympfh / cympfh@gmail.com
    </p>
</footer>
<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>
</body>
</html>
