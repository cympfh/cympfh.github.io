<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="unidoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>カーネル法 - 概要</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="https://cympfh.cc/resources/css/youtube.css" />
  <link href="https://unpkg.com/prismjs@1.x.0/themes/prism.css" rel="stylesheet" />
  <script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-chtml-full.js"></script>
  <link href="../resources/css/c.css" rel="stylesheet" />
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />

</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-stumbleupon"></i></a>
</header>

<h1 class="title">カーネル法 - 概要</h1>
<p><p class=date style='text-align: right'>2016-09-18 (Sun.)</p> <div class='is-pulled-right'> <a class='tag is-red' href=index.html#機械学習>機械学習</a> </div> % カーネル法によるパターン解析 第3章</p>
<h2>index</h2>
<p><div id=toc></div></p>
<h2>特徴空間での線形回帰</h2>
<h3>(普通の) 線形回帰</h3>
<p>まず普通の線形回帰を考える. 予測関数</p>
\[g(x) = \langle w,x \rangle = \sum_i w_i x_i\]
<p>を用いてパターンは</p>
\[f(x, y; w) = \mathcal{L}(y, g(x)) = |y - g(x)|^2 \approx 0\]
<p>がよく用いられる. \(w\) を行ベクトル、行列 \(X\) を列ベクトル \(x_1, x_2, \ldots, x_n\) を並べたものとする.</p>
\[\mathcal{L}(y, X; w) = |y - Xw|^2 = (y-Xw)&#x27; (y-Xw) ~~(\geq 0)\]
<p>ここで \(&#x27;\) は行列の転置. この値は非負であってゼロの場合に \(f\) は exact pattern となる. 従って最小化問題を解こうとなる.</p>
\[\frac{\partial \mathcal{L}}{\partial w} = -2X&#x27;(y - Xw) = 0\]
\[\iff X&#x27;Xw = X&#x27;y\]
<p>\((X&#x27;X)\) の逆行列が存在するなら</p>
\[w = (X&#x27;X)^{-1}X&#x27;y\]
<p>が導かれる. \((X&#x27;X)\) はデータ \(x\) が \(n\) 次元だとすると \(n \times n\) 行列で、逆行列を求めるのが最も高コストで \(O(n^3)\) .</p>
<p>\((X&#x27;X)\) の逆行列が存在するなら</p>
\[w = (X&#x27;X)^{-1}y = X&#x27; X (X&#x27;X)^{-2}y = X&#x27; \alpha\]
<p>と書ける. \(X&#x27;\) はデータを横に並べたもので、 \(\alpha\) は実数を縦に並べた列ベクトル. つまり、この右辺が意味していることは、</p>
\[w = \sum_i \alpha_i x_i\]
<p>ということ. 即ち、超平面 \(w\) は結局訓練データの線型結合であるということ.</p>
<h3>リッジ回帰 (Ridge regression)</h3>
<p>次を損失関数とする回帰をリッジ回帰という. ただし \(\lambda\) は適当な正実数.</p>
\[\mathcal{L}_\lambda(y,X;w) = \lambda |w|^2 + |y - Xw|^2\]
<p>先ほどと同様にこの最小化をしたいので \(w\) で偏微分してゼロなのを解くと</p>
\[\lambda w + X&#x27;Xw = X&#x27;y.\]
<p>逆行列が存在すれば</p>
\[w = (X&#x27;X+\lambda I)^{-1} X&#x27;y\]
<p>を導けるが \(\lambda &gt; 0\) の時、実はそれはいつも逆行列が存在する. これもやっぱり次元数 \(n\) に関して \(O(n^3)\) の計算コスト.</p>
<p>ところで、この \(w\) も結局データの線型結合であることは次のようにして確かめられる:</p>
<ul>
  <li>\(\lambda w + X&#x27;Xw = X&#x27;y\)</li>
  <li>\(\iff \lambda w = X&#x27;(y - Xw)\)</li>
  <li>\(\iff w = X&#x27; (y - Xw) &#x2F;\lambda = X&#x27; \alpha\)</li>
</ul>
<p>\(\alpha\) について陽に解く</p>
<ul>
  <li>\(w = (X&#x27;X+\lambda I)^{-1} X&#x27;y\)</li>
  <li>\(\iff X&#x27;\alpha = (X&#x27;X+\lambda I)^{-1} X&#x27;y\) (because \(w = X&#x27;\alpha\) )</li>
  <li>\(\iff X&#x27;y = (X&#x27;X+\lambda I) X&#x27; \alpha = X&#x27; (XX&#x27; + \lambda I) \alpha\)</li>
  <li>\(\Rightarrow \alpha = (XX&#x27;+\lambda I)^{-1} y\)</li>
</ul>
<p>また予測は</p>
\[g(x) = \langle w,x \rangle = \langle \sum_i \alpha_i x, x \rangle = \alpha&#x27; x = y&#x27;(XX&#x27; + \lambda I) k\]
<p>ここで \(k\) は列ベクトルで \(k_i = \langle x_i, x \rangle\) .</p>
<p>\(G = XX&#x27;\) とする. これはグラム行列 (Gram matrix) と呼ばれ、 ちょうど \(G_{i,j} = \langle x_i, x_j \rangle\) となっている. \(G\) の大きさはデータ数 \(m\) に対して \(m \times m\) 行列. 上の \(g\) を計算するコストは 次元数 \(n\) とデータ数 \(m\) に関して \(O(nm)\) .</p>
<h3>カーネル拡張</h3>
<p>データ \(x \in \mathbb{R}^n\) を別な空間 \(\phi(x) \in F \subseteq \mathbb{R}^N\) に写す. この空間でのリッジ回帰は先程の結論の \(x\) を \(\phi(x)\) に置き換えるだけでよくて</p>
<ul>
  <li>\(g(x) = y&#x27; (G+\lambda I)^{-1} k\)</li>
  <li>
    where
    <ul>
      <li>\(G_{i,j} = \langle \phi(x_i), \phi(x_j) \rangle\)</li>
      <li>\(k_i = \langle \phi(x_i), \phi(x) \rangle\)</li>
    </ul>
  </li>
</ul>
<p>次を満たす \(\kappa\) をカーネルという:</p>
\[\kappa(x, z) = \langle \phi(x), \phi(z) \rangle.\]
<h2>PCA</h2>
<p>PCA は \(x \in \mathbb{R}^n\) をより低い次元である \(X&#x27;X\) の固有ベクトルが張る \(k\) 次元空間に写す.</p>
<p>\(X&#x27;X\) の固有ベクトルを \(v_1, v_2, \ldots, v_k\) とするとき データ \(x\) は</p>
\[\langle x, v_i \rangle_{i=1,2,\ldots,k}\]
<p>に写される. この内積を \(\kappa(x, v_i)\) に置き換えることで特徴空間でのPCAを考えることができる. これは kernel PCA と言われる.</p>
<h2>クラスタリング (例)</h2>
<p>ユークリッド空間上の点集合をユークリッド距離に基づいてクラスタリングすることを考える.</p>
\[|x-z|^2 = \langle x,x \rangle + \langle z,z \rangle - 2 \langle x,z \rangle\]
<p>であることを考えると、ここをカーネルに置き換える拡張が考えられる.</p>
<h2>集合カーネル (例)</h2>
<p>Introduction でも述べたように、 \(\phi\) によって写した先が実空間 \(\mathbb{R}^N\) であれば元は何でも良い. すなわちカーネル \(\kappa\) も、任意のペアを実空間 \(\mathbb{R}\) に写しさえすれば何でも良い. 例えば集合に関する \(\phi, \kappa\) を設計することができる.</p>
<p>例えば集合 \(A_1, A_2 (\subseteq U)\) に関するカーネル</p>
\[\kappa(A_1, A_2) = 2^{|A_1 \cap A_2|}\]
<p>は、積集合 ( \(A_1 \cap A_2\) ) の部分集合の数を表す. これを再現する \(\phi\) を作ることはできる.</p>
<ul>
  <li>\(I = \{ D : D \subseteq U \}\) をインデックスとして</li>
  <li>
    \(A \mapsto \phi(A) = \{ D \mapsto \chi(D \subseteq A) : D \in I \}\)
    <ul>
      <li>
        where
        <ul>
          <li>\(\chi(Prop) = 1\) when \(Prop\) holds on</li>
          <li>\(\chi(Prop) = 0\) otherwise</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2>カーネルのモジュール性</h2>
<p>以上見てきたように、多くのパターン解析アルゴリズムがカーネルで拡張できる可能性を持つ.</p>

<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>

  <script src="https://cympfh.cc/resources/js/youtube.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>