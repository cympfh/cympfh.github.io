% Deep Metric Learning using Triplet Network
% https://arxiv.org/abs/1412.6622
% 距離学習 類似度学習

## 手法概要

類似度を学習するための Triplet network を提案する.
Siamese network のある種の拡張になっている.

Siamese network の入力は2つのアイテムのペアと、その2つの類似度であったが、
Triplet network では名前の通り、入力がアイテムの三組になっている.

- 入力はアイテムの三組 $(x, x^+, x^-)$
    - $x$: 適当なターゲット
    - $x^+$: $x$ と近い (類似してる) アイテム
    - $x^-$: $x$ と遠い (類似していない) アイテム
- これらを共通した一つのニューラネットワーク $f$ に通す
    - $(x, x^+, x^-) \mapsto (f(x), f(x^+), f(x^-)) = (z, z^+, z^-)$

```dot
digraph {
    node [shape=plaintext];
    rankdir=TB;
    bgcolor=transparent;
    "x+" -> f -> "z+";
    x -> f -> z;
    "x-" -> f -> "z-";
    "z+" -> z [label="d+" style=dashed arrowhead=none arrowtail=none dir=back];
    z -> "z-" [label="d-" style=dashed arrowhead=none arrowtail=none];
    f [shape=rect];
    {rank=same x "x+" "x-"}
    {rank=same z "z+" "z-"}
}
```

このときに
$(x, x^+)$ 及び
$(x, x^-)$ に関して Siamese network 的なことを行う.

すなわち、

- $x, x^+$ 間の距離 ($d^+$ とする) をゼロにすること
- $x, x^-$ 間の距離 ($d^-$ とする) は適度に離れてること

を目指す.

このアイテム間の距離として、$z=f(x)$ 同士の L2 距離を算出し、さらに次のように正規化を行う.

$$d^+ = \frac{\exp \| z - z^+ \| }{\exp \| z - z^+ \| + \exp \| z - z^- \| }$$
$$d^- = \frac{\exp \| z - z^- \| }{\exp \| z - z^+ \| + \exp \| z - z^- \| }$$

すなわち、 $d^+ + d^- = 1$ となるように正規化する.
$d^+ = 0$ のとき、$d^- = 1$ となるべきなので、損失は次のようにする.
これを triplet loss という.

- $L(x, x^+, x^-) = d(y, y^+)^2 + (1 - d(y, y^-))^2$

いま $d^+ + d^- = 1$ としてるので

- $L(x, x^+, x^-) = 2 d(y, y^+)^2$

となって結局、$d^+$ の最小化、または $d^-$ の最大化と一致する.

### 亜種?

この論文を読むより先に "triplet loss" でググってヒットした
[ディープラーニングによるファッションアイテム検出と検索 - VASILY DEVELOPERS BLOG](http://tech.vasily.jp/entry/detection_and_retrieval#f-44448e46)
という記事を読んでおり、その記事では損失関数として、

- $L(x, x^+, x^-) = \max \{ d(y, y^+) - d(y, y^-) - margin, 0 \}$
    - $d$ は L2距離

というのが "triplet loss" だとして紹介されていた.
どうも "contrastive loss" との折衷に見える.

恐らく、これは
[FaceNet](https://arxiv.org/abs/1503.03832)
で "triplet loss" として提案されたもののことだろう.
論文が出た順で言うと、本文書で紹介している論文のほうが古い.

## 評価実験

- Siamese ネットワークとの比較をしている.
    - MNIST/Cifar10/SVHN/STL10 でラベルの同一性に関して類似度を学習、埋め込み表現を得てその後は線形SVMまたはk近傍で分類をした.
    - まあ、そんな良くない

## 感想

- 初めに訓練データを三組に直す処理が必要で、実際に試そうと思うとすこし面倒くさい
    - 場合によってはラベル付きデータを用意するより楽かもしれない
        - 「同じラベルがついているペア」というものを探しさえすればいいので
        - 「異なるラベルがついているペア」というのは簡単なヒューリスティックで作りやすい
            - ランダムに選べば大抵異なるラベルであるかもしれない
- 三組を用意することは、Siamese Network で、ポジティブとネガティブとを均等に学習させられる利点である
- 実際に距離を、ポジティブとネガティブとの比にしていることの良さは、ネガティブとの距離を具体的に決めなくていいことである
    - 直感的に、ポジティブとは近ければ近いほど良く、距離ゼロという教師データが使えるが、ネガティブの場合は、ある一定上離れてくれてば良いだけなので、どれだけの距離が適切なのか分からない
        - "contrastive loss" は、一定以上 (マージン) 離れてればゼロになるような損失関数である
- 評価実験について
    - 毎回思うが、距離/類似度学習は分類のための手法ではないと思う


