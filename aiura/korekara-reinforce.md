% 強化学習
% 2017-12-16 (Sat.)
% 強化学習

## 教科書

「これからの強化学習」

## 状態遷移モデル

略

## 定義

### 報酬 (reward)

$r(s, a, s')$ で表される関数

### 収益 (G)

- ある時点 (時刻) での収益 $G$ とは、それ以降で得られる報酬の見込み
    - 例えばそれ以降 T ステップで得られる報酬の平均など
- 基本的にこれの最大化を目指す最適化を行う

例えば

- $s_0$
- $s_{t+1} = a_t(s_t)$ (アクション $a$ は遷移関数、または確率分布)
- $R_t = r(s_{t-1}, a_t, s_t)$

という収益に対して、

**割引報酬和 (discounted total reward)** とは、定数 $0 < \gamma < 1$ に対して
$$G_t = \sum_{i=0}^\infty \gamma^i R_{t+1} = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots$$
というもので、これがよく使われる.

### 方策 ($\pi$)

アクション $a$ の選び方のルールのこと.
便宜上、アクション集合の上の確率分布として与える.

### 状態価値関数 ($V$)

状態 $s$ の良さを収益 $G$ 及び方策 $\pi$ の下で計るもの.
$$V^\pi(s) = \mathbb{E}^\pi \left[ G_t : s_t = s \right]$$
すなわち、状態が $s$ の時点での $G_t$ の平均で、ただし方策として $\pi$ を採用したもの.

### 行動価値関数 ($Q$)

状態 $s$ の時点でアクション $a$ を採用することの良さを、$V$ と同様に計るもの.
$$Q^\pi(s, a) = \mathbb{E}^\pi \left[ G_t : s_t = s, a_t = a \right]$$

### 最適 - 関数

$V, Q$ は共に方策 $\pi$ を取る.
値を最大化するような $\pi$ を採用するものを、最適状態価値、最適行動価値という.
$$V^*(s) = \max_\pi V^\pi(s)$$
$$Q^*(s, a) = \max_\pi Q^\pi(s, a)$$
