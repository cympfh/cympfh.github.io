<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Multilayer Perceptron Algebra</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../resources/css/c.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-send-o" style="position:absolute; left:0.4em; top:0.4em; width:1.3em;border-radius:0.8em;"></i></a>
</header>
<header>
<h1 class="title">Multilayer Perceptron Algebra</h1>
</header>
<ul>
<li>
original paper: <a href=https://arxiv.org/abs/1701.04968>https://arxiv.org/abs/1701.04968</a>
</li>
</ul>
<div class="is-pulled-right">
<p><a class='tag is-blue' href=index.html#深層学習>深層学習</a> <a class='tag is-blue' href=index.html#代数>代数</a></p>
</div>
<h2 id="index">INDEX</h2>
<div id="toc">

</div>
<h2 id="概要">概要</h2>
<p>簡単のため、多層パーセプトロン (MLP) を扱う. 2つ以上の MLP の合成、或いは MLP の分解などの具体的な方法を定め、その性質を Accuracy を指標にして調べることを目的とする. ただし現状 (1701.04968v1) では、ほとんど自明な性質しか描かれてない上に数式に誤植が多いので脳内補完しながら読んだ. 当たり前だが出現する対訳は全て私が勝手に与えた</p>
<h2 id="私の感想">私の感想</h2>
<p>代数操作を定義するのに、いちいち、ここまで厳密に書かなくても、</p>
<p><span class="math display">\[(\mathcal{N}_1 + \mathcal{N}_2)(x)
=
\sigma\left(\omega\cdot\left[\begin{array}\\\mathcal{N}_1(x)\\\mathcal{N}_2(x)\end{array}\right] - \theta\right)\]</span></p>
<p>(<span class="math inline">\(\omega, \theta\)</span> がなんか <span class="math inline">\(\lambda\)</span> でうだうだ書かれる謎のパラメータ) って風に (外延的に) 定めて、具体的な構成はおまけページに書く程度にすればいいのに. なぜなら、この論文で定義として与えられた構成法は、わざわざ書く必要のないくらい、自明すぎる方法であるし、データ圧縮にもなってないので実用的ではないから.</p>
<h2 id="動機">動機</h2>
<p>予測器 <span class="math inline">\(\mathcal{X} \to \mathcal{Y}\)</span> を MLP で構成し、 ラベル付きのデータから教師あり学習をしたとする. ここで2つの異なるデータセット <span class="math inline">\(D_1\)</span> 及び <span class="math inline">\(D_2\)</span> を用いて学習したものを <span class="math inline">\(\mathcal{N}_1, \mathcal{N}_2\)</span> とする. どうすればこの2つの (学習済み) MLP から、 データセット <span class="math inline">\(D_1 \cup D_2\)</span> で学習したのと同等の MLP を得ることが出来るだろうか. もし出来るのであれば、そのような操作を和と定義して</p>
<p><span class="math display">\[\mathcal{N}_1 + \mathcal{N}_2\]</span></p>
<p>と書きたい. ただし、この目的は本研究では未だ達成されてないので註意.</p>
<h2 id="ell-層mlp-ell-layer-mlp"><span class="math inline">\(\ell\)</span> 層MLP (<span class="math inline">\(\ell\)</span>-layer MLP)</h2>
<blockquote>
<p>ここはみんな知ってることなのでそんなに厳密に書かない.</p>
</blockquote>
<p><span class="math inline">\(\mathcal{N}\)</span> が <span class="math inline">\(\ell\)</span> 層MLP であるとは、<span class="math inline">\(\ell\)</span> 個の正の整数</p>
<p><span class="math display">\[n_1, n_2, \ldots, n_\ell\]</span></p>
<p>があって、 次のような関数及び行列のこと.</p>
<ul>
<li>関数 <span class="math inline">\(b_k\)</span>:
<ul>
<li><span class="math inline">\(b_k(\mathcal{N}) = \mathbb{R}^{n_k}\)</span>
<ul>
<li>これは第 <span class="math inline">\(k\)</span> 層の空間のこと</li>
</ul></li>
<li><span class="math inline">\((k=1,2,\ldots, \ell)\)</span></li>
</ul></li>
<li>重み行列 <span class="math inline">\(\omega_k\)</span>
<ul>
<li><span class="math inline">\((k=1,2,\ldots, \ell-1)\)</span></li>
</ul></li>
<li>閾値ベクトル <span class="math inline">\(\theta_k\)</span>
<ul>
<li><span class="math inline">\((k=1,2,\ldots, \ell-1)\)</span></li>
</ul></li>
<li>関数 <span class="math inline">\(f_k : \mathbb{R}^{n_k} \to \mathbb{R}^{n_{k+1}}\)</span>
<ul>
<li><span class="math inline">\(f_k(x) = \sigma \left( \omega_k x - \theta_k \right)\)</span></li>
<li><span class="math inline">\((k=1,2,\ldots, \ell-1)\)</span>
<ul>
<li>ここで <span class="math inline">\(\sigma\)</span> は要素ごと (element-wise) のシグモイド関数</li>
</ul></li>
</ul></li>
</ul>
<p>そして <span class="math inline">\(f_1\)</span> から <span class="math inline">\(f_{\ell-1}\)</span> を順に合成した関数のことを <span class="math inline">\(\mathcal{N}\)</span> と同一視する:</p>
<p><span class="math display">\[\mathcal{N} = f_{\ell-1} \circ \cdots \circ f_2 \circ f_1\]</span></p>
<p>註意すべきこととして、 全ての層で <span class="math inline">\(\sigma\)</span> による活性化を仮定している. 従って <span class="math inline">\(\mathcal{N}\)</span> の値域は <span class="math inline">\((0, 1)^{n_\ell}\)</span>.</p>
<p>以上の定義は結局、関数 <span class="math inline">\(f_k\)</span> を定めるためのもので、 <span class="math inline">\(\omega_k\)</span> 及び <span class="math inline">\(\theta_k\)</span> (そしてそれらの次元を決定するための <span class="math inline">\(b_k\)</span>) はそのための <strong>パラメータ</strong> である. 本研究では、次のような状況を想定している. 即ち、まず MLP を構造を決定する <span class="math inline">\(b_k\)</span> を固定していて、 あるデータセットが与えられ、それに最適なパラメータを既に学習している. 学習済みのパラメータを含めて、MLP <span class="math inline">\(\mathcal{N}\)</span> と呼んでいる.</p>
<p>特に <span class="math inline">\(\mathcal{N}\)</span> のパラメータ <span class="math inline">\(\omega_k\)</span> のことを <span class="math inline">\(\omega_k(\mathcal{N})\)</span> と冗長に書くことにする. <span class="math inline">\(\theta_k\)</span> も同様.</p>
<h3 id="二値分類と多クラス分類">二値分類と多クラス分類</h3>
<ol type="1">
<li><p>二値分類するような <span class="math inline">\(\ell\)</span> 層 MLP <span class="math inline">\(\mathcal{N}\)</span> とは、 <span class="math inline">\(b_\ell(\mathcal{N}) = \mathbb{R}\)</span> であって、 <span class="math inline">\(\mathcal{Y} = \{+, -\}\)</span> で、 <span class="math inline">\(\mathcal{N}(x) &gt; 0.5\)</span> なら <span class="math inline">\(+ \in \mathcal{Y}\)</span> に、 さもなくば <span class="math inline">\(- \in \mathcal{Y}\)</span> に予測するもの.</p></li>
<li><p>多クラス分類するような <span class="math inline">\(\ell\)</span> 層 MLP <span class="math inline">\(\mathcal{N}\)</span> とは、 <span class="math inline">\(b_\ell(\mathcal{N}) = \mathbb{R}^m\)</span> であって (<span class="math inline">\(m\)</span> はクラス数)、 <span class="math inline">\(\mathcal{Y} = \{1,2,\ldots,m\}\)</span> で、 <span class="math inline">\(y = \text{argmax}_i \mathcal{N}(x)_i\)</span> に分類するようなもの.</p></li>
</ol>
<h4 id="正解率">正解率</h4>
<p>以上の予測の方法で、 データセット <span class="math inline">\(D\)</span> に関する <span class="math inline">\(\mathcal{N}\)</span> の正解率 (accuracy) を測って</p>
<p><span class="math display">\[\mathcal{A}_D(\mathcal{N})\]</span></p>
<p>と書く.</p>
<h2 id="mlp-操作の諸定義">MLP 操作の諸定義</h2>
<p>ここから MLP の変換・合成に関する操作をいくつか定義する.</p>
<h3 id="補mlp-complementary-net">補MLP (Complementary Net)</h3>
<p><span class="math inline">\(\ell\)</span> 層 MLP <span class="math inline">\(\mathcal{N}\)</span> に対して、関数の出力が <span class="math inline">\(y\)</span> のときに <span class="math inline">\(1-y\)</span> を出力するような <span class="math inline">\(\ell\)</span> 層 MLP を次のようにして定める. そしてこれを <span class="math inline">\(\mathcal{N}\)</span> の補MLPと呼び <span class="math inline">\(\mathcal{N}^c\)</span> と書く.</p>
<ul>
<li><span class="math inline">\(b_k(\mathcal{N}^c) = b_k(\mathcal{N})\)</span></li>
<li><span class="math inline">\(\omega_k(\mathcal{N}^c) = \begin{cases}\omega_k(\mathcal{N}) &amp; \text{when } k &lt; \ell -1 \\ - \omega_{\ell-1}(\mathcal{N}) &amp; \text{otherwise }\end{cases}\)</span></li>
<li><span class="math inline">\(\theta_k(\mathcal{N}^c) = \begin{cases}\theta_k(\mathcal{N}) &amp; \text{when } k &lt; \ell -1 \\ - \theta_{\ell-1}(\mathcal{N}) &amp; \text{otherwise }\end{cases}\)</span></li>
</ul>
<p>つまり、最後の層のパラメータだけ、符号を逆転させるのである.</p>
<h3 id="mlp和-sum-net">MLP和 (Sum Net)</h3>
<p><span class="math inline">\(b_1(\mathcal{N}_1) = b_1(\mathcal{N}_2)\)</span> かつ <span class="math inline">\(b_\ell(\mathcal{N}_1) = b_\ell(\mathcal{N}_2) = \mathbb{R}\)</span> であるような2つの <span class="math inline">\(\ell\)</span> 層 MLP <span class="math inline">\(\mathcal{N}_1\)</span> と <span class="math inline">\(\mathcal{N}_2\)</span> があるとする. これらの和 <span class="math display">\[\mathcal{N}_1 + \mathcal{N}_2\]</span> を <span class="math inline">\(\ell+1\)</span> 層 MLP として与える.</p>
<ul>
<li><span class="math inline">\(b_1(\mathcal{N}_1 + \mathcal{N}_2) = b_1(\mathcal{N}_1) = b_1(\mathcal{N}_2)\)</span></li>
<li><span class="math inline">\(b_k(\mathcal{N}_1 + \mathcal{N}_2) = b_k(\mathcal{N}_1) \otimes b_k(\mathcal{N}_2)\)</span> <span class="math inline">\((k=2,3,\ldots,\ell)\)</span></li>
<li><span class="math inline">\(b_{\ell+1}(\mathcal{N}_1 + \mathcal{N}_2) = \mathbb{R}\)</span></li>
<li><span class="math inline">\(\omega_1(\mathcal{N}_1 + \mathcal{N}_2) = \left[\begin{array}\\\omega_1(\mathcal{N}_1)\\\omega_1(\mathcal{N}_2)\end{array}\right]\)</span></li>
<li><span class="math inline">\(\omega_k(\mathcal{N}_1 + \mathcal{N}_2) = \left[\begin{array}\\\omega_k(\mathcal{N}_1)&amp;\\&amp;\omega_k(\mathcal{N}_2)\end{array}\right]\)</span> <span class="math inline">\((k=2,3,\ldots,\ell-1)\)</span></li>
<li><span class="math inline">\(\omega_\ell(\mathcal{N}_1 + \mathcal{N}_2) = \left[\begin{array}\\\lambda&amp;\lambda\end{array}\right]\)</span></li>
<li><span class="math inline">\(\theta_k(\mathcal{N}_1 + \mathcal{N}_2) = \left[\begin{array}\\\theta_k(\mathcal{N}_1)\\\theta_k(\mathcal{N}_2)\end{array}\right]\)</span> <span class="math inline">\((k=1,2,\ldots,\ell-1)\)</span></li>
<li><span class="math inline">\(\theta_\ell(\mathcal{N}_1 + \mathcal{N}_2) = \frac{1}{2} \lambda\)</span></li>
</ul>
<p><span class="math inline">\(\lambda\)</span> はたぶん適当な実数パラメータだと思う. 何の説明も無いけど.</p>
<p>要するに、 <span class="math inline">\(\mathcal{N}_1\)</span> と <span class="math inline">\(\mathcal{N}_2\)</span> という2つの計算があるから、その計算過程をそれぞれ並べて計算するような MLP を構成することができる. 当然2つの出力があるので、それを1つにまとめるような一層を、最後に加えている. <span class="math inline">\(\lambda\)</span> はその２つの結果のまとめ方に関するパラメータである. (だからって何でこうなってるのか分からない. もっといい方法が在るだろう.)</p>
<h4 id="mlp-一般和-multi-sum-net">MLP 一般和 (Multi-Sum Net)</h4>
<p>2つのMLPの和を定義したから次は一般に <span class="math inline">\(J\)</span> 個のMLP</p>
<p><span class="math display">\[\mathcal{N}_1, \mathcal{N}_2, \ldots, \mathcal{N}_J\]</span></p>
<p>の和 <span class="math display">\[\sum_j \mathcal{N}_j\]</span> を定義する. まあ大体上と同じなので大体省略するけど、違いは</p>
<ul>
<li><span class="math inline">\(\theta_\ell(\sum_j \mathcal{N}_j) = \frac{1}{J} \lambda\)</span></li>
</ul>
<p>くらい. うーん、 <span class="math inline">\(\theta_k\)</span> は掛け算するわけじゃないのにこれ何の意味があるんだろう.</p>
<h3 id="mlp-差-difference-net">MLP 差 (Difference Net)</h3>
<p><span class="math display">\[\mathcal{N}_1 - \mathcal{N}_2 = \mathcal{N}_1 + \mathcal{N}_2^c\]</span></p>
<h3 id="mlp-内積-i-product">MLP 内積 (I-Product)</h3>
<p><span class="math inline">\(b_\ell(\mathcal{N}_1) = b_\ell(\mathcal{N}_2) = \mathbb{R}\)</span> であるような2つの <span class="math inline">\(\ell\)</span> 層 MLP <span class="math inline">\(\mathcal{N}_1, \mathcal{N}_2\)</span> の積 (内積)</p>
<p><span class="math display">\[\mathcal{N}_1 \times \mathcal{N}_2\]</span></p>
<p>を <span class="math inline">\(\ell+1\)</span> 層 MLP として定義する.</p>
<ul>
<li><span class="math inline">\(b_k(\mathcal{N}_1 \times \mathcal{N}_2) = b_k(\mathcal{N}_1) \otimes b_k(\mathcal{N}_2)\)</span> <span class="math inline">\((k=1,2,\ldots,\ell)\)</span></li>
<li><span class="math inline">\(b_{\ell+1}(\mathcal{N}_1 \times \mathcal{N}_2) = \mathbb{R}\)</span></li>
<li><span class="math inline">\(\omega_k(\mathcal{N}_1 \times \mathcal{N}_2) = \left[\begin{array}\\\omega_k(\mathcal{N}_1)&amp;\\&amp;\omega_k(\mathcal{N}_2)\end{array}\right]\)</span> <span class="math inline">\((k=1,2,\ldots,\ell-1)\)</span></li>
<li><span class="math inline">\(\omega_\ell(\mathcal{N}_1 \times \mathcal{N}_2) = \left[\begin{array}\\\lambda&amp;\lambda\end{array}\right]\)</span></li>
<li><span class="math inline">\(\theta_k(\mathcal{N}_1 \times \mathcal{N}_2) = \left[\begin{array}\theta_k(\mathcal{N}_1)\\\theta_k(\mathcal{N}_2)\end{array}\right]\)</span> <span class="math inline">\((k=1,2,\ldots,\ell-1)\)</span></li>
<li><span class="math inline">\(\theta_\ell(\mathcal{N}_1 \times \mathcal{N}_2) = \frac{3}{2} \lambda\)</span></li>
</ul>
<p>和と違うのは最初の入力からすでに並列になってることくらい. 最後の <span class="math inline">\(\theta_\ell\)</span> は意味不明.</p>
<h4 id="mlp-一般内積">MLP 一般内積</h4>
<p><span class="math display">\[\prod_j \mathcal{N}_j\]</span></p>
<h2 id="特性-mlp-characteric-mlp">特性 MLP (Characteric MLP)</h2>
<p>あるデータ集合</p>
<p><span class="math display">\[\mathcal{X} \subset \mathbb{R}^m\]</span></p>
<p>からの有限のサンプリング (データセット)</p>
<p><span class="math display">\[D \subset \mathcal{X}\]</span></p>
<p>があるとする. <span class="math inline">\(\mathcal{X}\)</span> に関する特性関数とは、データ点 <span class="math inline">\(x\)</span> が <span class="math inline">\(\mathcal{X}\)</span> に属するかどうかで <span class="math inline">\(1, 0\)</span> を出力する関数であるが、 これに相当する、特性関数ならぬ <strong>特性MLP</strong> を <span class="math inline">\(D\)</span> から構成することを考える. 実際には <span class="math inline">\(\sigma\)</span> で活性化してるので厳密に <span class="math inline">\(1, 0\)</span> が出力されることはないが、 話を簡単にするために、厳密にそういう出力されるものを考える. また <span class="math inline">\(D\)</span> しか与えられないのでそから <span class="math inline">\(\mathcal{X}\)</span> を推定する必要がある. これらの事情から次の2つを要請する.</p>
<ol type="1">
<li><span class="math inline">\(D\)</span> の任意の要素 <span class="math inline">\(x\)</span> に対して <span class="math inline">\(1\)</span> を出力する</li>
<li><span class="math inline">\(D\)</span> の任意の点とも (ユークリッド距離で) <span class="math inline">\(\epsilon\)</span> だけ離れてる点 <span class="math inline">\(x\)</span> に対して <span class="math inline">\(0\)</span> を出力する</li>
</ol>
<p>特にこの2つ目であるが、<span class="math inline">\(D\)</span> から <span class="math inline">\(\mathcal{X}\)</span> を推測するために、 ある程度小さい正の数 <span class="math inline">\(\epsilon\)</span> だけ、どの <span class="math inline">\(D\)</span> の要素とも離れていれば、 <span class="math inline">\(\mathcal{X}\)</span> には属さないと仮定したネガティブサンプリングを行っている. すなわち、 <span class="math inline">\(D\)</span> に対して <span class="math inline">\(\epsilon\)</span> 近傍を (<span class="math inline">\(D_\epsilon\)</span>) の補集合 <span class="math inline">\(D_\epsilon^c\)</span> からサンプリングを行う.</p>
<p>改めて書くと、 点集合 <span class="math inline">\(D\)</span> の特性 MLP <span class="math inline">\(\mathcal{K}_D\)</span> とは、</p>
<ol type="1">
<li><span class="math inline">\(\mathcal{K}_D(D) = 1\)</span></li>
<li><span class="math inline">\(\mathcal{K}_D(D_\epsilon^c) = 0\)</span></li>
</ol>
<p>を近似的に満たすものと定める.</p>
<h3 id="特性-mlp-の性質">特性 MLP の性質</h3>
<p>正解率は全部、任意のあるデータセット (固定) について測るものとして添字を省略する (だよね?).</p>
<ul>
<li><span class="math inline">\(\mathcal{A}(\mathcal{K}_D^c) = \mathcal{A}(\mathcal{K}_{D^c})\)</span>
<ul>
<li><span class="math inline">\(\mathcal{K}_D^c = (\mathcal{K}_D)^c\)</span> だよね、普通に考えて.</li>
<li><span class="math inline">\(D^c = \mathbb{R}^n \setminus D_\epsilon\)</span> だと思う.</li>
</ul></li>
<li><span class="math inline">\(\mathcal{A}(\mathcal{K}_{D_1 \cup D_2}) = \mathcal{A}(\mathcal{K}_{D_1} + \mathcal{K}_{D_2})\)</span></li>
<li><span class="math inline">\(\mathcal{A}(\mathcal{K}_{D_1 \times D_2}) = \mathcal{A}(\mathcal{K}_{D_1} \times \mathcal{K}_{D_2})\)</span></li>
</ul>
<h2 id="mlp-操作の諸定義-part-2">MLP 操作の諸定義 (part 2)</h2>
<p>詳細は略すが他にも次のような操作を定めている.</p>
<ol type="1">
<li>分解 (Component Net)
<ul>
<li>最後の次元 <span class="math inline">\(n_\ell\)</span> が <span class="math inline">\(&gt;1\)</span> の場合にその第 <span class="math inline">\(i\)</span> 成分だけ取り出すネットワークに分解できる</li>
<li>というのも今までほとんど <span class="math inline">\(n_\ell=1\)</span> の場合しか扱ってなかったので</li>
</ul></li>
<li>外積 (O-Product)
<ul>
<li>逆に、合成するもの</li>
</ul></li>
</ol>
<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>
</body>
</html>
