<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="index/線形分類/双対表現/特徴空間/まとめ/カーネル関数を用いることの利点/" />
  <meta property="og:url" content="http://cympfh.cc/taglibro">
  <meta property="og:type" content="article">
  <meta property="og:title" content="カーネル法 - Introduction" />
  <meta property="og:description" content="index/線形分類/双対表現/特徴空間/まとめ/カーネル関数を用いることの利点/" />
  <meta property="og:image" content="http://cympfh.cc/resources/img/identicon.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@cympfh" />
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>カーネル法 - Introduction</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../resources/css/c.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-stumbleupon"></i></a>
</header>
<header>
<h1 class="title">カーネル法 - Introduction</h1>
</header>
<p class="date" style="text-align: right">
2016-09-18 (Sun.)
</p>
<div class="is-pulled-right">
<p><a class='tag is-red' href=index.html#機械学習>機械学習</a></p>
</div>
<h2 id="index">index</h2>
<div id="toc">

</div>
<h2 id="線形分類">線形分類</h2>
<p>データ <span class="math inline">\(x \in \mathbb{R}^n\)</span> を二値 <span class="math inline">\(y \in \{-1,1\}\)</span> に分類したい. <strong>線形分類</strong> は <span class="math inline">\(\mathbb{R}^n\)</span> 上に超平面 (二次元上なら直線) を引いて、<span class="math inline">\(x\)</span> がそれより上にあるか下にあるかで分類する方法である. どちらでも良いのだが、超平面より上にあるなら <span class="math inline">\(y=+1\)</span> で、下にあるなら <span class="math inline">\(y=-1\)</span> とする.</p>
<p><span class="math inline">\(\mathbb{R}^n\)</span> 上の超平面はベクトル <span class="math inline">\(w \in \mathbb{R}^n\)</span> とスカラー <span class="math inline">\(b \in \mathbb{R}\)</span> を以って、方程式</p>
<p><span class="math display">\[\langle w, x \rangle + b = 0\]</span></p>
<p>と表せる. ここで <span class="math inline">\(\langle x_1, x_2 \rangle\)</span> はベクトル <span class="math inline">\(x_1, x_2\)</span> の内積のこと. 幾何的な文脈では <span class="math inline">\(w\)</span> は法線ベクトルで、<span class="math inline">\(b\)</span> はy切片である. どうせ次元 <span class="math inline">\(n\)</span> が1増えても誰も気にしないので、<span class="math inline">\(x\)</span> の後ろに定数 1 を結合して、<span class="math inline">\(w\)</span> の後ろに <span class="math inline">\(b\)</span> を結合することで、</p>
<p><span class="math display">\[\langle w, x\rangle = 0\]</span></p>
<p>と書ける. 式を少しでもすっきりさせたいので、こうする. データが上にあるか下にあるかは <span class="math inline">\(&gt;0\)</span> か <span class="math inline">\(&lt;0\)</span> で表現できる.</p>
<p>線形分類器とは、与えられた <span class="math inline">\(\{(x_i, y_i)\}_{i=1,2,\ldots,N}\)</span> について、それを正しく (もしかしたら完全にではなくとも、できるだけ) 分類するような超平面のことである. 即ち、 <span class="math display">\[\forall i, (\langle w,x_i \rangle &gt; 0 \land y_i = +1) \lor (\langle w,x_i \rangle &lt; 0 \land y_i = -1)\]</span> が成り立つような <span class="math inline">\(w\)</span> のことである.</p>
<p>また逆に、ある <span class="math inline">\(w\)</span> によって上が満たせるとき、データ <span class="math inline">\(\{x_i\}_i\)</span> は <strong>線形分離可能</strong> である、と言う.</p>
<p>また、そのような <span class="math inline">\(w\)</span> を計算する過程のことを、 <strong>線形分類器の学習</strong> と呼ぶ. 学習のためのアルゴリズムはいくつかあるが、<a href="NNs.html">パーセプトロン</a> は単純ながらも興味深い. この方法を踏襲した<a href="arow.pdf">AROW</a> や、また<a href="svm.html">SVM</a> も原理的にはこれと同じである. そしてこれら全てに共通してることとして次のような事実がある.</p>
<p>パーセプトロン、AROW、SVM で学習して得られた <span class="math inline">\(w\)</span> は、ある <span class="math inline">\(\{\alpha_i\}_{i=1,2,\ldots,N}\)</span> (<span class="math inline">\(\alpha_i \in \mathbb{R}\)</span>) が存在して次で表現される: <span class="math display">\[w = \sum_i \alpha_i x_i.\]</span> 即ち、学習される超平面は、学習データ <span class="math inline">\(x_i\)</span> の線形和で表される.</p>
<h2 id="双対表現">双対表現</h2>
<p><span class="math inline">\(w\)</span> の別な表現があるのなら、それを代入したくなるのが人情.</p>
<p><span class="math display">\[\langle w, x&#39; \rangle = \sum_i \alpha_i \langle x_i, x&#39; \rangle\]</span></p>
<h2 id="特徴空間">特徴空間</h2>
<p>線形分類器の致命的な問題は、そもそもデータは線形分離可能とは限らないこと. 曲面でしか分離できないようなケースも十分考えられる. 適当な関数 <span class="math inline">\(\phi\)</span> によって <span class="math inline">\(x \in \mathbb{R}^n \mapsto \phi(x) \in \mathbb{R}^m\)</span> と写すことで線形分離が可能になるかもしれない. この写した先の空間を <strong>特徴空間</strong> と呼ぶ.</p>
<p>特徴空間での線形分類を考えるには、前章までで述べていた <span class="math inline">\(x\)</span> を <span class="math inline">\(\phi(x)\)</span> に置き換えれば良い.</p>
<p>双対表現を用いる場合、<span class="math inline">\(\phi\)</span> 自体を陽に記述できる必要はない. なぜなら <span class="math display">\[\langle w, \phi(x&#39;) \rangle = \sum_i \alpha_i \langle \phi(x_i), \phi(x&#39;) \rangle\]</span> の右辺が計算できれば良いので、 <span class="math inline">\(\kappa(x_1, x_2) = \langle \phi(x_1), \phi(x_2) \rangle\)</span> なる <span class="math inline">\(\kappa\)</span> を直接考えても問題ないから. これは <span class="math inline">\(\kappa\)</span> の中身に依っては計算量の削減にもつながる. この <span class="math inline">\(\kappa\)</span> を <strong>カーネル関数</strong> と呼ぶ.</p>
<h2 id="まとめ">まとめ</h2>
<p>線形分類器は次のような <span class="math inline">\(f\)</span> である. <span class="math inline">\(f\)</span> の構成には、学習データ <span class="math inline">\(\{x_i\}_{i=1,2,\ldots,N}\)</span> 、パラメータ <span class="math inline">\(\{\alpha_i\}_{i=1,2,\ldots,N}\)</span> 及びカーネル関数 <span class="math inline">\(\kappa\)</span> が必要.</p>
<ul>
<li><span class="math inline">\(f(x) = \sum_{i=1}^N \alpha_i \kappa(x_i, x)\)</span>.</li>
</ul>
<p>そしてこれを用いた分類はこう:</p>
<ul>
<li><span class="math inline">\(y = +1\)</span> if <span class="math inline">\(f(x) &gt; 0\)</span> else <span class="math inline">\(-1\)</span>.</li>
</ul>
<h2 id="カーネル関数を用いることの利点">カーネル関数を用いることの利点</h2>
<p>もしかしたら計算量が削れるのも良いことだが、利点はそれだけではない. カーネル関数の型は <span class="math inline">\(\kappa: (T, T) \rightarrow \mathbb{R}\)</span> であれば何だって良い. 今まで、 <span class="math inline">\(x \in \mathbb{R}^n\)</span> としていたが、 <span class="math inline">\(x\)</span> を実ベクトルに限る必要は最早ない. <span class="math inline">\(x\)</span> が集合であってもグラフであっても、適切なカーネル関数を設計できるなら線形分類器を構成することができる.</p>
<p>あと、直接 <span class="math inline">\(\phi\)</span> を設計することは <span class="math inline">\(\kappa\)</span> を設計することに表現力は落ちる. <span class="math inline">\(\kappa\)</span> は <span class="math inline">\(\phi\)</span> から導けるからである. しかし逆に <span class="math inline">\(\phi\)</span> を <span class="math inline">\(\kappa\)</span> から導けるとは限らない. 例えば<a href="http://ibisforest.org/index.php?Gaussカーネル">rbfカーネル</a> は <span class="math inline">\(\phi\)</span> を陽に記述することはできないが、 なかなか強いカーネルとしてしばしば用いられる. この事実は <span class="math inline">\(\kappa\)</span> の設計は <span class="math inline">\(\phi\)</span> の設計 (つまりどんな特徴空間を用いるか、の設計) よりも大きな表現力を持つことを言っている.</p>
<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>
</body>
</html>
