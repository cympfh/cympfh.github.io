% Wed Jun 20 2018

## 日記 - ここ数ヶ月

自分が追ってる分野は機械学習・自然言語処理なのだが、論文を読んでいると、大げさな命名に辟易する.
ましてや、実質的にただ過去手法を引用してるだけなのに、勝手に改名してみたり.

### Introspective Neural Network

真面目な解説は
[paper/ST-INN](http://cympfh.cc/paper/ST-INN.html)
に書いた.
ここでは数式を省略して気持ちベースで説明する.

この提案手法は Introspective Learning をベースとする.
勝手に対訳をつけるなら「内省学習」とでも言おうか.
"ナニナニ learning"
という命名もよくある.
大抵、その "ナニナニ" には、日本人には馴染みのない、直観に訴えるような、術語らしくない語が入る.

そしてその「内省学習」を行うための Neural Network だから、 Introspective Neural Network ということらしい.
提案手法の名前はまるで広く使われているのだから当然であるかのように、頭文字を取って省略するのが普通だ.
この場合は INN となる.

さてこの INN が何かと言えば、Tu の生成モデルである.
ちなみに Tu の論文では Introspective なんていう語は登場しない.

Tu は、生成モデルと分類モデルが密接に関係することを発見した.
分かりやすいから、画像の生成モデル/分類モデルの例で言う.
それも2値分類で言うことにする.

C という物体が描かれている画像であるかどうか (C とは例えば「猫」) を判定するための機械が「分類モデル」であり、
C という物体が描かれている画像を生成する機械 (ランダムに色んなバリエーションを生み出すのが望ましい) が「生成モデル」である.

どちらにしても問題がある.
**「画像」** とは何か.
ピクセルが並んだ行列だと説明できる.
画像サイズ、つまり行列の大きさも無限通りあるが、これは便宜上固定するのが通例である.
適当に切り取ったり伸縮したりする.
しかしだとしても、そのバリエーションは膨大である.
なんせ、1ピクセルだけでもRGBの色がそれぞれ256通りあるから.
つまり言いたいのは、 **「画像空間」** は膨大に広いのだということ.

素朴な機械学習では、事例 (sample) を用いる.
つまり、どこかから拾ってきた C の画像、Cでない画像を沢山集めてくる.
しかし、膨大過ぎる「画像空間」はまるで網羅できないだろう.

ところで「画像」が何であるかを決めていなかった.
何でもありにしたからこそ、「画像空間」が膨大になってしまった.
例えば白と黒がランダムに並んだ画像 (ノイズ) は画像だろうか.
画像といえばもちろん画像だろうが...

アプリケーションを考えてしまうのも、1つの手だ.
実際には、分類モデルに、そんな砂嵐のようなノイズ画像を入力するだろうか.
また、生成したい画像は C の画像である. 砂嵐画像を考慮する必要があるだろうか.

**イイエ** が答えだとして話を進める.
入力される画像は少なくとも文脈から切り離して人間が見て解釈ができる画像だとする.
何かの物体が描かれているとか、風景の写真だとかだ.
抽象画だって、ノイズに比べれば、「線がつながっている」くらいの合理性はある (それでもあまり考慮したくないが).

というわけで、出来るだけ C であるような画像を集めることに意味がある.
犬の写真だっていい. ランダムに生成したノイズ画像に比べればずっと猫らしい.

Tu の生成モデルはこうだ.
初めは何でもありの画像空間を構成する.
各ピクセルは同様の確からしさで値を持つ.
ほとんどの場合、ただのノイズで、奇跡的な確率で C を描く.
そこからランダムに画像を繰り返し抽出し、C である確率を計算する.
その確率が高い方高い方に、画像空間を狭めていく.
いわばその空間そのものが C の生成モデルとなる.
これが生成モデルの作り方.

今の説明の中で、C である確率を計算する必要があった.
これが分類モデルの仕事である.
分類モデルはまず、普通に集めた事例から作る.
その事例は、まさに C である画像と C ではない画像を集めたものである.
しかしまだ、画像空間は膨大で、それに対して事例はあまりにも少なく、従ってそこから作った分類モデルも貧弱に違いない.
そこで先程の生成モデルを活用する.
生成モデルは、ノイズよりは少しはマシな C らしい画像を作り上げる.
これを C ではない画像だとして事例に加える.
生成モデルはまだ初めは貧弱だから、C ではないことが多いだろうから.

つまりこうだ.
生成モデルを作るためには弱い分類モデルが必要で、分類モデルを作るためには弱い生成モデルが必要である.
従って、その2つを交互に鍛えていけばいい.
この作り方で果たして真の分類モデル/生成モデルに収束するのかという問題については、Tu の論文を参照してほしいが、収束するというのが答えだ.

ここまでが INN の話で、更にコレに ST の部分を付け加えて ST-INN というのが提案手法なのだが、飽きたのでここで話は終わり.

## 日記 - ここ2年

法律というのは基本的に、我々の預かり知らぬところで発案されて議論されて、そうして決定したりするものだ.
新しい法が決定されれば、我々はそれに従うことが要請される.
議論に参加したものにとっては普及に努めるのが当然だし自分たちもそれに従うのだろうが、こちらとしては、どこまでも、知ったことではない.
議論に参加したかった、という、嫉妬なのかもしれない.

ところでコンサル出身というのは厄介だぞ.
