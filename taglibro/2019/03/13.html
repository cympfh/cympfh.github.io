<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Wed Mar 13 2019</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../../../resources/css/c.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="page-header">
    <a href='../../index.html'><i class="fa fa-save" style="position:absolute; left:0.4em; top:0.4em; width:1.3em;border-radius:0.8em;"></i></a>
</header>
<header>
<h1 class="title">Wed Mar 13 2019</h1>
</header>
<h2 id="日記---20190312">日記 - 2019/03/12</h2>
<p>朝の五時半頃に起きて六時半ごろに家を出た. 東京駅にツイたのが七時少し前. 東京駅のみどりの窓口は朝の五時半に開くそうで、さすがだ. 窓際の席を確保するのに便を二本見送って乗り込んだ. ところで駅弁を眺めていたら私と同様に眺める外国人のおっさんがいたのだが、見間違いでなければ知り合いだった. でも向こうは私の顔に覚えがないようだったので無視した.</p>
<h2 id="日記---20190313">日記 - 2019/03/13</h2>
<p>やはりお手軽ツールとして <code>fastText</code> をみんな使っていてよい. 私も好き. facebook AI research は実は fastText の後に <code>StarSpace</code> というのも出していたそうで、それを使って実験をしてる人も一人だけいた. そんなの全然聞いたことなかった. どうもテキストだけでなく画像なんかにも使えるツールらしい.</p>
<p>ところで単語、ないしはワードピースの埋め込み表現として当然に数ベクトルを使う. ユークリッド空間の中の点として扱えるのが嬉しいのだろう. 類似度には内積かユークリッド距離なんかが使えるのも分かりやすい. 足し算が出来ることはあまり使われない性質だけど、関係を見るのには使うらしい. &quot;知識グラフの埋め込み&quot; というのは実質的に、関係を距離と思って距離学習をするような枠組みと一致するかもしれないと思った.</p>
<p>さて実際のNLPへの応用を考えると本当に欲しかったのは別に単語ベクトルではなく、文や文章のベクトルであってことを思い出す. そういう場合に雑だがそれなりに効果があることもあるような方法として平均というのがある. つまり文や文章は単語の列だと思えるから、各単語について単語ベクトルを取ってその平均を取る. これは語順を考慮しないのである意味 Bag-of-Words のようなことをしてるようにも思える.</p>
<p>この至極単純な方法による文ベクトルの構成は, 長さの等しいベクトルは足せるという性質を使っているとも言える. そして問題点は <span class="math inline">\(u+v = v+u\)</span> という可換性だと言った. 語順は可換ではないから, 明らかに単語の和を取る操作は文や語句の構成を表現していない.</p>
<p>というわけで行列を使う. 単語を <span class="math inline">\(n \times n\)</span> 行列で表現することを考える. 行列には積があってこれは非可換である. これが語の合成に相当していたら間違いなく面白い.</p>
<p>行列の積という操作は 線形写像 <span class="math inline">\(\mathbb R^n \to \mathbb R^n\)</span> のことにほかならない. つまり長さ <span class="math inline">\(n\)</span> のベクトルの左に行列を掛ければ, やはり長さ <span class="math inline">\(n\)</span> のベクトルを得る. 語が行列の場合, 何かがベクトルになって語はそれを変形するものとみなせる.</p>
<p>ベクトル同士の類似度には内積を取った. 行列同士の類似度は何で測るのが自然なんだろう.</p>
<p>あと行列の積の計算が遅いから (頑張っても <span class="math inline">\(O(n^2.3737)\)</span>), 再現できても実用化シにくそう.</p>
<h3 id="追記">追記</h3>
<p>計算量はまあどうでもいいとして (定数バイ増えるだけなので), 行列を掛けることは NN の言葉で言うと bias の無い全結合層のこと. というわけで単語 <span class="math inline">\(w\)</span> を一つの全結合層 <span class="math inline">\(L_w\)</span> だと思うことにする.</p>
<p>入力は文脈. 文脈がわからないときは適当なゼロでない定数 <span class="math inline">\(c\)</span> を使う. 今までの単語分散表現 とは <span class="math inline">\(L_w c\)</span> のこと. <span class="math inline">\(c\)</span> はなんでもいいので仮に <span class="math inline">\([1,0,\ldots,0]^T\)</span> とすると, <span class="math inline">\(L_w\)</span> の1行目に今までの結果を代入すればよい. 他の行は他の文脈でのベクトルと, また, 合成の仕方を表している.</p>
<p>単語列 <span class="math inline">\(w_1, w_2, w_3, s_4\)</span> について, skip-gram 的に <span class="math inline">\(w_2 w_3\)</span> を学習するには <span class="math display">\[w_{23} = L_3 L_2 c\]</span> として, このベクトルから <span class="math inline">\(w_1, w_4\)</span> を予測させればよい. これは skip-gram の一般化になっている.</p>
<footer>
    <p class="is-pulled-right">
    @cympfh / mail@cympfh.cc
    </p>
</footer>
<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>
</body>
</html>
