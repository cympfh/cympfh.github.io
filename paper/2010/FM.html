<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="unidoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Factorization Machines</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link href="https://cdn.jsdelivr.net/npm/remixicon@4.6.0/fonts/remixicon.min.css" rel="stylesheet">
  <link href="https://unpkg.com/prismjs@1.x.0/themes/prism.css" rel="stylesheet" />
  <link href="../css/c.css" rel="stylesheet" />
  <link href="/resources/css/c.css" rel="stylesheet" />
  <link href="/resources/css/youtube.css" rel="stylesheet" />
  <script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-chtml-full.js"></script>
</head>
<body>

  <header class="page-header">
    <a href='../index.html'><i class="ri-send-plane-fill"></i></a>
  </header>

  <h1 class="title">Factorization Machines</h1>

  <div class="metainfo">
    <ul>
      <li>
        <i class="ri-link"></i>
        <a href="https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf" target="_blank">https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf</a>
      </li>
      <li>
        <i class="ri-lightbulb-flash-line"></i>
        線形 SVM と行列分解を組み合わせた予測器
      </li>
      <li>
        <i class="ri-price-tag-3-line"></i>
        <span class="paper_tag">スパース</span><span class="paper_tag">推薦</span><span class="paper_tag">FMs</span>
      </li>
    </ul>
  </div>

  \[
\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\Normal{\mathcal{N}}
\def\ip#1#2{\langle #1, #2 \rangle}
\def\Pr{\mathop{\mathrm{Pr}}}
\]
<h2>Intro</h2>
<p>新しい予測器モデル FM を提案する. 線形 SVM と行列分解を組み合わせて両方の良さを持った形をしている. データはスパースを前提にしている.</p>
<h2>FM Model</h2>
<p>データ \(x \in \R^{n}\) について, モデル \(\R^{n} \to \R\) を次で定める.</p>
\[w_0 + \ip{w}{x} + \sum_{i \lt j} \ip{v_i}{v_j} x_i x_j\]
<p>ここで</p>
<ul>
  <li>\(w_0 \in \R\)</li>
  <li>\(w \in \R^n\)</li>
  <li>
    \(V \in \R^{n \times k}\)
    <ul>
      <li>\(v_i \in \R^k\) は行ベクトル</li>
    </ul>
  </li>
  <li>\(x_i \in \R\) は \(x\) の第 \(i\) 成分</li>
  <li>\(\ip{-}{-}\) は内積</li>
</ul>
<blockquote>この形式を 2-way FM と呼ぶ. 後で一般化する.</blockquote>
<p>第二項までは通常の線形モデル. 第三項は \(x\) の (2-way) interaction を表す. つまり \(i\) と \(j\) の相互作用を予測している. その作用の重さを \(\ip{v_i}{v_j}\) で決めている.</p>
<blockquote>相互作用の重さを \(W \in \R^{n \times n}\) と陽に持ってしまわないのは, 単純に空間量が \(O(n^2)\) から \(O(nk)\) に減らせることと, 適切な \(k\) を選んで減らしたほうが汎化性能が上がるから. このあたりは行列分解を知っていれば当然に思える.</blockquote>
<h3>使い方</h3>
<p>特徴量 \(x\) がスパースであることを大前提にしている. 計算コストは値が入っている箇所にしか効かないので, 見た目上でどれだけ次元数が膨大であっても構わない. どんな特徴量でも横に結合していけばよい. Fig.1 はこのことを言っている. 例えば User ID を one-hot encoding している.</p>
<h3>第三項の計算コスト</h3>
<p>\(\sum_{i \lt j} \ip{v_i}{v_j} x_i x_j\) の計算だが, これはナイーブに計算すると \(O(n^2 k)\) 掛かる.</p>
<pre><code class="code language-python">def naiive(v, x) -&gt; float:
    value = 0.0
    for i in range(n):
        for j in range(i + 1, n):
            value += (v[i, :] @ v[j, :]) * x[i] * x[j]
    return value
</code></pre>
<p>よく展開して計算すると次が成り立つ.</p>
\[\sum_{i \lt j} \ip{v_i}{v_j} x_i x_j
= \frac{1}{2} \sum_{j=1}^k \left( \ip{v^j}{x}^2 - \ip{(v^j)^{\odot 2}}{x^{\odot 2}} \right)\]
<p>ここで \(v^j\) は第 \(j\) 列ベクトル. また \((v^j)^{\odot 2}, x^{\odot 2}\) はそれぞれ要素ごとの二乗. この計算式で素直に計算すると \(O(nk)\) で済む.</p>
<pre><code class="code language-python">def fast(v, x) -&gt; float:
    value = 0.0
    for j in range(k):
        value += (v[:, j] @ x) ** 2
        value -= ((v[:, j] * x) ** 2).sum()
    return value &#x2F; 2.0
</code></pre>
<h3>最適化</h3>
<p>素直に SGD するなら次を使う.</p>
<ul>
  <li>\(\frac{\partial}{\partial w_0} \hat{y} = 1\)</li>
  <li>\(\frac{\partial}{\partial w_i} \hat{y} = x_i\)</li>
  <li>\(\frac{\partial}{\partial v_{i,k}} \hat{y} = x_i \sum_j v_{j,k} x_j - v_{i,k} x_i^2\)</li>
</ul>
<h3>d-way FM への拡張</h3>
<p>相互作用がちょうど2つから成っていたのを \(d\) 個に拡張することが考えられる.</p>
\[\hat{y} = w_0 + \langle w, x \rangle + \sum_d \sum_{i_1 \lt i_2 \lt \cdots \lt i_d} \left( \prod_i x_i \right) \langle v_{i_1}, v_{i_2}, \ldots v_{i_d} \rangle\]
<p>ここで \(\langle v_1,v_2,\ldots,v_d \rangle\) は 成分間の積の和で \(\sum_i \sum_k v_{i,k}\) と定める.</p>
<h2>SVM との比較</h2>
<p>FMs は線形 SVMs の拡張だと思える. 特に \(d=1\) の 1-way FM は線形 SVM そのもの.</p>
<p>さて SVM はカーネルを取り替えてその自由度を上げることができた. \(K(x,z) = (\ip{x}{z} + 1)^2\) という多項式カーネルを使うとこれは相互作用を持つことが出来る.</p>
<p>SVM のモデルをこのカーネルを使って書き下すと次を得る.</p>
\[w_0 + \sqrt{2} \ip{w}{x}
+ \sum_i v_{ii} x_i^2
+ \sqrt{2} \sum_{i \lt j} v_{ij} x_i x_j\]
<p>ここで \(w_0, w\) は FM 同様だが \(V = (v_{ij})\) という行列が重みとして追加されている. FM の \(V\) を陽に持った場合とよく似た形になっていることが分かる. \(V\) の対角成分をゼロにして適切に行列分解して潰せば同値だ.</p>
<h2>行列分解との比較</h2>
<p>行列分解は次のようなものを考える. 2つの異なるカテゴリ集合 \(U,I\) (典型的には Users, Items だ) があって, \(U\) と \(I\) の組で決まる値が行列 \(X = \R^{U \times I}\) として記述されている. このとき, この行列成分を予測するモデル \(U \times I \to \R\) を で構築しようというのが行列分解である.</p>
<p>典型的な行列分解はこのモデルを \(X_{ui} = \ip{w_u}{v_i}\) で定める. ここで行列 \(X\) が \((w_u)_u\) と \((v_i)_i\) という2つの行列に分解されている.</p>
<p>FM では次のように考える.</p>
<p>\((u, i) \in U \times I\) に対して, 次のベクトル \(x\) を割り当てる. ここでベクトルの添字は \(\mathcal J = U + I\) という直和からなる.</p>
\[
x_j = \begin{cases}
    1 &amp; j = u ~ (u \in U) \\
    1 &amp; j = i ~ (i \in I) \\
    0 &amp; \text{otherwise}
\end{cases}
\]
<p>上記で定まるベクトル \(x\) のことを \(x^{(ui)}\) と書くことにしよう. 行列分解が \((u, i)\) から \(X_{ui}\) を予測することは FM では \(x^{(ui)}\) から \(X_{ui}\) を予測することに相当する.</p>
<p>さてこれを FM の式に実際に代入する. 値があるのは \(j=u\) と \(j=i\) の二箇所だけであるので結局</p>
\[w_0 + w_u + w_i + \ip{v_u}{v_i}\]
<p>バイアス項を無視すればこれは行列分解そのものに他ならない.</p>


  <script src="/resources/js/toc.js?20250428"></script>
  <script src="/resources/js/youtube.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>
