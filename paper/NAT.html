<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Unsupervised Learning by Predicting Noise</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../resources/css/c.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="page-header">
    <a href='index.html'><i class="fa fa-send-o" style="position:absolute; left:0.4em; top:0.4em; width:1.3em;border-radius:0.8em;"></i></a>
</header>
<header>
<h1 class="title">Unsupervised Learning by Predicting Noise</h1>
</header>
<ul>
<li>
original paper: <a href=https://arxiv.org/abs/1704.05310>https://arxiv.org/abs/1704.05310</a>
</li>
</ul>
<div class="is-pulled-right">
<p><a class='tag is-blue' href=index.html#深層学習>深層学習</a> <a class='tag is-blue' href=index.html#画像認識>画像認識</a></p>
</div>
<h2 id="概要">概要</h2>
<p>教師ナシで ImageNet を解く. Noise As Target (NAT) という手法を提案しており、文字通り、ノイズを教師信号に用いる.</p>
<h2 id="手法">手法</h2>
<p><span class="math inline">\(n\)</span> 枚ある画像データを用いて、適切な <span class="math inline">\(d\)</span> 次元特徴ベクトルへ写す <span class="math inline">\(f_\theta\)</span> を学習したい. 画像 <span class="math inline">\(x_i\)</span> に対して 適切なラベル <span class="math inline">\(y_i\)</span> がある教師アリの状態では、これは例えば</p>
<p><span class="math display">\[\min_{\theta, \phi}\ell(g_\phi(f_\theta(x_i)), y_i)\]</span></p>
<p>を解くことなどによって良さそうな特徴ベクトルが得られる. ここで <span class="math inline">\(\ell\)</span> は適当な損失関数.</p>
<p>今問題を簡単にするために、特徴ベクトルを <span class="math inline">\(S^{d-1}\)</span> 上に限る. 即ち (非ゼロな) <span class="math inline">\(d\)</span> 次元ベクトルを得た後に正規化を行う. <span class="math inline">\(n\)</span> 枚の画像がこの上に写るわけだが、一様に写ることを理想だとする. 基本のアイデアは、<span class="math inline">\(S^{d-1}\)</span> から十分個数の点を一様にサンプリングしたとき、いずれかの画像が写るべき点が1つはあるはずだ、というものである. また、異なる画像が同じ点には写らない (縮退しない) こともついでに要請する.</p>
<p>画像が <span class="math inline">\(n\)</span> 枚に対して十分多くの <span class="math inline">\(k\)</span> 個のベクトル <span class="math display">\[c_1, c_2, \ldots,c_k\]</span> を <span class="math inline">\(S^{d-1}\)</span> から一様サンプリングする. これを並べて出来る行列</p>
<p><span class="math display">\[C = \left[c_1, c_2, \ldots, c_k\right] (\in \mathbb{R}^{k \times d})\]</span></p>
<p>これと、どの特徴ベクトル <span class="math inline">\(c_j\)</span> がどの画像 <span class="math inline">\(x_i\)</span> に割り当てられるかという行列</p>
<p><span class="math display">\[P = \{0,1\}^{n \times k}\]</span></p>
<p>とを掛けあわせた</p>
<p><span class="math display">\[Y = PC\]</span></p>
<p>が、<span class="math inline">\(X\)</span> が写るべき先というわけで、</p>
<p><span class="math display">\[\min_{\theta, P}\ell(f_\theta(X), PC)\]</span></p>
<p>を目指す. ここで割当行列 <span class="math inline">\(P\)</span> もパラメータであることに註意.</p>
<p>さて <span class="math inline">\(P\)</span> には制約がある. それは、一つの画像は一つの特徴ベクトルが割り当てられ、同じ特徴ベクトルは高々一つの画像にしか割り当てられないというものである. 論文ではこれを</p>
<ul>
<li><span class="math inline">\(P 1_k \leq 1_n\)</span></li>
<li><span class="math inline">\(P^t 1_n = 1_k\)</span></li>
</ul>
<p>と書かれていた. これは独自解釈だが、<span class="math inline">\(1_k, 1_n\)</span> はそれぞれ <span class="math inline">\(k \times n, n \times k\)</span> な対角成分が 1 の行列なんだと思う. じゃないと合わないので. でも普通、<span class="math inline">\(1_m\)</span> って正方な単位行列だと思わん？？？？</p>
<h3 id="学習方法">学習方法</h3>
<p><span class="math inline">\(P\)</span> の更新と <span class="math inline">\(\theta\)</span> の更新を交互に繰り返す.</p>
<p><span class="math inline">\(\theta\)</span> の更新は所謂勾配法による. ここは普通に CNN の学習.</p>
<p><span class="math inline">\(P\)</span> の更新は割当問題を解くことによる.<br />
厳密的に解くにはハンガリアン法で解けるがこれはかなり遅い (分かんないけど <span class="math inline">\(O(n^2k)\)</span> か <span class="math inline">\(O(nk^2)\)</span> かですかね). そこでまず <span class="math inline">\(k=n\)</span> とする (マジかよ…). 特徴ベクトルはちょうどどれか一つの画像に割り当てられているという条件から、制約は <span class="math inline">\(P 1_n = 1_n, P^t 1_n = 1_n\)</span> と書かれる.</p>
<p>そして、どうせ深層学習の部分で <span class="math inline">\(n\)</span> の画像を一つのサイズを <span class="math inline">\(b\)</span> とする <span class="math inline">\(n/b\)</span> 個のミニバッチに分けている. そこで、ミニバッチごとに <span class="math inline">\(b \times b\)</span> の割当行列 <span class="math inline">\(P\)</span> を求めることにする. この <span class="math inline">\(P\)</span> の最適化はハンガリアン法でやって <span class="math inline">\(O(b^3)\)</span> 掛かる. これが <span class="math inline">\(n/b\)</span> 個なので結局、全体で <span class="math inline">\(O(nb^2)\)</span> かかる.</p>
<h2 id="感想">感想</h2>
<p>ランダムはベクトルを教師信号に使うとだけ聞くとかなり驚いたが、割当行列も一緒に学習すると聞けば納得する. 上手いなあ. ただ <span class="math inline">\(P\)</span> の学習方法がしょぼい…。</p>
<!--

  以下を埋め込むと H2 タグを列挙してそれぞれへのリンクにする.
  ただし "INDEX" は除外する.

    <div id=toc></div>


  H2, H3 タグまでを列挙するには以下を埋め込む.

    <div id=toc-level-2></div>

-->
<script>
(function() {

  function naming(obj, name) {
      var PREF = document.createElement('a');
      PREF.name = name;
      obj.appendChild(PREF);
  }

  function level1() {

    var sections = document.getElementsByTagName('h2');
    var OL = document.createElement('ol');
    for (var i=0; i < sections.length; ++i) {
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = sections[i].innerHTML;
      if (A.innerHTML.toUpperCase() == 'INDEX') continue;
      A.href = '#' + i;
      LI.appendChild(A);
      OL.appendChild(LI);
      naming(sections[i], i);
      // var PREF = document.createElement('a');
      // PREF.name = i;
      // sections[i].appendChild(PREF);
    }

    return OL;
  }

  function level2() {

    var sections = document.querySelectorAll('h2,h3');
    var tree = [];
    for (var i = 0; i < sections.length; ++i) {
      if (sections[i].tagName == 'H2') {
        if (sections[i].innerHTML.toUpperCase() === 'INDEX') continue;
        tree.push([sections[i]]);
      } else {
        if (tree.length > 0) {
          tree[tree.length-1].push(sections[i]);
        } else {
          tree.push([sections[i]]);
        }
      }
    }

    var OL = document.createElement('ol');
    for (var i = 0; i < tree.length; ++i) {

      // h2-level
      var LI = document.createElement('li');
      var A = document.createElement('a');
      A.innerHTML = tree[i][0].innerHTML;
      A.href = '#' + i;
      naming(tree[i][0], i);
      LI.appendChild(A);

      // h3-level
      if (tree[i].length > 1) {
        var OL_sub = document.createElement('ol');
        for (var j = 1; j < tree[i].length; ++j) {
          var LI_sub = document.createElement('li');
          var A = document.createElement('a');
          A.innerHTML = tree[i][j].innerHTML;
          A.href = `#${i}-${j}`;
          naming(tree[i][j], `${i}-${j}`);
          LI_sub.appendChild(A);
          OL_sub.appendChild(LI_sub);
        }
        LI.appendChild(OL_sub);
      }

      OL.appendChild(LI);
    }

    return OL;
  }

  function append_toc() {
    if (document.getElementById('toc')) {
      document.getElementById('toc').appendChild(level1());
    }
    if (document.getElementById('toc-level-2')) {
      document.getElementById('toc-level-2').appendChild(level2());
    }
  }

  window.addEventListener('DOMContentLoaded', append_toc, false);
}());
</script>
</body>
</html>
