<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="unidoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Local Ordinal Embedding</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link href="https://cdn.jsdelivr.net/npm/remixicon@4.6.0/fonts/remixicon.min.css" rel="stylesheet">
  <link href="https://unpkg.com/prismjs@1.x.0/themes/prism.css" rel="stylesheet" />
  <link href="../css/c.css" rel="stylesheet" />
  <link href="/resources/css/c.css" rel="stylesheet" />
  <link href="/resources/css/youtube.css" rel="stylesheet" />
  <script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-chtml-full.js"></script>
</head>
<body>

  <header class="page-header">
    <a href='../index.html'><i class="ri-send-plane-fill"></i></a>
  </header>

  <h1 class="title">Local Ordinal Embedding</h1>

  <div class="metainfo">
    <ul>
      <li>
        <i class="ri-link"></i>
        <a href="http://www.tml.cs.uni-tuebingen.de/team/luxburg/publications/TeradaLuxburg_ICML2014.pdf" target="_blank">http://www.tml.cs.uni-tuebingen.de/team/luxburg/publications/TeradaLuxburg_ICML2014.pdf</a>
      </li>
      <li>
        <i class="ri-lightbulb-flash-line"></i>
        距離の順序構造から埋め込み表現を獲得する
      </li>
      <li>
        <i class="ri-price-tag-3-line"></i>
        <span class="paper_tag">埋め込み表現</span><span class="paper_tag">多様体学習</span><span class="paper_tag">距離学習</span><span class="paper_tag">SOE</span><span class="paper_tag">LOE</span>
      </li>
    </ul>
  </div>

  \[
\def\N{\mathbb{N}}
\def\R#1{\mathbb{R}^{#1}}
\def\Pr{\mathop{\mathrm{Pr}}}
\]
<h2>Intro</h2>
<p>距離に関する順序構造だけが与えられたときに, これを再現するような埋込表現を獲得することを考える. これを最適化問題に落とし込んで解く SOE を提案する. さらに問題を制約を kNN に限定した LOE を提案する.</p>
<h2>Ordinal Embedding</h2>
<p>対象の集合 \(X = \{x_1, x_2, \ldots, x_n\}\) とそれらの距離 \(\xi \colon X \times X \to \mathbb R_{\geq 0}\) があるとする. ここで \(X\) は陽に与えられるが \(\xi\) は観測できない. その代わりに次のような順序に関する制約 \(A\) が与えられている.</p>
\[A \subset X^4\]
\[(i,j,k,l) \in A \implies \xi_{ij} \lt \xi_{kl}\]
<p>そこで適切な埋め込み表現</p>
\[\hat{x_i} \in \mathbb R^p\]
<p>を全ての対象について割り当てることで,</p>
\[\xi_{ij} \lt \xi_{kl} \iff \| \hat{x_i} - \hat{x_j} \| \lt \| \hat{x_k} - \hat{x_l} \|\]
<p>これを満たすようにしたい.</p>
<blockquote>このような問題は Shepard 1962, Kruskal 1964 の時代から研究されてきたそう. 最近は機械学習の文脈でよく出てくるようになった.</blockquote>
<h2>Soft Ordinal Embedding; SOE</h2>
<h3>問題設定</h3>
<p>対象を \([n] = \{1,2,\ldots,n\}\) だとし, 制約として \(A \subset [n]^4\) が与えられる. これの意味は,</p>
\[(i,j,k,l) \in A \implies \xi_{ij} \lt \xi_{kl}\]
<p>ということ.</p>
<p>このときに埋め込み表現の行列</p>
\[X \in \mathbb R^{n \times p}\]
<p>を構成することを考える. ここで \(p\) は最初に決めて固定しておく. \(X\) の第 \(i\) 行ベクトルを \(x_i\) と書いてこれが対象 \(i\) の表現ベクトルになる. 普通のユークリッド距離を \(d_{ij}(X) = \| x_i - x_j \|\) と書く.</p>
<p>制約 \(A\) を全て満たすことは次の \(\mathcal L_1\) がゼロになることだと言える.</p>
\[\mathcal L_1(X \mid A) = \sum_A \mathbb{1}[d_{ij}(X) \geq d_{kl}(X)]\]
<p>しかし, これは連続関数でもないし直接最適化の目的関数にするのは難しい.</p>
<p>そこでこれを緩和した方法が提案されている. 次は Johnson, 1973 による方法.</p>
\[\mathcal L_2(X \mid A) = \frac{\sum_A (\max(0, d^2_{ij}(X) - d^2_{kl}(X)))^2 }{ \sum_A d^2_{ij}(X) - d^2_{kl}(X) }\]
<h3>SOE</h3>
<p>一方で本論文では別な方法を提案してる. 正のスケールパラメータ \(\delta &gt; 0\) を用いて,</p>
\[\mathcal L(X \mid A, \delta) = \sum_A (\max(0, d_{ij}(X)+\delta-d_{kl}(X)))^2\]
<p>これの最小化を考える. この最適化問題を <strong>SOE; soft ordinal embedding</strong> と名付けてある.</p>
<p>論文の記法に従うと \(o_{ijkl} = \mathbb{1}[(i,j,k,l) \in A]\) を使って,</p>
\[\mathcal L(X \mid A, \delta) = \sum_{i,j,k,l} o_{ijkl} (\max(0, d_{ij}(X)+\delta-d_{kl}(X)))^2\]
<p>としてある.</p>
<p>SOE とその \(\delta\) に関して次の性質がある. \(A,p\) について, \(\delta_1, \delta_2\) のそれぞれで SOE の最適解を求めることを考える. \(\delta_1\) による最適解を \(X\) とする. このとき</p>
\[X&#x27; = \frac{\delta_2}{\delta_1} X\]
<p>は \(\delta_2\) による最適解（の一つ）になっている.</p>
<blockquote>\(\delta=0\) のとき, SOE には自明な解として \(X=0, d=0\) がある. これは一番最初の厳密なバージョン \((\mathcal L_1)\) でも同じことが言えた. 2つ目の Johnson による方法 \((\mathcal L_2)\) では分母にも距離があるからこれが防げていた. 今回は \(\delta\) をある程度大きくしておくことで, 単純に \(d=0\) とするだけでは不十分で, \(d_{kl} - d_{ij} \geq \delta\) という差が付くような距離を持たせる必要がある. このことによって自明な距離にはならない. では \(\delta\) としてどんな値に設定する必要があるかという別な問題が生じるように思えるところだが, 先述した性質は, 実はどんな \(\delta\) であっても, 本質的には（up to 拡大縮小）一意な最適解が得られることを言っている.</blockquote>
<h3>Majorization Algorithm（優関数法）</h3>
<p>SOE の最適化を求めるアルゴリズム.</p>
<p>実関数 \(f \colon X \to \mathbb R\) について, \(g\) が \(f\) の majorizing 関数であるとは, 次のようなもののこと.</p>
<ul>
  <li>\(g \colon X \times X \to \mathbb R\)</li>
  <li>\(f(x_0) = g(x_0, x_0) ~ \forall x_0 \in X\)</li>
  <li>\(f(x) \leq g(x, x_0) ~ \forall x,x_0 \in X\)</li>
</ul>
<p>例えば今 \(x,x&#x27;\) について \(g(x&#x27;,x) \leq g(x,x)\) だとすると, \(g\) の定義から \(f(x&#x27;) \leq g(x&#x27;,x) \leq g(x,x)=f(x)\) を得る. つまり \(g\) に関して最小化することは \(f\) に関する最小化にもなっていることが分かる. \(f\) よりも最小化しやすい majorizing \(g\) を構成できれば</p>
\[x_{t+1} \leftarrow \mathrm{argmin}_x g(x, x_t)\]
<p>を逐次的に実行してくような最適化法がありえる.</p>
<p>というわけで SOE の \(\mathcal L\) の majorizing 関数を構成する.</p>
\[\tilde{L}(X,Y) = \alpha_{ijkl} (x_i - x_j)^2
+ \alpha_{ijkl}^\ast (x_k - x_l)^2
-2\beta_{ijkl} (x_i - x_j)^t (y_i - y_j)
-2\beta_{ijkl}^\ast (x_k - x_l)^t (y_k - y_l)
+\gamma_{ijkl}\]
<p>このとき</p>
<ul>
  <li>\(\mathcal L(X) \leq \tilde{L}(X,Y)\)</li>
  <li>\(\mathcal L(X) = \tilde{L}(X,X)\)</li>
</ul>
<p>が満たされていて確かに \(\tilde{L}\) は \(\mathcal L\) の majorizing 関数になっている. ただしここで \(\alpha,\alpha^\ast,\beta,\beta^\ast,\gamma\) は \(Y\) から適切に決まるパラメータ. 論文の補遺ページにあるがここでは省略.</p>
<h2>Local Ordinal Embedding; LOE</h2>
<h3>問題設定</h3>
<p>一般の順序制約は \(O(n^4)\) に関するものなので広すぎる. 一気に狭めた問題を考える.</p>
<p>対象を \([n]=\{1,2,\ldots,n\}\) とする. 制約として kNN と呼ばれる各対象の隣接点集合 \(\Gamma\) が与えられている.</p>
\[\Gamma \colon [n] \to 2^n\]
\[\Gamma \colon i \mapsto \Gamma_i \subset [n]\]
<p>ただし</p>
\[|\Gamma_i| = k.\]
<p>ここで kNN の意味は次のようである.</p>
\[j \in \Gamma_i \land l \not\in \Gamma_i \implies \xi_{ij} \lt \xi_{il}\]
<p>つまり kNN \(\Gamma_i\) は \(i\) から見て近い対象を集めてきた集合である.</p>
<p>kNN \(\Gamma\) はグラフで表現することも出来る. すなわち, 対象を頂点とし, \(j \in \Gamma_i\) のことを有向辺 \(i \to j\) で表現する. これを kNN グラフ \(G\) という.</p>
<p>kNN グラフ \(G\) が与えられたときに, 対象に埋め込み表現 \(X \in \mathbb R^{n \times p}\) を適切に表現することで, \(G\) を再現するようなものを求めよというのが問題である.</p>
<h3>学習アルゴリズム</h3>
<p>次元 \(p\) は最初に決めて固定してある.</p>
<p>kNN グラフ \(G\) の隣接行列を \(A = (a_{ij})_{ij}\) とする. 辺 \((i \to j)\) があるとき（すなわち \(j \in \Gamma_i\) ） \(a_{ij}=1\) さもなくばゼロである.</p>
\[a^\ast_{ijk} = a_{ij} (1 - a_{ik})\]
<p>という値を定める. このとき, kNN の制約を満たすことは次の値が最小化されゼロであること.</p>
\[\mathcal{L}(X \mid \delta) = \sum_{i,j,k} a^\ast_{ijk}
\max\left( 0, d_{ij}(X) + \delta - d_{ik}(X) \right)^2\]
<p>これについて先述した majorizing 関数を構成すると次のようになる.</p>
\[\mathcal{L}(X \mid \delta) \leq \sum_{s=1}^p \left[
x_s^t M x_s - 2x_s^t H y_s
\right] + \gamma\]
<p>ここで \(x_s, y_s\) は \(X,Y \in \mathbb{R}^{n \times p}\) の列ベクトル. \(M,H,\gamma\) は \(Y\) から適切に定まるパラメータ. 論文の補遺ページにあるがここでは省略. そして右辺は普通に微分できる形をしてるので, \(Y\) に対して 最小化する \(X\) は代数的に求まる.</p>
<h2>LOE の実験</h2>
<p>適当に作ったダミーデータから kNN グラフを構築, もとのダミーデータが復元できるかを見る. 大体出来てる. 本当にもとの座標を復元したいなら LOE 一択といった結果.</p>
<p><img src="https://i.imgur.com/HXtJq8A.png" alt="" /></p>
<p>二次元データや三次元データ, kNN グラフからの復元など何通りかの実験結果が載ってある. 上図はその一つ.</p>
<p>補遺の Figure 2 は kNN の k を変えた場合に復元されたデータを比較してる. 少なすぎる \((k=10)\) と難しいのは当然だが, 多すぎる \((k \geq 450)\) 場合でも次第に潰れるようになってる. 全部が隣接点になるから, それもそうか.</p>
<h2>感想</h2>
<p>Majorization Algorithm が複雑だ. 適当な任意の勾配法で学習させてもいいのかな.</p>
<p>実験を見ると復元でき具合は LOE が抜き出てる. t-SNE がピーキーだ. といっても t-SNE は復元が目的じゃなくて, 点と点が隣同士にあって繋がってるかどうかだけを気にしてるような手法だと思うので比較対象としてフェアじゃない気もする.</p>


  <script src="/resources/js/toc.js?20250428"></script>
  <script src="/resources/js/youtube.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>
