% Wed Jan 15 2020

## 画像分類について

画像データの 1 pixel は普通有限 bit で表現される.
簡単にグレイスケール画像で 1 pixel が 8bit とすれば, 各ピクセルとは整数 $[0, 256)$ の値のこと.
しかしながら画像処理に置いても人間の認知を考えても整数とは思っておらず, 実数 $[0, 256)$ の上の代表値としてその整数を取ったものと見做すのが普通だ.
だから, 画像 $x$ が与えられたときには, それは, まあ大体 $(x-1, x+1)$ という値の範囲から一つ代表値を選んで表示したものと思うだろう.

次に実数の上の滑らかな確率分布のことを思い出す.
例えば区間 $[0,1]$ の上の一様分布を考えてみる.
つまり $0$ 以上 $1$ 以下の実数がどれも等しい確率で選ばれるような確率変数 $X$.
この値がぴったり $X = 0.5$ を取る確率はゼロである.
無限に値を取り得る値がピッタリ 0.5 であるようなことは確かにゼロに思える.
つまり任意の実数 $a \in [0,1]$ について $P(X=a)=0$ である.
実数の上の分布の場合については $P(a \leq X \leq b)$ を考えることしか出来ない.
例えば今の一様分布の場合, $0 \leq a \leq b \leq 1$ について, $P(a \leq X \leq b) = b - a$ である.
さらにこれを微分することで確率分布という概念が出来る.
確率分布 $\mu$ とは $\int_a^b \mu(x) dx = P(a \leq X \leq b)$ であるような関数のこと.
今の一様分布なら $\mu(x) = 1$ がそれになる.

画像とは実数（以上の濃度）の空間の上の部分空間で, その上に何かの確率分布を考えてみる.
例えば猫画像であることの確率を考える.
画像データを表す確率変数 $X$ と, 猫画像であることを表す確率変数 $C$ を考える.
$X$ の値域はユークリッド空間の部分空間.
$C$ の値域は $\{0,1\}$ で $C=1$ が猫であることを意味する.
画像空間の上の分布なので,
$P(X|C=1)$
という条件付き確率を考えるとちゃんと確率分布になる.
それを $\mu_C$ と書くことにする.

$\mu_C$ が本当に存在するなら（そしてそれはきっとあると思う）,
さっき延べたように
$\forall x, P(X=x|C=1) = 0$
である.
条件付き確率の定義（或いはベイズの定理）から,
$P(X=x,C=1)=0$.
では, $P(C=1|X=x) = P(X=x,C=1) / P(X=x)$ は?
右辺は素直に計算するとゼロ割るゼロとなって値を定めない.
厳密には極限で計算しないといけない.

で, 思ったのは, 実際に画像 $x$ が与えられた場合, それは実は区間 $(x-1,x+1)$ から与えられているという話だ.
確率 $P(X=x)$ を考えるべきではなくて,
$\int_{x-1}^{x+1} dx ~ \mu_C(x)$
という積分値を考えないといけないのではないか?
それだったら,
$P(C=1 | x-1<X<x+1) = P(x-1 < X < x+1 ,C=1) / P(x-1 < X < X)$
を計算することになって,
右辺は実数割る実数になって計算が容易になる.

などと考えた.

実際には $(x-1,x+1)$ 上でそれらの確率はほぼ定数だから, 関係ないだろうけど.

普通のカテゴリ予測器であれば,
$P(C|X)$
という関数を, $X$ を入れたら $P(C|X)$ を返す機械として定めてモデル化するのに対して,
上の方法では
$P(x-1 < X < x+1 ,C=1)$
と
$P(x-1 < X < X)$
とのそれぞれを予測するモデルが必要になるので手間だ.
しかも学習するのは $P(X=x)$ という確率ではなくて, 確率分布.
確率分布は結局積分する形式でしか利用しないので,
入力 $a,b$ について $P(a < X < b)$ を計算する機械として定めれば同じことだ.
訓練データとして実数のペア $a,b$ について $(a,b)$ の範囲にある学習データとその尤度確率をラベルに学習すればいいのかも？
$C$ との同時確率バージョンなら,
訓練データ $D = \{(x_i, y_i)\}$ に対して $A = \{ i \mid a < x_i < b \land y_i = C \}$ を取ってきて,
$P(a < X < b, C) = |A| / |D|$
を学習するみたいな感じ.
$b=a+2$ に制限してもしなくてもいいかもしれない.

## 確率の学習について

本当はこんなことを考えてたわけじゃなくて,
ディープラーニングって確率を考えてたことをいつの間にかカテゴリ分類, 乃至は one-hot encoding をターゲットにすり替えていて,
それが気持ち悪いなあとずっと考えてた.
つまり, 本当は 0.8 みたいな中途半端な確率を取るはずなのに, 訓練データでは簡単に 0 か 1 か或いはせいぜいそれをスムージングした値くらいを学習データに入れてしまう.
それでいて最終層には softmax なんかを入れて, これは確率ですよって顔をしている.
確かに, 予測時にはちゃんと微妙な値を取ったりするので, なんだか確率らしきものを学習してる気分にならなくもない.
でも学習データにおける平均を出してるだけだとも思う.
だったら初めから平均を算出してそれを訓練信号にしたほうが, 学習もスムーズになったりしないだろうか?

考えている確率のほとんどは先に述べたような確率分布を持つ, と思う.
確率分布を学習させるのは本質な気がする.
